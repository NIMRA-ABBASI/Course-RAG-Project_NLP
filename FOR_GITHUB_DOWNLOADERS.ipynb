{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe20603",
   "metadata": {},
   "source": [
    "# 🚀 Start Project From Here\n",
    "\n",
    "Welcome! If you have downloaded this project from GitHub, please follow these steps to get started:\n",
    "\n",
    "1. **Read the `README.md` file** for setup instructions and environment requirements.  \n",
    "2. **Download all required models and data** as described in the README.  \n",
    "3. **Run this Jupyter Notebook** starting from this cell to initialize and use the project.  \n",
    "4. All code and pipeline steps are organized below for easy execution.\n",
    "\n",
    "> **Note:** Make sure you have installed all dependencies and placed the necessary files in the correct directories as per the instructions.\n",
    "\n",
    "Happy coding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in b:\\course rag project\\course_rag\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: huggingface_hub in b:\\course rag project\\course_rag\\lib\\site-packages (0.32.3)\n",
      "Requirement already satisfied: numpy in b:\\course rag project\\course_rag\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: sentence-transformers in b:\\course rag project\\course_rag\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: filelock in b:\\course rag project\\course_rag\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in b:\\course rag project\\course_rag\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in b:\\course rag project\\course_rag\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in b:\\course rag project\\course_rag\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in b:\\course rag project\\course_rag\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in b:\\course rag project\\course_rag\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in b:\\course rag project\\course_rag\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in b:\\course rag project\\course_rag\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in b:\\course rag project\\course_rag\\lib\\site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in b:\\course rag project\\course_rag\\lib\\site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in b:\\course rag project\\course_rag\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in b:\\course rag project\\course_rag\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in b:\\course rag project\\course_rag\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in b:\\course rag project\\course_rag\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in b:\\course rag project\\course_rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in b:\\course rag project\\course_rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in b:\\course rag project\\course_rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in b:\\course rag project\\course_rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in b:\\course rag project\\course_rag\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in b:\\course rag project\\course_rag\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in b:\\course rag project\\course_rag\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in b:\\course rag project\\course_rag\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in b:\\course rag project\\course_rag\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in b:\\course rag project\\course_rag\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in b:\\course rag project\\course_rag\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in b:\\course rag project\\course_rag\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in b:\\course rag project\\course_rag\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Run this first to donwalod all the required libraries\n",
    "\n",
    "%pip install numpy transformers huggingface_hub numpy sentence-transformers\n",
    "\n",
    "%pip install faiss-cpu\n",
    "\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e25598",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Donwloading the Model: \"intfloat__e5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Target download directory\n",
    "local_model_dir = os.path.join(\"models\", \"intfloat__e5-base\")\n",
    "\n",
    "# Download from Hugging Face to a temp location\n",
    "print(\"📥 Downloading model...\")\n",
    "downloaded_path = snapshot_download(repo_id=\"intfloat/e5-base\")\n",
    "\n",
    "# Copy downloaded model to desired directory if not already there\n",
    "if not os.path.exists(local_model_dir):\n",
    "    print(f\"📦 Copying model to {local_model_dir} ...\")\n",
    "    shutil.copytree(downloaded_path, local_model_dir)\n",
    "\n",
    "print(f\"✅ Model is saved in {local_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14352939",
   "metadata": {},
   "source": [
    "### GO to this link adn download the LLM MODEL: Llama-3.2-1B-Instruct-Q4_K_M.gguf\n",
    "\n",
    "### LINK: https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/blob/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf\n",
    "\n",
    "### CLick on download and store in location of Models directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1554eb",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------\n",
    "### --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Load the Models Once In Memory. \n",
    "## This is done so you don't have to load it everytime.\n",
    "\n",
    "### --------------------------------------------------------------------------------------------------------------\n",
    "### --------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to import llama-cpp-python\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "    LLAMA_CPP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LLAMA_CPP_AVAILABLE = False\n",
    "    print(\"Warning: llama-cpp-python not installed. LLM generation will be simulated.\")\n",
    "    print(\"         Install with: pip install llama-cpp-python (or llama-cpp-python[cuda] for GPU)\")\n",
    "\n",
    "# Configuration for local setup\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration class for RAG pipeline\"\"\"\n",
    "    # Paths (relative to where you run the script, assuming standard project structure)\n",
    "    # These should point to the output of your Step 4 (FAISS Vector Store Creation)\n",
    "    faiss_index_path: str = \"faiss_vector_store/faiss_index.index\"\n",
    "    chunk_metadata_path: str = \"faiss_vector_store/chunk_metadata.json\"\n",
    "    vector_store_metadata_path: str = \"faiss_vector_store/vector_store_metadata.json\"\n",
    "    \n",
    "    # Model paths\n",
    "    # SentenceTransformer will automatically download 'intfloat/e5-base' to your local cache\n",
    "    embedding_model_name: str = \"intfloat/e5-base\"\n",
    "    # LLaMA 3.2-1B GGUF model path: You need to download this model manually\n",
    "    # (e.g., from Hugging Face Hub) and place it in your local 'models/' folder.\n",
    "    llm_model_path: str = \"models/llama-3.2-1b-instruct-q4_k_m.gguf\"  # IMPORTANT: Update this path!\n",
    "    \n",
    "    # Retrieval parameters\n",
    "    top_k_dense: int = 3  # Number of chunks to retrieve\n",
    "    similarity_threshold: float = 0.4  # Minimum similarity score (adjust if too many irrelevant chunks or too few relevant)\n",
    "    \n",
    "    # LLM parameters\n",
    "    max_tokens: int = 1200  # Max tokens for LLM response\n",
    "    temperature: float = 0.6\n",
    "    top_p: float = 0.85       ## FFooccuusseedd bbuutt nnoott ttoooo rreessttrriiccttiivve\n",
    "    context_length: int = 2800  # Context window for LLaMA 3.2-1B (ensure it matches the model's actual capacity)\n",
    "\n",
    "class DenseRetriever:\n",
    "    \"\"\"Handles dense retrieval using FAISS and sentence transformers\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.embedding_model = None\n",
    "        self.faiss_index = None\n",
    "        self.chunk_metadata = None\n",
    "        self.vector_store_metadata = None\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the retriever components\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🔄 INITIALIZING DENSE RETRIEVER\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load embedding model\n",
    "        print(\"📥 Loading embedding model (this may take a moment, downloads if not cached)...\")\n",
    "        start_time = time.time()\n",
    "        # SentenceTransformer manages model download and loading\n",
    "        self.embedding_model = SentenceTransformer(self.config.embedding_model_name)\n",
    "        print(f\"✅ Embedding model loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        print(\"📥 Loading FAISS index...\")\n",
    "        start_time = time.time()\n",
    "        if not os.path.exists(self.config.faiss_index_path):\n",
    "            raise FileNotFoundError(f\"FAISS index not found at: {self.config.faiss_index_path}\\n\"\n",
    "                                    \"Please check your RAGConfig.faiss_index_path and ensure \"\n",
    "                                    \"you've downloaded 'faiss_vector_store' folder from Drive.\")\n",
    "        \n",
    "        self.faiss_index = faiss.read_index(self.config.faiss_index_path)\n",
    "        print(f\"✅ FAISS index loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"📊 Index contains {self.faiss_index.ntotal} vectors\")\n",
    "        \n",
    "        # Load chunk metadata\n",
    "        print(\"📥 Loading chunk metadata...\")\n",
    "        if not os.path.exists(self.config.chunk_metadata_path):\n",
    "            raise FileNotFoundError(f\"Chunk metadata not found at: {self.config.chunk_metadata_path}\\n\"\n",
    "                                    \"Please check your RAGConfig.chunk_metadata_path and ensure \"\n",
    "                                    \"you've downloaded 'faiss_vector_store' folder from Drive.\")\n",
    "        with open(self.config.chunk_metadata_path, 'r', encoding='utf-8') as f:\n",
    "            self.chunk_metadata = json.load(f)\n",
    "        \n",
    "        # Load vector store metadata (optional, but good for tracking)\n",
    "        print(\"📥 Loading vector store metadata...\")\n",
    "        if not os.path.exists(self.config.vector_store_metadata_path):\n",
    "            print(f\"⚠️ Vector store metadata not found at: {self.config.vector_store_metadata_path}\\n\"\n",
    "                  \"Proceeding without it, but consider checking the path.\")\n",
    "            self.vector_store_metadata = {}\n",
    "        else:\n",
    "            with open(self.config.vector_store_metadata_path, 'r', encoding='utf-8') as f:\n",
    "                self.vector_store_metadata = json.load(f)\n",
    "            \n",
    "        print(f\"✅ Loaded metadata for {len(self.chunk_metadata)} chunks\")\n",
    "        print(\"🚀 Dense Retriever initialized successfully!\")\n",
    "        \n",
    "    def embed_query(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Embed a query using the same model used for documents\"\"\"\n",
    "        # Add query prefix for optimal e5 performance as per model's guidelines\n",
    "        prefixed_query = f\"query: {query}\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Ensure embedding is float32 as FAISS expects this\n",
    "        embedding = self.embedding_model.encode([prefixed_query], normalize_embeddings=True).astype(np.float32)\n",
    "        embed_time_ms = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "        \n",
    "        print(f\"⚡ Query embedded in {embed_time_ms:.1f}ms\")\n",
    "        return embedding[0] # Return the single embedding vector\n",
    "    \n",
    "    def search_similar_chunks(self, query_embedding: np.ndarray) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for similar chunks using FAISS\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform similarity search\n",
    "        # query_embedding must be 2D array: (1, embedding_dim)\n",
    "        scores, indices = self.faiss_index.search(\n",
    "            query_embedding.reshape(1, -1), \n",
    "            self.config.top_k_dense # Retrieve top_k_dense chunks\n",
    "        )\n",
    "        \n",
    "        search_time_ms = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "        print(f\"🔍 FAISS search completed in {search_time_ms:.1f}ms\")\n",
    "        \n",
    "        # Prepare results, filtering by similarity threshold\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            # Check if index is valid and score meets threshold\n",
    "            if idx < len(self.chunk_metadata) and score >= self.config.similarity_threshold:\n",
    "                chunk_data = self.chunk_metadata[idx].copy()\n",
    "                chunk_data['similarity_score'] = float(score)\n",
    "                chunk_data['retrieval_rank'] = i + 1 # Rank from FAISS search\n",
    "                results.append(chunk_data)\n",
    "            # If score is below threshold, stop adding more results (since FAISS returns sorted by score)\n",
    "            elif score < self.config.similarity_threshold:\n",
    "                break\n",
    "        \n",
    "        print(f\"📋 Retrieved {len(results)} relevant chunks (min threshold: {self.config.similarity_threshold:.2f})\")\n",
    "        return results\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Main retrieval method combining embedding and search\"\"\"\n",
    "        print(f\"\\n\" + \"-\"*80)\n",
    "        print(f\"🔍 RETRIEVAL PHASE FOR QUERY: '{query[:100]}{'...' if len(query) > 100 else ''}'\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Embed query\n",
    "        query_embedding = self.embed_query(query)\n",
    "        \n",
    "        # Search for similar chunks\n",
    "        results = self.search_similar_chunks(query_embedding)\n",
    "        \n",
    "        # Log results summary (optional, but useful for debugging retrieval)\n",
    "        if results:\n",
    "            print(f\"\\n📊 TOP {len(results)} RETRIEVED CHUNKS (Summary):\")\n",
    "            for i, result_chunk in enumerate(results):\n",
    "                file_name = Path(result_chunk.get('source_file', 'Unknown')).name\n",
    "                print(f\"  {result_chunk['retrieval_rank']}. File: {file_name} (Score: {result_chunk['similarity_score']:.4f})\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No chunks retrieved for this query based on set parameters.\")\n",
    "\n",
    "        return results\n",
    "\n",
    "class LLMGenerator:\n",
    "    \"\"\"Handles LLM generation using LLaMA 3.2-1B\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.llm = None\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the LLM\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🤖 INITIALIZING LLM GENERATOR\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if not LLAMA_CPP_AVAILABLE:\n",
    "            print(\"⚠️  llama-cpp-python not available. LLM generation will be simulated.\")\n",
    "            return\n",
    "            \n",
    "        if not os.path.exists(self.config.llm_model_path):\n",
    "            print(f\"⚠️  LLM model not found at: {self.config.llm_model_path}\")\n",
    "            print(\"📝 Please download a LLaMA 3.2-1B GGUF model and update the RAGConfig.llm_model_path.\")\n",
    "            print(\"   Using simulation mode for LLM generation.\")\n",
    "            return\n",
    "        \n",
    "        print(\"📥 Loading LLaMA 3.2-1B GGUF model (this can take several seconds)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # n_gpu_layers=-1 attempts to offload all layers to GPU if available and llama-cpp-python[cuda] is installed\n",
    "            self.llm = Llama(\n",
    "                model_path=self.config.llm_model_path,\n",
    "                n_ctx=self.config.context_length,\n",
    "                n_threads=os.cpu_count(),  # Use all available CPU cores\n",
    "                verbose=False, # Set to True for verbose LLM loading output\n",
    "                n_gpu_layers=-1 # Try to use all GPU layers if available\n",
    "            )\n",
    "            print(f\"✅ LLM loaded in {time.time() - start_time:.2f} seconds\")\n",
    "            print(\"🚀 LLM Generator initialized successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading LLM from {self.config.llm_model_path}: {e}\")\n",
    "            print(\"   Ensure the GGUF model is valid and the path is correct.\")\n",
    "            print(\"   If using GPU, ensure llama-cpp-python[cuda] is installed and compatible drivers are present.\")\n",
    "            print(\"   Using simulation mode for LLM generation.\")\n",
    "            self.llm = None\n",
    "    \n",
    "    def format_prompt(self, query: str, retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Format the prompt with retrieved context for LLaMA 3.2 Instruct\"\"\"\n",
    "        # Build context from retrieved chunks\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(retrieved_chunks):\n",
    "            source_file_name = Path(chunk.get('source_file', 'Unknown Document')).name\n",
    "            text = chunk.get('text', '')\n",
    "            score = chunk.get('similarity_score', 0.0)\n",
    "            \n",
    "            context_parts.append(\n",
    "                f\"<document id={i+1} source={source_file_name} relevance={score:.3f}>\\n{text}\\n</document>\"\n",
    "            )\n",
    "        \n",
    "        context_string = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Enhanced LLaMA 3.2 Instruct format with more robust instructions\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an intelligent AI assistant that provides comprehensive and helpful answers. Your primary task is to answer questions using the provided context documents, but you should also apply your knowledge and reasoning to give complete, useful responses.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. **PRIMARY SOURCE**: Use information from the provided context documents as your main source\n",
    "2. **CITE SOURCES**: Always cite document sources using format [Document X: filename.ext] when referencing specific information\n",
    "3. **BE COMPREHENSIVE**: If the documents provide partial information, supplement with your own knowledge while clearly distinguishing between document-based and general knowledge\n",
    "4. **BE DIRECT**: Give clear, actionable answers rather than overly cautious responses\n",
    "5. **NO RELEVANT DOCUMENTS**: If NO documents contain relevant information, respond with: \"NO RELEVANT INFORMATION FOUND in the provided documents for this query.\"\n",
    "6. **PARTIAL INFORMATION**: If documents contain some relevant info but not a complete answer, use what's available and supplement thoughtfully\n",
    "7. **SYNTHESIZE**: Combine information from multiple documents when applicable\n",
    "8. **BE HELPFUL**: Your goal is to be maximally helpful to the user while being accurate\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Context Documents:\n",
    "{context_string}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive and helpful answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def generate_response(self, query: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response using LLM\"\"\"\n",
    "        print(f\"\\n\" + \"-\"*80)\n",
    "        print(f\"🤖 GENERATION PHASE\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        if not retrieved_chunks:\n",
    "            print(\"⚠️ No relevant chunks were retrieved. Generating rigid 'no information found' response.\")\n",
    "            return {\n",
    "                'response': \"NO RELEVANT INFORMATION FOUND in the provided documents for this query.\",\n",
    "                'sources': [],\n",
    "                'generation_time': 0,\n",
    "                'token_count': 0,\n",
    "                'simulated': True,\n",
    "                'no_context': True\n",
    "            }\n",
    "        \n",
    "        # Format prompt\n",
    "        prompt = self.format_prompt(query, retrieved_chunks)\n",
    "        \n",
    "        if self.llm is None:\n",
    "            # Enhanced simulation mode with more realistic responses\n",
    "            print(\"🔄 Simulating LLM response (llama-cpp-python not loaded or model not found)...\")\n",
    "            time.sleep(1.5)  # Simulate processing time\n",
    "            \n",
    "            # Get unique source filenames for simulated response\n",
    "            sources_list = sorted(list(set(Path(chunk.get('source_file', 'Unknown')).name for chunk in retrieved_chunks)))\n",
    "            \n",
    "            # More realistic simulated response\n",
    "            simulated_response = f\"\"\"Based on the retrieved document context, I can provide information related to your query: \"{query}\".\n",
    "\n",
    "**Sources Found**: {len(retrieved_chunks)} relevant document chunks from: {', '.join(sources_list)}\n",
    "\n",
    "**Key Information**: The documents contain relevant information with similarity scores ranging from {min(chunk.get('similarity_score', 0) for chunk in retrieved_chunks):.3f} to {max(chunk.get('similarity_score', 0) for chunk in retrieved_chunks):.3f}.\n",
    "\n",
    "[SIMULATED RESPONSE - Install llama-cpp-python and download the LLaMA 3.2-1B GGUF model for actual AI-generated answers]\n",
    "\n",
    "**Answer**: [The actual LLM would provide a comprehensive answer here synthesizing the document content with additional context and reasoning]\n",
    "\"\"\"\n",
    "            print(f\"✅ Simulated response generated in 1.5 seconds.\")\n",
    "            return {\n",
    "                'response': simulated_response,\n",
    "                'sources': sources_list,\n",
    "                'generation_time': 1.5,\n",
    "                'token_count': len(simulated_response.split()), # Rough estimate\n",
    "                'simulated': True\n",
    "            }\n",
    "        \n",
    "        # Real LLM generation\n",
    "        print(f\"🧠 Calling LLaMA 3.2-1B for generation...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Use 'create_completion' for simpler text generation if 'chat.completion' is not desired\n",
    "            output = self.llm.create_completion(\n",
    "                prompt,\n",
    "                max_tokens=self.config.max_tokens,\n",
    "                temperature=self.config.temperature,\n",
    "                top_p=self.config.top_p,\n",
    "                stop=[\"<|eot_id|>\"], # Stop sequence for LLaMA 3.2 instruct format\n",
    "                echo=False # Do not echo the prompt in the response\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            response_text = output['choices'][0]['text'].strip()\n",
    "            \n",
    "            # Extract unique source filenames from the retrieved chunks for metadata\n",
    "            sources_list = sorted(list(set(Path(chunk.get('source_file', 'Unknown')).name for chunk in retrieved_chunks)))\n",
    "            \n",
    "            print(f\"✅ LLM Response generated in {generation_time:.2f} seconds\")\n",
    "            print(f\"📊 Tokens generated: {output['usage']['completion_tokens']}\")\n",
    "            \n",
    "            return {\n",
    "                'response': response_text,\n",
    "                'sources': sources_list,\n",
    "                'generation_time': generation_time,\n",
    "                'token_count': output['usage']['completion_tokens'],\n",
    "                'simulated': False\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during LLM generation: {e}\")\n",
    "            print(\"   Ensure the prompt fits within context length and model is loaded correctly.\")\n",
    "            return {\n",
    "                'response': f\"An error occurred during response generation. Please check the logs. Error: {str(e)}\",\n",
    "                'sources': [],\n",
    "                'generation_time': 0,\n",
    "                'token_count': 0,\n",
    "                'error': True,\n",
    "                'simulated': False\n",
    "            }\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"Complete RAG Pipeline combining retrieval and generation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig = None):\n",
    "        self.config = config or RAGConfig()\n",
    "        self.retriever = DenseRetriever(self.config)\n",
    "        self.generator = LLMGenerator(self.config)\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize both retriever and generator\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🚀 INITIALIZING COMPLETE RAG PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        self.retriever.initialize()\n",
    "        self.generator.initialize()\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ RAG PIPELINE READY!\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def save_pipeline(self, filepath: str = \"rag_pipeline.pkl\"):\n",
    "        \"\"\"Save the initialized pipeline to a file\"\"\"\n",
    "        print(f\"\\n💾 Saving RAG pipeline to {filepath}...\")\n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "            print(f\"✅ RAG pipeline saved successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving pipeline: {e}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_pipeline(cls, filepath: str = \"rag_pipeline.pkl\"):\n",
    "        \"\"\"Load a pre-initialized pipeline from a file\"\"\"\n",
    "        print(f\"\\n📥 Loading RAG pipeline from {filepath}...\")\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                pipeline = pickle.load(f)\n",
    "            print(f\"✅ RAG pipeline loaded successfully!\")\n",
    "            return pipeline\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading pipeline: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a complete RAG query\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 5: Dense Retrieval\n",
    "        retrieved_chunks = self.retriever.retrieve(question)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        # Step 6: LLM Generation\n",
    "        generation_start = time.time()\n",
    "        result = self.generator.generate_response(question, retrieved_chunks)\n",
    "        generation_time = time.time() - generation_start\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Compile final result\n",
    "        final_result = {\n",
    "            'query': question,\n",
    "            'retrieved_chunks_count': len(retrieved_chunks),\n",
    "            'retrieval_time': retrieval_time,\n",
    "            'generation_time': generation_time,\n",
    "            'total_time': total_time,\n",
    "            'response': result['response'],\n",
    "            'sources': result['sources'], # Unique source filenames\n",
    "            'chunk_details': retrieved_chunks, # Full data for each retrieved chunk\n",
    "            'no_context': result.get('no_context', False) # Flag for when no relevant docs found\n",
    "        }\n",
    "        \n",
    "        return final_result\n",
    "\n",
    "def setup_rag_pipeline():\n",
    "    \"\"\"Setup and initialize the RAG pipeline, then save it for later use\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"🚀 RAG PIPELINE SETUP AND INITIALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # --- IMPORTANT LOCAL SETUP STEPS ---\n",
    "    # 1. Ensure required libraries are installed:\n",
    "    #    pip install numpy faiss-cpu sentence-transformers llama-cpp-python\n",
    "    #    (Use llama-cpp-python[cuda] if you have compatible GPU for LLM)\n",
    "    # 2. Download FAISS vector store folder:\n",
    "    #    Download 'faiss_vector_store' from your Google Drive and place it\n",
    "    #    in the same directory as this script.\n",
    "    # 3. Download LLaMA 3.2-1B GGUF model:\n",
    "    #    Find 'llama-3.2-1b-instruct-q4_k_m.gguf' (or similar) on Hugging Face.\n",
    "    #    Create a 'models/' folder in the same directory as this script,\n",
    "    #    and place the GGUF file inside it: models/llama-3.2-1b-instruct-q4_k_m.gguf\n",
    "    #    Ensure the 'llm_model_path' in RAGConfig exactly matches this path and filename.\n",
    "    # ---\n",
    "    \n",
    "    # Initialize configuration\n",
    "    config = RAGConfig()\n",
    "    \n",
    "    # Initialize RAG pipeline\n",
    "    rag = RAGPipeline(config)\n",
    "    \n",
    "    try:\n",
    "        # Load all models and data\n",
    "        rag.initialize()\n",
    "        \n",
    "        # Save the initialized pipeline for later use\n",
    "        rag.save_pipeline()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ SETUP COMPLETE!\")\n",
    "        print(\"The RAG pipeline has been initialized and saved.\")\n",
    "        print(\"You can now run the query session script.\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return rag\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during setup: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_rag_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32631c99",
   "metadata": {},
   "source": [
    "## NOW write you query it will take generally 50 seconds to respond\n",
    "\n",
    "### Ask as many queries you want the output will properly display all the information, make sure to click scrollable element in output so that you able to see all the output parts \n",
    "\n",
    "### or copy the output of the cell and paste it to word if you donot know what scrollabel element is \n",
    "\n",
    "### Write exit when you want to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "def load_rag_pipeline(filepath: str = \"rag_pipeline.pkl\"):\n",
    "    \"\"\"Load a pre-initialized RAG pipeline from file\"\"\"\n",
    "    print(f\"\\n📥 Loading RAG pipeline from {filepath}...\")\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pipeline = pickle.load(f)\n",
    "        print(f\"✅ RAG pipeline loaded successfully!\")\n",
    "        return pipeline\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Pipeline file not found at {filepath}\")\n",
    "        print(\"Please run the setup script first to initialize and save the pipeline.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading pipeline: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_query_result(result: Dict[str, Any]):\n",
    "    \"\"\"Display the results of a RAG query in a formatted way\"\"\"\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL RAG RESPONSE FOR: '{result['query']}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display the generated response\n",
    "    print(f\"\\n📝 Generated Response:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['response'])\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Display details of the retrieved documents\n",
    "    if result['chunk_details']:\n",
    "        print(f\"\\n📚 Retrieved Documents (Top {result['retrieved_chunks_count']}):\")\n",
    "        print(\"-\" * 70)\n",
    "        for j, chunk in enumerate(result['chunk_details']):\n",
    "            # Path().name extracts just the filename from the full path\n",
    "            file_name = Path(chunk.get('source_file', 'Unknown')).name \n",
    "            print(f\"  {j+1}. Document: {file_name}\")\n",
    "            print(f\"     Category: {chunk.get('category', 'Unknown')}\")\n",
    "            print(f\"     Relevance Score: {chunk.get('similarity_score', 0):.4f}\")\n",
    "            # You can uncomment the line below if you want a small text preview of each chunk\n",
    "            # print(f\"     Preview: {chunk.get('text', '')[:150]}...\")\n",
    "            print(\"-\" * 70) # Separator for each document\n",
    "    else:\n",
    "        print(\"\\n⚠️ No relevant documents were retrieved for this query.\")\n",
    "    \n",
    "    # Display overall pipeline metrics\n",
    "    print(f\"\\n📊 Pipeline Metrics:\")\n",
    "    print(f\"  • Retrieval Time: {result['retrieval_time']:.3f} seconds\")\n",
    "    print(f\"  • Generation Time: {result['generation_time']:.3f} seconds\")\n",
    "    print(f\"  • Total Time: {result['total_time']:.3f} seconds\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\") # End of query summary for better spacing\n",
    "\n",
    "def run_interactive_session(rag_pipeline):\n",
    "    \"\"\"Run a simple interactive query session with the loaded RAG pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 RAG PIPELINE INTERACTIVE SESSION\")\n",
    "    print(\"Type 'exit' or 'quit' to end the session.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    while True:\n",
    "        # Simple prompt for user query\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        user_query = input(\"🤔 Your question: \").strip()\n",
    "        \n",
    "        # Check for exit commands\n",
    "        if user_query.lower() in ['exit', 'quit']:\n",
    "            print(\"\\nGoodbye! 👋\")\n",
    "            break\n",
    "            \n",
    "        # Skip empty queries\n",
    "        if not user_query:\n",
    "            print(\"⚠️ Please enter a question.\")\n",
    "            continue\n",
    "            \n",
    "        # Process the query\n",
    "        try:\n",
    "            print(\"\\n🔄 Processing your question...\")\n",
    "            result = rag_pipeline.query(user_query)\n",
    "            \n",
    "            # Display just the response cleanly\n",
    "            print(f\"\\n💡 **Answer:**\")\n",
    "            print(\"-\" * 50)\n",
    "            print(result['response'])\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Show sources if available\n",
    "            if result['sources']:\n",
    "                print(f\"\\n📚 **Sources:** {', '.join(result['sources'])}\")\n",
    "            \n",
    "            # Show timing\n",
    "            print(f\"⏱️ **Response time:** {result['total_time']:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            print(\"Please try asking your question differently.\")\n",
    "\n",
    "def run_single_query(rag_pipeline, query: str):\n",
    "    \"\"\"Run a single query without interactive mode\"\"\"\n",
    "    print(f\"\\n🔍 Processing single query: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        result = rag_pipeline.query(query)\n",
    "        display_query_result(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing query: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for the query session\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"🚀 RAG PIPELINE QUERY SESSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load the pre-initialized pipeline\n",
    "    rag_pipeline = load_rag_pipeline()\n",
    "    \n",
    "    if rag_pipeline is None:\n",
    "        print(\"\\n❌ Could not load RAG pipeline.\")\n",
    "        print(\"Please run the setup script first:\")\n",
    "        print(\"  python rag_setup.py\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n✅ RAG pipeline loaded and ready!\")\n",
    "    \n",
    "    # Run the interactive session\n",
    "    run_interactive_session(rag_pipeline)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
