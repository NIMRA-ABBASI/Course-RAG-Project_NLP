Similarity Measure â€” Overview



A similarity measure is a mathematical function or algorithm that quantifies how alike two objects are. In the context of information retrieval (IR) and natural language processing (NLP), these objects are often represented as vectors (lists of numbers). For example, a user's search query and a document from a collection can both be transformed into vector representations, and a similarity measure can then be used to determine how closely related they are.

________________________________________

Why use similarity measures?



Similarity measures are crucial in various applications for several reasons:

*   To rank documents: They allow systems to order documents based on how relevant they are to a given query. Documents with higher similarity scores to the query are ranked higher.

*   To filter results: A similarity threshold can be set, so only documents exceeding that level of similarity to a query or another document are considered or returned.

*   To improve search accuracy: By providing a quantitative way to assess relevance, similarity measures help in delivering more accurate and pertinent search results.

*   For clustering: Grouping similar items together.

*   For recommendations: Suggesting items similar to what a user has liked or viewed.

*   For anomaly detection: Identifying items that are dissimilar to the norm.

________________________________________

Common Similarity Measures



1. Jaccard Similarity

*   Measures the overlap between two sets of items. It is calculated as the size of the intersection of the sets divided by the size of their union.

*   Formula:

    Jaccard(Set_A, Set_B) = |Set_A intersect Set_B| / |Set_A union Set_B|

    Where:

        `|Set_A intersect Set_B|` is the number of elements common to both Set_A and Set_B.

        `|Set_A union Set_B|` is the total number of unique elements in either Set_A or Set_B.

*   Useful for comparing sets of words (e.g., unique keywords in documents, items in user profiles).

*   Range: 0 (indicating no overlap between the sets) to 1 (indicating the sets are identical).

*   Example:

    Doc1 words = {apple, orange, banana}

    Doc2 words = {apple, banana, kiwi}

    Intersection (Doc1, Doc2) = {apple, banana} -> Size = 2

    Union (Doc1, Doc2) = {apple, orange, banana, kiwi} -> Size = 4

    Jaccard Similarity = 2 / 4 = 0.5

________________________________________

2. Cosine Similarity

*   Measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. It focuses on the orientation of the vectors, not their magnitude.

*   Formula:

    Cosine_Similarity(Vector_A, Vector_B) = (Vector_A . Vector_B) / (||Vector_A|| * ||Vector_B||)

    Where:

        `Vector_A . Vector_B` is the dot product of Vector_A and Vector_B.

        `||Vector_A||` is the magnitude (or Euclidean norm/length) of Vector_A.

        `||Vector_B||` is the magnitude of Vector_B.

*   Values range from -1 (exactly opposite) to 1 (exactly the same direction). For vectors with non-negative components (like TF-IDF scores or word counts), the range is typically 0 (orthogonal, no similarity) to 1 (same direction, maximal similarity).

*   Popular in text similarity tasks, especially with TF-IDF vectors or word embedding vectors.

*   Example:

    Vector A = [1, 2, 3]

    Vector B = [2, 3, 4]

    Dot Product (A . B) = (1*2) + (2*3) + (3*4) = 2 + 6 + 12 = 20

    Magnitude of A (||A||) = sqrt(1^2 + 2^2 + 3^2) = sqrt(1 + 4 + 9) = sqrt(14)

    Magnitude of B (||B||) = sqrt(2^2 + 3^2 + 4^2) = sqrt(4 + 9 + 16) = sqrt(29)

    Cosine Similarity = 20 / (sqrt(14) * sqrt(29))

                      = 20 / (approx 3.742 * approx 5.385)

                      = 20 / (approx 20.15)

                      which is approximately 0.992

________________________________________

3. Euclidean Distance

*   Measures the straight-line distance (or "as the crow flies" distance) between two points (or vectors) in Euclidean space.

*   Formula:

    Euclidean_Distance(Vector_A, Vector_B) = sqrt( sum_for_each_dimension_i ( (A_i - B_i)^2 ) )

    Where A_i and B_i are the components of vectors A and B in the i-th dimension.

*   A lower distance indicates higher similarity (i.e., the points are closer). A distance of 0 means the points are identical.

*   Often used in clustering algorithms (like K-means) and for comparing vectors with continuous-valued features. To use it as a similarity measure (where higher is better), it often needs to be transformed (e.g., 1 / (1 + distance) or exp(-distance)).

________________________________________

4. Edit Distance (Specifically Levenshtein Distance)

*   Measures the similarity between two strings by counting the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.

*   Useful for comparing short texts, names, identifying spelling errors, or in bioinformatics for DNA sequences.

*   A lower edit distance means the strings are more similar. An edit distance of 0 means the strings are identical.

*   Example:

    To transform "kitten" into "sitting":

    1. k -> s (substitution: "sitten")

    2. e -> i (substitution: "sittin")

    3.   -> g (insertion: "sitting")

    The Levenshtein distance is 3.

________________________________________

5. Word Embedding Similarity

*   Compares word or document vectors that are generated from word embedding models like Word2Vec, GloVe, FastText, or sentence/document embedding models like Sentence-BERT.

*   These models learn dense vector representations where words or texts with similar meanings are located close to each other in the vector space.

*   The similarity between these embedding vectors is usually measured using Cosine Similarity.

*   This approach captures semantic similarity (meaning-based similarity) that goes beyond simple surface-level word overlap. For example, "king" and "queen" would have high similarity.

________________________________________

6. Semantic Similarity Measures

*   A broader category of measures that aim to capture the similarity in meaning or semantic content between pieces of text, rather than just lexical overlap.

*   Example: Two sentences that use different words but convey a similar idea can achieve a high semantic similarity score.

*   Methods include:

    *   Knowledge-based measures: Utilizing structured knowledge bases like WordNet (which groups words into sets of synonyms called synsets and defines relationships between them).

    *   Corpus-based measures: Statistical methods derived from large text corpora, including distributional similarity (words appearing in similar contexts are similar) and techniques like Latent Semantic Analysis (LSA).

    *   Embedding-based measures: As described in "Word Embedding Similarity," using vectors from models like BERT, ELMo, Universal Sentence Encoder.

    *   Transformer models: Modern deep learning models like BERT can directly provide similarity scores or embeddings for comparison that capture rich contextual meaning.

________________________________________

7. Topic Modeling-based Similarity

*   Uses topic distributions derived from topic models like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF).

*   In these models, documents are represented as a mixture of underlying topics, and each topic is a distribution over words.

*   So, each document can be represented as a vector where each component is the probability or proportion of a particular topic in that document.

*   Similarity between documents is then computed by comparing their topic distributions (e.g., using Cosine Similarity, Jensen-Shannon divergence, or Hellinger distance on the topic vectors).

*   Useful for comparing documents at a conceptual or thematic level rather than relying on exact word matches.