Confusion Matrix Example: Email Spam Detection

Confusion Matrix Example: Email Spam Detection

Scenario:

A spam detection model has been developed and was tested on a dataset of 1000 emails. The goal is to classify emails as either "Spam" (positive class) or "Not Spam" (negative class, i.e., legitimate email).

Test Results Overview:

*   Total emails tested: 1000

*   Actual spam emails in the dataset: 100

*   Actual legitimate (not spam) emails in the dataset: 900

Confusion Matrix:

The results of the model's predictions are summarized in the following confusion matrix:

                Predicted: Spam   Predicted: Not Spam   | Actual Totals

              +-----------------+---------------------+

Actual: Spam  |       85 (TP)   |        15 (FN)      |     100

              +-----------------+---------------------+

Actual: Not   |       25 (FP)   |       875 (TN)      |     900

Spam          +-----------------+---------------------+

              | Predicted Totals|                     |

              |      110        |        890          |    1000 (Grand Total)





Breaking down the numbers:

True Positives (TP): 85 spam emails correctly identified as spam

True Negatives (TN): 875 legitimate emails correctly identified as legitimate

False Positives (FP): 25 legitimate emails incorrectly flagged as spam

False Negatives (FN): 15 spam emails that slipped through as legitimate



Calculated Performance Metrics:



1.  Accuracy:

    *   Formula: (TP + TN) / (TP + FP + FN + TN)

    *   Calculation: (85 + 875) / (85 + 25 + 15 + 875) = 960 / 1000 = 0.96

    *   Interpretation: 96% or 0.96. The model correctly classified 960 out of the 1000 emails overall.



2.  Precision (for the "Spam" class):

    *   Formula: TP / (TP + FP)

    *   Calculation: 85 / (85 + 25) = 85 / 110 ≈ 0.773

    *   Interpretation: Approximately 77.3%. Of all the emails that the model predicted as spam, 77.3% were actually spam. This means that about 22.7% (100% - 77.3%) of emails flagged as spam were actually legitimate (false alarms).



3.  Recall (Sensitivity, True Positive Rate for the "Spam" class):

    *   Formula: TP / (TP + FN)

    *   Calculation: 85 / (85 + 15) = 85 / 100 = 0.85

    *   Interpretation: 85%. The model successfully caught 85% of all the actual spam emails present in the dataset. However, it missed 15% of the spam emails.



4.  F1-Score (for the "Spam" class):

    *   Formula: 2 * (Precision * Recall) / (Precision + Recall)

    *   Calculation (using rounded precision 0.77 for simplicity as in original): 2 * (0.77 * 0.85) / (0.77 + 0.85) = 2 * 0.6545 / 1.62 = 1.309 / 1.62 ≈ 0.808

    *   (Using more precise precision 0.773: 2 * (0.773 * 0.85) / (0.773 + 0.85) = 2 * 0.65705 / 1.623 = 1.3141 / 1.623 ≈ 0.809)

    *   Interpretation: Approximately 81%. This balanced metric, which considers both precision and recall, indicates that the model has a reasonably good overall performance in identifying spam, though there's room for improvement, especially in precision.





Calculated Metrics:

Accuracy: (85 + 875) / 1000 = 0.96 or 96% The model correctly classified 960 out of 1000 emails.

Precision: 85 / (85 + 25) = 0.77 or 77% Of all emails flagged as spam, 77% were actually spam. This means 23% of flagged emails were false alarms.

Recall: 85 / (85 + 15) = 0.85 or 85% The model caught 85% of all actual spam emails, but missed 15% of spam.

F1-Score: 2 × (0.77 × 0.85) / (0.77 + 0.85) = 0.81 or 81% This balanced metric shows the model performs reasonably well overall.



The model shows good overall performance with 96% accuracy, but the 77% precision means roughly 1 in 4 spam-flagged emails are actually legitimate.

