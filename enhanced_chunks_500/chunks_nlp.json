[
  {
    "chunk_id": 91,
    "chunk_hash": "68cba46d1913",
    "text": "Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. It combines linguistics (how language works) with computer science to create intelligent systems that can process natural (human) languages like English, Urdu, Chinese, etc. Simple Real-World Examples Example 1: Chatbot\nYou type: “How are you?”\nThe computer replies: “I’m doing great, thank you!”\nThis shows NLP understanding your input and generating a human-like response. Example 2: Translation\nYou enter: “Good morning”\nNLP translates it to: “صباح الخير” (Arabic)\nHere, the system understands the meaning and rewrites it in another language. Example 3: Sentiment Analysis\nYou write a review: “The product is terrible.”\nThe system reads your words and detects negative emotion. It understands the feeling behind the words. Example 4: Voice Assistant\nYou say: “Play some music.”\nYour device understands your voice and starts playing songs. This combines speech recognition and NLP to carry out your request. Example 5: Email Filtering\nYou receive lots of emails. NLP helps automatically move spam messages to the spam folder, and organize others into categories like “Promotions,” “Updates,” etc. Example 6: Search Engine\nYou type: “Best pizza in Islamabad”\nThe system understands what you're looking for and shows nearby pizza places. It doesn’t just look for keywords — it understands the intent.",
    "enhanced_text": "[NLP] Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. It combines linguistics (how language works) with computer science to create intelligent systems that can process natural (human) languages like English, Urdu, Chinese, etc. Simple Real-World Examples Example 1: Chatbot\nYou type: “How are you?”\nThe computer replies: “I’m doing great, thank you!”\nThis shows NLP understanding your input and generating a human-like response. Example 2: Translation\nYou enter: “Good morning”\nNLP translates it to: “صباح الخير” (Arabic)\nHere, the system understands the meaning and rewrites it in another language. Example 3: Sentiment Analysis\nYou write a review: “The product is terrible.”\nThe system reads your words and detects negative emotion. It understands the feeling behind the words. Example 4: Voice Assistant\nYou say: “Play some music.”\nYour device understands your voice and starts playing songs. This combines speech recognition and NLP to carry out your request. Example 5: Email Filtering\nYou receive lots of emails. NLP helps automatically move spam messages to the spam folder, and organize others into categories like “Promotions,” “Updates,” etc. Example 6: Search Engine\nYou type: “Best pizza in Islamabad”\nThe system understands what you're looking for and shows nearby pizza places. It doesn’t just look for keywords — it understands the intent.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(a) Natural Language Processing.txt",
    "file_name": "lec1-(a) Natural Language Processing.txt",
    "position_in_document": 13,
    "filename_keywords": [
      "processing",
      "natural",
      "language",
      "lec1"
    ],
    "content_keywords": [
      "world examples",
      "natural language processing",
      "simple real",
      "artificial intelligence",
      "english",
      "urdu",
      "chinese",
      "nlp"
    ],
    "all_keywords": [
      "world examples",
      "processing",
      "natural language processing",
      "simple real",
      "artificial intelligence",
      "english",
      "language",
      "urdu",
      "chinese",
      "nlp",
      "natural",
      "lec1"
    ],
    "keyword_string": "world examples processing natural language processing simple real artificial intelligence english language urdu chinese nlp natural lec1",
    "token_count": 317,
    "word_count": 221,
    "sentence_count": 12,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6971608832807571,
    "avg_sentence_length": 18.416666666666668
  },
  {
    "chunk_id": 92,
    "chunk_hash": "85db01269a3a",
    "text": "Why is NLP Confusing (Even for Computers)? Natural Language (NL), or human language, is incredibly complex, messy, and full of exceptions. Even humans sometimes struggle to understand it—especially when learning a new language. So, for computers that don’t think or feel like humans, understanding language becomes even more challenging. Key Reasons Why NLP is Confusing for Computers: Language is Ambiguous\nWords or sentences can have multiple meanings, depending on how and where they’re used. “He saw the man with the telescope.”\nDoes this mean he used a telescope to see the man or the man had a telescope? A computer can’t easily decide without context. Same Words, Different Meanings (Polysemy)\nMany words in English have multiple meanings. “Bank” can mean a financial institution or the side of a river. The sentence “I’m going to the bank” isn’t clear unless the full context is known. Different Words, Same Meaning (Synonyms)\nDifferent words can mean the same thing, but not always in the exact same way. “Begin” and “Start” are similar, but sometimes only one fits naturally in a sentence. “Buy” and “Purchase” are similar, but “purchase” sounds more formal. Language Depends on Context\nThe meaning of a sentence often depends on what was said before, who is speaking, or where they are. “Can you pass me the salt?”\nLiterally, it’s a question about ability, but really, it’s a polite request. Idioms and Expressions\nPhrases don’t always mean what the words say. “Kick the bucket” means to die, not to literally kick something. Computers struggle with this because they analyze words literally. Word Order Matters\nA small change in word order can change the entire meaning. “The dog bit the man.” “The man bit the dog.”\nSame words, but completely different meanings. Grammar Rules Have Many Exceptions\nThere are rules in language, but there are lots of exceptions. “Go → Went” is an irregular past tense. Computers have to learn these exceptions separately — they’re not obvious. Funny Example of Language Confusion: “Time flies like an arrow.”\n“Fruit flies like a banana.” The first one means time moves quickly, but the second sounds like a joke — suggesting fruit flies enjoy bananas. Same structure, totally different meaning. This is very confusing for a machine!",
    "enhanced_text": "[NLP] Why is NLP Confusing (Even for Computers)? Natural Language (NL), or human language, is incredibly complex, messy, and full of exceptions. Even humans sometimes struggle to understand it—especially when learning a new language. So, for computers that don’t think or feel like humans, understanding language becomes even more challenging. Key Reasons Why NLP is Confusing for Computers: Language is Ambiguous\nWords or sentences can have multiple meanings, depending on how and where they’re used. “He saw the man with the telescope.”\nDoes this mean he used a telescope to see the man or the man had a telescope? A computer can’t easily decide without context. Same Words, Different Meanings (Polysemy)\nMany words in English have multiple meanings. “Bank” can mean a financial institution or the side of a river. The sentence “I’m going to the bank” isn’t clear unless the full context is known. Different Words, Same Meaning (Synonyms)\nDifferent words can mean the same thing, but not always in the exact same way. “Begin” and “Start” are similar, but sometimes only one fits naturally in a sentence. “Buy” and “Purchase” are similar, but “purchase” sounds more formal. Language Depends on Context\nThe meaning of a sentence often depends on what was said before, who is speaking, or where they are. “Can you pass me the salt?”\nLiterally, it’s a question about ability, but really, it’s a polite request. Idioms and Expressions\nPhrases don’t always mean what the words say. “Kick the bucket” means to die, not to literally kick something. Computers struggle with this because they analyze words literally. Word Order Matters\nA small change in word order can change the entire meaning. “The dog bit the man.” “The man bit the dog.”\nSame words, but completely different meanings. Grammar Rules Have Many Exceptions\nThere are rules in language, but there are lots of exceptions. “Go → Went” is an irregular past tense. Computers have to learn these exceptions separately — they’re not obvious. Funny Example of Language Confusion: “Time flies like an arrow.”\n“Fruit flies like a banana.” The first one means time moves quickly, but the second sounds like a joke — suggesting fruit flies enjoy bananas. Same structure, totally different meaning. This is very confusing for a machine!",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(b) NL is Confusing for Computers.txt",
    "file_name": "lec1-(b) NL is Confusing for Computers.txt",
    "position_in_document": 30,
    "filename_keywords": [
      "confusing",
      "computers",
      "lec1"
    ],
    "content_keywords": [
      "even",
      "computers",
      "natural language",
      "nlp",
      "why",
      "nlp confusing"
    ],
    "all_keywords": [
      "computers",
      "even",
      "natural language",
      "lec1",
      "nlp",
      "why",
      "nlp confusing",
      "confusing"
    ],
    "keyword_string": "computers even natural language lec1 nlp why nlp confusing confusing",
    "token_count": 492,
    "word_count": 372,
    "sentence_count": 26,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7560975609756098,
    "avg_sentence_length": 14.307692307692308
  },
  {
    "chunk_id": 93,
    "chunk_hash": "4e7449bb8a80",
    "text": "Common Applications of NLP NLP is everywhere around us. It powers many tools and technologies we use every day. Below are some of the most common applications of Natural Language Processing, with simple explanations. This is the process of automatically translating text from one language to another. It helps people understand content written in other languages. Translating “Good morning” from English to French → “Bonjour” NLP can shorten long documents into brief summaries while keeping the main message. It is useful for news apps, research papers, and reports. A 3-page news article gets summarized in 3 sentences. Searching (Information Retrieval) Search engines like Google use NLP to understand your query and find the most relevant web pages. It doesn’t just look for keywords; it understands the meaning of your question. You search: “Best restaurants in Lahore” Google shows local listings, not just pages that mention the word “restaurant.” This refers to systems that can read a text or understand a query and give a direct answer. Used in search engines, voice assistants, and customer support. You ask: “Who is the founder of Microsoft?” The system replies: “Bill Gates” Named Entity Recognition (NER) NER is used to find and label important pieces of information in text like names of people, places, organizations, dates, etc. From the sentence: “Apple was founded by Steve Jobs in California,” Parts-of-Speech Tagging (POS Tagging) This involves identifying the grammatical role of each word in a sentence—whether it’s a noun, verb, adjective, etc. It helps machines understand sentence structure and meaning. POS tags: “The (determiner) cat (noun) sleeps (verb)” This is used to group similar documents or texts together automatically, without labels. Very useful for organizing large sets of unstructured data like customer reviews, tweets, or news articles. All articles about sports are grouped together, while articles about politics form another group. This is the process of identifying emotions or opinions in text—whether something is positive, negative, or neutral. Used in reviews, surveys, social media, etc. “I love this phone!” → Positive sentiment “This service is terrible.” → Negative sentiment NLP systems can automatically assign a category or label to a piece of text. It’s used for spam detection, topic labeling, intent detection, etc. An email is labeled as “Spam” or “Important”",
    "enhanced_text": "[NLP] Common Applications of NLP NLP is everywhere around us. It powers many tools and technologies we use every day. Below are some of the most common applications of Natural Language Processing, with simple explanations. This is the process of automatically translating text from one language to another. It helps people understand content written in other languages. Translating “Good morning” from English to French → “Bonjour” NLP can shorten long documents into brief summaries while keeping the main message. It is useful for news apps, research papers, and reports. A 3-page news article gets summarized in 3 sentences. Searching (Information Retrieval) Search engines like Google use NLP to understand your query and find the most relevant web pages. It doesn’t just look for keywords; it understands the meaning of your question. You search: “Best restaurants in Lahore” Google shows local listings, not just pages that mention the word “restaurant.” This refers to systems that can read a text or understand a query and give a direct answer. Used in search engines, voice assistants, and customer support. You ask: “Who is the founder of Microsoft?” The system replies: “Bill Gates” Named Entity Recognition (NER) NER is used to find and label important pieces of information in text like names of people, places, organizations, dates, etc. From the sentence: “Apple was founded by Steve Jobs in California,” Parts-of-Speech Tagging (POS Tagging) This involves identifying the grammatical role of each word in a sentence—whether it’s a noun, verb, adjective, etc. It helps machines understand sentence structure and meaning. POS tags: “The (determiner) cat (noun) sleeps (verb)” This is used to group similar documents or texts together automatically, without labels. Very useful for organizing large sets of unstructured data like customer reviews, tweets, or news articles. All articles about sports are grouped together, while articles about politics form another group. This is the process of identifying emotions or opinions in text—whether something is positive, negative, or neutral. Used in reviews, surveys, social media, etc. “I love this phone!” → Positive sentiment “This service is terrible.” → Negative sentiment NLP systems can automatically assign a category or label to a piece of text. It’s used for spam detection, topic labeling, intent detection, etc. An email is labeled as “Spam” or “Important”",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(c) Common Applications.txt",
    "file_name": "lec1-(c) Common Applications.txt",
    "position_in_document": 36,
    "filename_keywords": [
      "common",
      "applications",
      "lec1"
    ],
    "content_keywords": [
      "nlp",
      "nlp nlp",
      "common applications"
    ],
    "all_keywords": [
      "common",
      "nlp nlp",
      "applications",
      "nlp",
      "common applications",
      "lec1"
    ],
    "keyword_string": "common nlp nlp applications nlp common applications lec1",
    "token_count": 509,
    "word_count": 375,
    "sentence_count": 23,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7367387033398821,
    "avg_sentence_length": 16.304347826086957
  },
  {
    "chunk_id": 94,
    "chunk_hash": "787a761329b5",
    "text": "Natural Language Processing is one of the fastest-growing areas in the field of Artificial Intelligence and Data Science. With the rise of voice assistants, chatbots, search engines, translation tools, and social media analysis, companies around the world are investing in NLP technology. Many global and local companies are already working on NLP-based products and services. iManage – A successful company co-founded by an American-Pakistani, known for working with intelligent document and knowledge management. Their tools use NLP to understand legal documents, contracts, and business files. Teradata – A multinational company that uses NLP in large-scale data platforms for extracting insights from unstructured text like emails, reports, and customer feedback. Systems Limited – A Pakistani IT company involved in projects using AI and NLP, offering smart solutions for businesses in banking, telecom, and healthcare. NLP is in High Demand Across Industries Jobs in NLP are available in fields like: Healthcare – Reading medical records, voice-based diagnostics, patient feedback. Finance – Analyzing financial documents, reports, and fraud detection. Legal – Reading and summarizing long contracts and legal documents. Customer Service – Chatbots, email sorting, customer feedback analysis. E-commerce – Product recommendations, review analysis, search optimization. Education – Automated grading, summarizing lectures, student feedback analysis. In-Demand Skills for NLP Jobs To enter the NLP field, you should be familiar with: Libraries like spaCy, NLTK, HuggingFace Transformers Machine Learning & Deep Learning (e.g., using scikit-learn, TensorFlow, PyTorch) Text processing techniques (tokenization, stemming, lemmatization) Understanding of linguistics and statistics For more about NLP careers, skills, and job roles, visit:\n🔗 KnowledgeHut Blog on NLP Careers",
    "enhanced_text": "[NLP] Natural Language Processing is one of the fastest-growing areas in the field of Artificial Intelligence and Data Science. With the rise of voice assistants, chatbots, search engines, translation tools, and social media analysis, companies around the world are investing in NLP technology. Many global and local companies are already working on NLP-based products and services. iManage – A successful company co-founded by an American-Pakistani, known for working with intelligent document and knowledge management. Their tools use NLP to understand legal documents, contracts, and business files. Teradata – A multinational company that uses NLP in large-scale data platforms for extracting insights from unstructured text like emails, reports, and customer feedback. Systems Limited – A Pakistani IT company involved in projects using AI and NLP, offering smart solutions for businesses in banking, telecom, and healthcare. NLP is in High Demand Across Industries Jobs in NLP are available in fields like: Healthcare – Reading medical records, voice-based diagnostics, patient feedback. Finance – Analyzing financial documents, reports, and fraud detection. Legal – Reading and summarizing long contracts and legal documents. Customer Service – Chatbots, email sorting, customer feedback analysis. E-commerce – Product recommendations, review analysis, search optimization. Education – Automated grading, summarizing lectures, student feedback analysis. In-Demand Skills for NLP Jobs To enter the NLP field, you should be familiar with: Libraries like spaCy, NLTK, HuggingFace Transformers Machine Learning & Deep Learning (e.g., using scikit-learn, TensorFlow, PyTorch) Text processing techniques (tokenization, stemming, lemmatization) Understanding of linguistics and statistics For more about NLP careers, skills, and job roles, visit:\n🔗 KnowledgeHut Blog on NLP Careers",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(d) Job Market for NLP.txt",
    "file_name": "lec1-(d) Job Market for NLP.txt",
    "position_in_document": 22,
    "filename_keywords": [
      "nlp",
      "market",
      "job",
      "lec1"
    ],
    "content_keywords": [
      "natural language processing",
      "artificial intelligence",
      "data science",
      "nlp",
      "many",
      "with"
    ],
    "all_keywords": [
      "with",
      "natural language processing",
      "artificial intelligence",
      "data science",
      "job",
      "market",
      "nlp",
      "many",
      "lec1"
    ],
    "keyword_string": "with natural language processing artificial intelligence data science job market nlp many lec1",
    "token_count": 375,
    "word_count": 260,
    "sentence_count": 14,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6933333333333334,
    "avg_sentence_length": 18.571428571428573
  },
  {
    "chunk_id": 95,
    "chunk_hash": "7e473239893b",
    "text": "Basic Vocabulary in NLP Before building any Natural Language Processing system, it's important to understand some basic concepts and terms. These terms help us describe and process human language in a structured way. Example to Begin With: Let's take a sentence:\n“The cats are chasing mice.” Words or Tokens: \"The\", \"cats\", \"are\", \"chasing\", \"mice\"\nThese are the smallest units of meaning we can extract. A lexicon is like a smart dictionary for a language. It contains not just words and their meanings, but also: Their grammatical properties (noun, verb, etc.) Semantic relationships (synonyms, opposites) Multi-word expressions and idioms (e.g., \"kick the bucket\") So, a lexicon isn't just a word list — it's a full guide to how those words behave in a language. Vocabulary refers to the complete set of unique words found in a collection of documents or a dataset. It does not include word meanings or grammar — just the words themselves. For example:\nIf your dataset includes: Then your vocabulary is:\n[“the”, “cat”, “sleeps”, “dog”] Each word is counted only once, even if it appears multiple times. Tokenization is the process of splitting text into small parts called tokens. Tokens are usually words, but they can also be punctuation marks or subwords. Example:\nInput: “Cats are running.”\nAfter tokenization: [“Cats”, “are”, “running”, “.”] This is often the first step in most NLP tasks. Special tools called tokenizers do this job. Stemming is the process of cutting off prefixes or suffixes from words to get to their root form. It doesn't always produce a real word — just a basic stem. “Universal” → “Univers” “University” → “Univers” This is helpful when you want to treat similar forms of a word as one item. However, stemming is rough — it might cut too much (over-stemming) or too little (under-stemming). Popular stemming algorithms include: Lemmatization is similar to stemming but smarter. It uses grammar rules and a dictionary to reduce a word to its base or dictionary form, called a lemma.",
    "enhanced_text": "[NLP] Basic Vocabulary in NLP Before building any Natural Language Processing system, it's important to understand some basic concepts and terms. These terms help us describe and process human language in a structured way. Example to Begin With: Let's take a sentence:\n“The cats are chasing mice.” Words or Tokens: \"The\", \"cats\", \"are\", \"chasing\", \"mice\"\nThese are the smallest units of meaning we can extract. A lexicon is like a smart dictionary for a language. It contains not just words and their meanings, but also: Their grammatical properties (noun, verb, etc.) Semantic relationships (synonyms, opposites) Multi-word expressions and idioms (e.g., \"kick the bucket\") So, a lexicon isn't just a word list — it's a full guide to how those words behave in a language. Vocabulary refers to the complete set of unique words found in a collection of documents or a dataset. It does not include word meanings or grammar — just the words themselves. For example:\nIf your dataset includes: Then your vocabulary is:\n[“the”, “cat”, “sleeps”, “dog”] Each word is counted only once, even if it appears multiple times. Tokenization is the process of splitting text into small parts called tokens. Tokens are usually words, but they can also be punctuation marks or subwords. Example:\nInput: “Cats are running.”\nAfter tokenization: [“Cats”, “are”, “running”, “.”] This is often the first step in most NLP tasks. Special tools called tokenizers do this job. Stemming is the process of cutting off prefixes or suffixes from words to get to their root form. It doesn't always produce a real word — just a basic stem. “Universal” → “Univers” “University” → “Univers” This is helpful when you want to treat similar forms of a word as one item. However, stemming is rough — it might cut too much (over-stemming) or too little (under-stemming). Popular stemming algorithms include: Lemmatization is similar to stemming but smarter. It uses grammar rules and a dictionary to reduce a word to its base or dictionary form, called a lemma.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(e) Basic Vocabulary.txt",
    "file_name": "lec1-(e) Basic Vocabulary.txt",
    "position_in_document": 31,
    "filename_keywords": [
      "vocabulary",
      "basic",
      "lec1"
    ],
    "content_keywords": [
      "basic vocabulary",
      "natural language processing",
      "these",
      "nlp",
      "nlp before"
    ],
    "all_keywords": [
      "basic vocabulary",
      "natural language processing",
      "vocabulary",
      "basic",
      "these",
      "nlp",
      "nlp before",
      "lec1"
    ],
    "keyword_string": "basic vocabulary natural language processing vocabulary basic these nlp nlp before lec1",
    "token_count": 486,
    "word_count": 331,
    "sentence_count": 19,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6810699588477366,
    "avg_sentence_length": 17.42105263157895
  },
  {
    "chunk_id": 96,
    "chunk_hash": "bdf59c1cfcb9",
    "text": "“Universal” → “Univers” “University” → “Univers” This is helpful when you want to treat similar forms of a word as one item. However, stemming is rough — it might cut too much (over-stemming) or too little (under-stemming). Popular stemming algorithms include: Lemmatization is similar to stemming but smarter. It uses grammar rules and a dictionary to reduce a word to its base or dictionary form, called a lemma. Example:\nOriginal sentence: “The cats are chasing mice.”\nAfter lemmatization: “The cat be chase mouse.” Unlike stemming, lemmatization gives real words, and it understands context. It knows the difference between “better” and “good” or “ran” and “run”. Tools called lemmatizers perform this task. They rely on linguistics and lexical databases like WordNet. Difference: Stemming vs Lemmatization Stemming is fast and simple but may create fake or broken words. Lemmatization is slower but gives correct and meaningful words. Stemming uses fixed rules. Lemmatization uses grammar, context, and dictionary lookups. Corpus (Plural: Corpora) A corpus is a large collection of texts, often used for training or testing NLP models. Transcripts of spoken conversations Examples of known corpora: BBC News Articles Corpus You can find datasets on: Google Dataset Search Stop words are very common words in a language that don’t add much meaning by themselves. Examples in English include: is, am, are, the, a, an, in, on, at, to, etc. These are often removed during text processing to focus on the more meaningful words. Example: Python Code to Get Stop Words Using spaCy spacy.cli.download(\"en_core_web_sm\") nlp = spacy.load(\"en_core_web_sm\") stopwords = nlp.Defaults.stop_words Text normalization means converting text into a standard, clean, and consistent format. It includes many small steps like: Lowercasing (converting all text to lowercase) Tokenization (splitting into words) Stemming and lemmatization Handling numbers, dates, and symbols Expanding abbreviations (e.g., “don’t” → “do not”) All these steps make the text easier for a machine to read and understand.",
    "enhanced_text": "[NLP] “Universal” → “Univers” “University” → “Univers” This is helpful when you want to treat similar forms of a word as one item. However, stemming is rough — it might cut too much (over-stemming) or too little (under-stemming). Popular stemming algorithms include: Lemmatization is similar to stemming but smarter. It uses grammar rules and a dictionary to reduce a word to its base or dictionary form, called a lemma. Example:\nOriginal sentence: “The cats are chasing mice.”\nAfter lemmatization: “The cat be chase mouse.” Unlike stemming, lemmatization gives real words, and it understands context. It knows the difference between “better” and “good” or “ran” and “run”. Tools called lemmatizers perform this task. They rely on linguistics and lexical databases like WordNet. Difference: Stemming vs Lemmatization Stemming is fast and simple but may create fake or broken words. Lemmatization is slower but gives correct and meaningful words. Stemming uses fixed rules. Lemmatization uses grammar, context, and dictionary lookups. Corpus (Plural: Corpora) A corpus is a large collection of texts, often used for training or testing NLP models. Transcripts of spoken conversations Examples of known corpora: BBC News Articles Corpus You can find datasets on: Google Dataset Search Stop words are very common words in a language that don’t add much meaning by themselves. Examples in English include: is, am, are, the, a, an, in, on, at, to, etc. These are often removed during text processing to focus on the more meaningful words. Example: Python Code to Get Stop Words Using spaCy spacy.cli.download(\"en_core_web_sm\") nlp = spacy.load(\"en_core_web_sm\") stopwords = nlp.Defaults.stop_words Text normalization means converting text into a standard, clean, and consistent format. It includes many small steps like: Lowercasing (converting all text to lowercase) Tokenization (splitting into words) Stemming and lemmatization Handling numbers, dates, and symbols Expanding abbreviations (e.g., “don’t” → “do not”) All these steps make the text easier for a machine to read and understand.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(e) Basic Vocabulary.txt",
    "file_name": "lec1-(e) Basic Vocabulary.txt",
    "position_in_document": 63,
    "filename_keywords": [
      "vocabulary",
      "basic",
      "lec1"
    ],
    "content_keywords": [
      "basic vocabulary",
      "natural language processing",
      "these",
      "nlp",
      "nlp before"
    ],
    "all_keywords": [
      "basic vocabulary",
      "natural language processing",
      "vocabulary",
      "basic",
      "these",
      "nlp",
      "nlp before",
      "lec1"
    ],
    "keyword_string": "basic vocabulary natural language processing vocabulary basic these nlp nlp before lec1",
    "token_count": 500,
    "word_count": 313,
    "sentence_count": 18,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.626,
    "avg_sentence_length": 17.38888888888889
  },
  {
    "chunk_id": 97,
    "chunk_hash": "4c9ccf8c1fc0",
    "text": "A Basic Natural Language Processing (NLP) System To understand how an NLP system works, imagine it as a four-step pipeline where human language enters one end, and machine-generated or processed language comes out the other. Each step plays a vital role in how machines handle natural language. This is the starting point. A user gives some natural language input — it could be in the form of: Text (e.g., “What’s the weather today?”) Voice (which is first converted to text using speech recognition) The input is usually messy, unstructured, and full of variations, just like how people talk or write. Understanding the Language Once the language enters the system, the machine tries to understand the meaning. This step is the heart of NLP. It involves tasks like: Tokenization (splitting the sentence into words) Parsing (understanding sentence structure) Part-of-speech tagging Named entity recognition Semantic analysis (what the sentence actually means) This is the \"U\" in NLP: Understanding. Here, the computer is trained to read and comprehend language the way humans do. Computer Processing (Reasoning and Decision-Making) After understanding the input, the system uses logic, machine learning models, or pre-defined rules to decide what to do. A chatbot figures out how to respond. A translation model determines the equivalent sentence in another language. A search engine finds the most relevant result. This is where the thinking or computation happens based on what the system understood. Language Generation (Output) Finally, the system generates language as output. This output could be: A direct response (e.g., “It’s sunny and 30°C”) A translation (e.g., converting English to French) A summary or a sentiment label This is the “G” part: Generation. The computer creates or outputs text (or speech) that is clear and meaningful for a human to understand. User: \"What's the weather today?\" Input Language: User speaks or types the question. Understanding: The system breaks the sentence into parts, identifies the question type, and understands the meaning. Processing: It searches a weather database for today’s data. Generation: The system replies: “The weather is sunny and 30 degrees.”",
    "enhanced_text": "[NLP] A Basic Natural Language Processing (NLP) System To understand how an NLP system works, imagine it as a four-step pipeline where human language enters one end, and machine-generated or processed language comes out the other. Each step plays a vital role in how machines handle natural language. This is the starting point. A user gives some natural language input — it could be in the form of: Text (e.g., “What’s the weather today?”) Voice (which is first converted to text using speech recognition) The input is usually messy, unstructured, and full of variations, just like how people talk or write. Understanding the Language Once the language enters the system, the machine tries to understand the meaning. This step is the heart of NLP. It involves tasks like: Tokenization (splitting the sentence into words) Parsing (understanding sentence structure) Part-of-speech tagging Named entity recognition Semantic analysis (what the sentence actually means) This is the \"U\" in NLP: Understanding. Here, the computer is trained to read and comprehend language the way humans do. Computer Processing (Reasoning and Decision-Making) After understanding the input, the system uses logic, machine learning models, or pre-defined rules to decide what to do. A chatbot figures out how to respond. A translation model determines the equivalent sentence in another language. A search engine finds the most relevant result. This is where the thinking or computation happens based on what the system understood. Language Generation (Output) Finally, the system generates language as output. This output could be: A direct response (e.g., “It’s sunny and 30°C”) A translation (e.g., converting English to French) A summary or a sentiment label This is the “G” part: Generation. The computer creates or outputs text (or speech) that is clear and meaningful for a human to understand. User: \"What's the weather today?\" Input Language: User speaks or types the question. Understanding: The system breaks the sentence into parts, identifies the question type, and understands the meaning. Processing: It searches a weather database for today’s data. Generation: The system replies: “The weather is sunny and 30 degrees.”",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(f) basic Natural Language Processing (NLP) system.txt",
    "file_name": "lec1-(f) basic Natural Language Processing (NLP) system.txt",
    "position_in_document": 38,
    "filename_keywords": [
      "processing",
      "basic",
      "language",
      "system",
      "nlp",
      "natural",
      "lec1"
    ],
    "content_keywords": [
      "nlp",
      "system to",
      "each",
      "a basic natural language processing"
    ],
    "all_keywords": [
      "processing",
      "system to",
      "each",
      "basic",
      "language",
      "a basic natural language processing",
      "system",
      "nlp",
      "natural",
      "lec1"
    ],
    "keyword_string": "processing system to each basic language a basic natural language processing system nlp natural lec1",
    "token_count": 465,
    "word_count": 341,
    "sentence_count": 21,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7333333333333333,
    "avg_sentence_length": 16.238095238095237
  },
  {
    "chunk_id": 98,
    "chunk_hash": "4f3fcaa6c407",
    "text": "Why NLP Needs Special Techniques Because human language is full of ambiguity, context, hidden meanings, and exceptions, we can’t just use simple rules or logic to make computers understand it. We need to combine knowledge from many different fields to teach machines how to process language in a human-like way. Here’s a short explanation of the key fields that NLP depends on: Linguistics – Understanding how language works Linguistics helps us understand the structure of language, like grammar, syntax (sentence structure), semantics (meaning), and phonetics (sound). This knowledge is essential for breaking down sentences, identifying parts of speech, and figuring out how words relate to each other. Mathematics and Statistics – Finding patterns in language Language data is huge and messy. We use mathematical models and statistical methods to analyze patterns, like how often words appear together, or what word is likely to come next in a sentence. For example, we can calculate the probability that the next word after “I am” is “happy”. Artificial Intelligence and Machine Learning – Learning from data Instead of writing manual rules, we teach machines to learn from examples. Machine learning allows a system to look at thousands of sentences and figure out how language is used. Over time, it can predict answers, translate text, or understand emotions in new sentences it has never seen before. Psychology – Understanding how humans communicate Psychology helps us understand how people think, speak, and understand language. It gives insight into meaning, emotions, memory, and behavior — all important when building NLP systems like chatbots or voice assistants that need to “think” like a human. Databases and Algorithms – Handling big data and smart processing Language data comes from the web, books, news, tweets, emails — it's massive! Databases help us store and organize this data. Algorithms help us search, sort, and process it quickly — for example, finding the right answer in a question-answering system or clustering similar documents together. Each of these fields plays a unique role in helping machines understand the very thing that defines us as humans: language.",
    "enhanced_text": "[NLP] Why NLP Needs Special Techniques Because human language is full of ambiguity, context, hidden meanings, and exceptions, we can’t just use simple rules or logic to make computers understand it. We need to combine knowledge from many different fields to teach machines how to process language in a human-like way. Here’s a short explanation of the key fields that NLP depends on: Linguistics – Understanding how language works Linguistics helps us understand the structure of language, like grammar, syntax (sentence structure), semantics (meaning), and phonetics (sound). This knowledge is essential for breaking down sentences, identifying parts of speech, and figuring out how words relate to each other. Mathematics and Statistics – Finding patterns in language Language data is huge and messy. We use mathematical models and statistical methods to analyze patterns, like how often words appear together, or what word is likely to come next in a sentence. For example, we can calculate the probability that the next word after “I am” is “happy”. Artificial Intelligence and Machine Learning – Learning from data Instead of writing manual rules, we teach machines to learn from examples. Machine learning allows a system to look at thousands of sentences and figure out how language is used. Over time, it can predict answers, translate text, or understand emotions in new sentences it has never seen before. Psychology – Understanding how humans communicate Psychology helps us understand how people think, speak, and understand language. It gives insight into meaning, emotions, memory, and behavior — all important when building NLP systems like chatbots or voice assistants that need to “think” like a human. Databases and Algorithms – Handling big data and smart processing Language data comes from the web, books, news, tweets, emails — it's massive! Databases help us store and organize this data. Algorithms help us search, sort, and process it quickly — for example, finding the right answer in a question-answering system or clustering similar documents together. Each of these fields plays a unique role in helping machines understand the very thing that defines us as humans: language.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(h) NLP need special techniques.txt",
    "file_name": "lec1-(h) NLP need special techniques.txt",
    "position_in_document": 23,
    "filename_keywords": [
      "special",
      "techniques",
      "nlp",
      "need",
      "lec1"
    ],
    "content_keywords": [
      "nlp",
      "why nlp needs special techniques because"
    ],
    "all_keywords": [
      "special",
      "techniques",
      "why nlp needs special techniques because",
      "nlp",
      "need",
      "lec1"
    ],
    "keyword_string": "special techniques why nlp needs special techniques because nlp need lec1",
    "token_count": 421,
    "word_count": 344,
    "sentence_count": 16,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.8171021377672208,
    "avg_sentence_length": 21.5
  },
  {
    "chunk_id": 99,
    "chunk_hash": "60f6bb7ecd4c",
    "text": "Information Extraction (IE) Information Extraction is the process of pulling out specific pieces of information from unstructured text — like articles, emails, or reports — and organizing it into a structured form that computers can use. Instead of reading the entire text manually, IE helps systems automatically detect important facts, such as names, dates, locations, or events. 🔍 What Does “Unstructured Text” Mean? Unstructured text means natural language, written the way people talk or write — not stored in neat rows and columns like a spreadsheet. Example of unstructured text: \"Mohammad Hassan, a 26-year-old aspiring photographer, was stuck in a dilemma when he found out that his wheat yield...” It’s messy for a computer to read, but full of useful data for a human. 🎯 Goal of Information Extraction The main goal is to identify and extract only the relevant information, such as: Amounts or quantities This helps convert unstructured text into structured data, which can then be used for analysis, searching, or automation. Your Email → Calendar\nYou receive an email: “Let’s meet on Monday at 2 PM at the coffee shop.” Your email app detects the date and time and suggests adding a calendar event — this is Information Extraction in action. 🧠 Why is Information Extraction Useful? Information Extraction is important for: Smart search and classification\nAutomatically labeling emails or documents (e.g., \"Job Offer\", \"Travel Plan\") Automation\nPulling details from thousands of resumes, reports, or forms without human effort Trend analysis\nMining articles or social media to find out what's trending (e.g., people talking about “floods” in “Sindh”) Finding hidden connections\nDetecting relationships between people, companies, events, or topics 📝 Example Text for Extraction: Mohammad Hassan, a 26-year-old aspiring photographer, was stuck in a dilemma when he found out that his wheat yield, the primary source of income for his family, had dramatically fallen from the usual five sacks to three. An acute water shortage had gripped his hometown, Skardu, located in the heart of the Karakoram mountains, bringing the lives of its residents to a halt. Let’s extract key information from this paragraph: Name: Mohammad Hassan Occupation: Aspiring Photographer Location: Skardu, Karakoram Mountains Event: Wheat yield dropped due to water shortage Cause: Acute water shortage",
    "enhanced_text": "[NLP] Information Extraction (IE) Information Extraction is the process of pulling out specific pieces of information from unstructured text — like articles, emails, or reports — and organizing it into a structured form that computers can use. Instead of reading the entire text manually, IE helps systems automatically detect important facts, such as names, dates, locations, or events. 🔍 What Does “Unstructured Text” Mean? Unstructured text means natural language, written the way people talk or write — not stored in neat rows and columns like a spreadsheet. Example of unstructured text: \"Mohammad Hassan, a 26-year-old aspiring photographer, was stuck in a dilemma when he found out that his wheat yield...” It’s messy for a computer to read, but full of useful data for a human. 🎯 Goal of Information Extraction The main goal is to identify and extract only the relevant information, such as: Amounts or quantities This helps convert unstructured text into structured data, which can then be used for analysis, searching, or automation. Your Email → Calendar\nYou receive an email: “Let’s meet on Monday at 2 PM at the coffee shop.” Your email app detects the date and time and suggests adding a calendar event — this is Information Extraction in action. 🧠 Why is Information Extraction Useful? Information Extraction is important for: Smart search and classification\nAutomatically labeling emails or documents (e.g., \"Job Offer\", \"Travel Plan\") Automation\nPulling details from thousands of resumes, reports, or forms without human effort Trend analysis\nMining articles or social media to find out what's trending (e.g., people talking about “floods” in “Sindh”) Finding hidden connections\nDetecting relationships between people, companies, events, or topics 📝 Example Text for Extraction: Mohammad Hassan, a 26-year-old aspiring photographer, was stuck in a dilemma when he found out that his wheat yield, the primary source of income for his family, had dramatically fallen from the usual five sacks to three. An acute water shortage had gripped his hometown, Skardu, located in the heart of the Karakoram mountains, bringing the lives of its residents to a halt. Let’s extract key information from this paragraph: Name: Mohammad Hassan Occupation: Aspiring Photographer Location: Skardu, Karakoram Mountains Event: Wheat yield dropped due to water shortage Cause: Acute water shortage",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(a) Information Extraction.txt",
    "file_name": "lec2-(a) Information Extraction.txt",
    "position_in_document": 30,
    "filename_keywords": [
      "information",
      "lec2",
      "extraction"
    ],
    "content_keywords": [
      "information extraction",
      "instead"
    ],
    "all_keywords": [
      "information",
      "extraction",
      "information extraction",
      "lec2",
      "instead"
    ],
    "keyword_string": "information extraction information extraction lec2 instead",
    "token_count": 490,
    "word_count": 369,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.753061224489796,
    "avg_sentence_length": 33.54545454545455
  },
  {
    "chunk_id": 100,
    "chunk_hash": "29d42611c4d8",
    "text": "Typical Information Extraction Applications Business Intelligence\nDescription:\nExtracting structured data from large volumes of text such as reports, emails, news articles, and social media to help business analysts make informed decisions. The goal is to identify key facts like company names, product launches, market trends, or financial figures automatically. Example:\nInput Text:\n“ABC Corp announced a 10% increase in revenue this quarter and plans to launch a new product next month.”\nExtracted Information: Event: Product launch Financial Investigation\nDescription:\nUsing NLP to find hidden connections and patterns in financial documents, transaction logs, or emails that could indicate fraud, money laundering, or other financial crimes. Example:\nInput Text:\n“John transferred $5 million to XYZ Ltd., which is linked to offshore accounts.”\nExtracted Information: Suspicious Link: Offshore accounts Scientific Research\nDescription:\nAutomatically extracting references, key findings, or related work from scientific papers to assist researchers in literature review or paper recommendation systems. Example:\nInput Text:\n“The method proposed by Smith et al. (2020) improves neural network accuracy by 15%.”\nExtracted Information: Authors: Smith et al. Contribution: Improved neural network accuracy Improvement Percentage: 15% Media Monitoring\nDescription:\nTracking mentions of companies, products, or people across news articles, blogs, or social media to analyze public sentiment or reputation. Example:\nInput Text:\n“Brand X received positive feedback after their latest product release.”\nExtracted Information: Sentiment: Positive feedback Event: Product release Healthcare Records Management & Pharma Research\nDescription:\nExtracting patient information, diagnosis, medication, and treatment plans from unstructured clinical notes or research papers to improve healthcare delivery and pharmaceutical studies. Example:\nInput Text:\n“Patient diagnosed with diabetes type 2 was prescribed Metformin 500mg daily.”\nExtracted Information: Patient Condition: Diabetes type 2 Medication: Metformin",
    "enhanced_text": "[NLP] Typical Information Extraction Applications Business Intelligence\nDescription:\nExtracting structured data from large volumes of text such as reports, emails, news articles, and social media to help business analysts make informed decisions. The goal is to identify key facts like company names, product launches, market trends, or financial figures automatically. Example:\nInput Text:\n“ABC Corp announced a 10% increase in revenue this quarter and plans to launch a new product next month.”\nExtracted Information: Event: Product launch Financial Investigation\nDescription:\nUsing NLP to find hidden connections and patterns in financial documents, transaction logs, or emails that could indicate fraud, money laundering, or other financial crimes. Example:\nInput Text:\n“John transferred $5 million to XYZ Ltd., which is linked to offshore accounts.”\nExtracted Information: Suspicious Link: Offshore accounts Scientific Research\nDescription:\nAutomatically extracting references, key findings, or related work from scientific papers to assist researchers in literature review or paper recommendation systems. Example:\nInput Text:\n“The method proposed by Smith et al. (2020) improves neural network accuracy by 15%.”\nExtracted Information: Authors: Smith et al. Contribution: Improved neural network accuracy Improvement Percentage: 15% Media Monitoring\nDescription:\nTracking mentions of companies, products, or people across news articles, blogs, or social media to analyze public sentiment or reputation. Example:\nInput Text:\n“Brand X received positive feedback after their latest product release.”\nExtracted Information: Sentiment: Positive feedback Event: Product release Healthcare Records Management & Pharma Research\nDescription:\nExtracting patient information, diagnosis, medication, and treatment plans from unstructured clinical notes or research papers to improve healthcare delivery and pharmaceutical studies. Example:\nInput Text:\n“Patient diagnosed with diabetes type 2 was prescribed Metformin 500mg daily.”\nExtracted Information: Patient Condition: Diabetes type 2 Medication: Metformin",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(b) Typical Information Extraction Applications.txt",
    "file_name": "lec2-(b) Typical Information Extraction Applications.txt",
    "position_in_document": 22,
    "filename_keywords": [
      "information",
      "extraction",
      "lec2",
      "typical",
      "applications"
    ],
    "content_keywords": [
      "the",
      "typical information extraction applications business intelligence\ndescription",
      "extracting"
    ],
    "all_keywords": [
      "information",
      "extraction",
      "lec2",
      "extracting",
      "typical",
      "applications",
      "typical information extraction applications business intelligence\ndescription",
      "the"
    ],
    "keyword_string": "information extraction lec2 extracting typical applications typical information extraction applications business intelligence\ndescription the",
    "token_count": 371,
    "word_count": 277,
    "sentence_count": 9,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7466307277628033,
    "avg_sentence_length": 30.77777777777778
  },
  {
    "chunk_id": 101,
    "chunk_hash": "6ef4e4a28093",
    "text": "What is Information Retrieval (IR)? Information Retrieval means finding useful information from a large collection of resources like websites, documents, images, or sounds. Imagine you want to find answers to a question or get documents related to a topic. IR systems help you do that by searching through all available data and showing you the most relevant results. Two ways to think about IR: Definition 1:\nIt’s the process of getting information (like documents or websites) that match what you’re looking for from a big collection. Definition 2:\nIt’s the science of searching through different types of data — like documents, images, or even sounds — to find exactly what you need. You have a collection of documents (called a corpus). You enter a search query (a few words or a question). The IR system looks through the documents and gives you a list of documents ranked by how relevant they are to your query. How Does an IR System Work? You type your query into the system. The system looks through its collection of documents. It ranks these documents by how well they match your query. It shows you the ranked list of documents. Detailed IR Process in Simple Terms: Documents collection:\nThe system starts with many documents — articles, webpages, reports, etc. Indexing:\nThe system organizes and summarizes these documents in a way that makes searching faster. This is like making a \"map\" or \"catalog\" of what’s inside each document. Query processing:\nWhen you enter your search words, the system processes your query to understand what you want. Searching:\nThe system compares your processed query to the index, trying to find documents that match best. It uses special models (like comparing words as vectors or calculating probabilities) to decide which documents are most relevant. Ranking:\nFinally, it orders the documents from most to least relevant and shows you the list.",
    "enhanced_text": "[NLP] What is Information Retrieval (IR)? Information Retrieval means finding useful information from a large collection of resources like websites, documents, images, or sounds. Imagine you want to find answers to a question or get documents related to a topic. IR systems help you do that by searching through all available data and showing you the most relevant results. Two ways to think about IR: Definition 1:\nIt’s the process of getting information (like documents or websites) that match what you’re looking for from a big collection. Definition 2:\nIt’s the science of searching through different types of data — like documents, images, or even sounds — to find exactly what you need. You have a collection of documents (called a corpus). You enter a search query (a few words or a question). The IR system looks through the documents and gives you a list of documents ranked by how relevant they are to your query. How Does an IR System Work? You type your query into the system. The system looks through its collection of documents. It ranks these documents by how well they match your query. It shows you the ranked list of documents. Detailed IR Process in Simple Terms: Documents collection:\nThe system starts with many documents — articles, webpages, reports, etc. Indexing:\nThe system organizes and summarizes these documents in a way that makes searching faster. This is like making a \"map\" or \"catalog\" of what’s inside each document. Query processing:\nWhen you enter your search words, the system processes your query to understand what you want. Searching:\nThe system compares your processed query to the index, trying to find documents that match best. It uses special models (like comparing words as vectors or calculating probabilities) to decide which documents are most relevant. Ranking:\nFinally, it orders the documents from most to least relevant and shows you the list.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(c) Information Retrieval.txt",
    "file_name": "lec2-(c) Information Retrieval.txt",
    "position_in_document": 23,
    "filename_keywords": [
      "information",
      "retrieval",
      "lec2"
    ],
    "content_keywords": [
      "imagine",
      "information retrieval",
      "what"
    ],
    "all_keywords": [
      "information",
      "retrieval",
      "information retrieval",
      "lec2",
      "what",
      "imagine"
    ],
    "keyword_string": "information retrieval information retrieval lec2 what imagine",
    "token_count": 382,
    "word_count": 312,
    "sentence_count": 21,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.8167539267015707,
    "avg_sentence_length": 14.857142857142858
  },
  {
    "chunk_id": 102,
    "chunk_hash": "a2aebfcad78a",
    "text": "What is Question Answering (QA)? QA systems are computer programs that answer questions posed in natural language. These systems often work by extracting facts from large sources like the web or databases and matching them to the question you ask. Common Types of Questions in QA Systems These are simple fact-based questions. Usually start with question words like Who, What, When, Where. Example: “Who is the president of the USA?” Answer: A specific fact like “Joe Biden.” Questions that require multiple answers in a list form. Example: “What are the countries in the European Union?” Answer: A list of countries (France, Germany, Italy, etc. These need explanations or reasons about something. Often start with How or Why. Example: “Why do leaves change color in autumn?” Answer: An explanation involving science or facts. Confirmation Questions Require a simple Yes or No answer. These need the system to understand and reason based on facts or common sense. Example: “Is Paris the capital of Germany?” Hypothetical Questions Ask about imagined or “what if” situations. Usually start with “What would happen if…” There are no fixed answers, often speculative. Example: “What would happen if humans could fly?” Answer: Open-ended or speculative explanations.",
    "enhanced_text": "[NLP] What is Question Answering (QA)? QA systems are computer programs that answer questions posed in natural language. These systems often work by extracting facts from large sources like the web or databases and matching them to the question you ask. Common Types of Questions in QA Systems These are simple fact-based questions. Usually start with question words like Who, What, When, Where. Example: “Who is the president of the USA?” Answer: A specific fact like “Joe Biden.” Questions that require multiple answers in a list form. Example: “What are the countries in the European Union?” Answer: A list of countries (France, Germany, Italy, etc. These need explanations or reasons about something. Often start with How or Why. Example: “Why do leaves change color in autumn?” Answer: An explanation involving science or facts. Confirmation Questions Require a simple Yes or No answer. These need the system to understand and reason based on facts or common sense. Example: “Is Paris the capital of Germany?” Hypothetical Questions Ask about imagined or “what if” situations. Usually start with “What would happen if…” There are no fixed answers, often speculative. Example: “What would happen if humans could fly?” Answer: Open-ended or speculative explanations.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(d) Question Answering.txt",
    "file_name": "lec2-(d) Question Answering.txt",
    "position_in_document": 25,
    "filename_keywords": [
      "answering",
      "lec2",
      "question"
    ],
    "content_keywords": [
      "question answering",
      "these",
      "what"
    ],
    "all_keywords": [
      "question answering",
      "answering",
      "lec2",
      "what",
      "these",
      "question"
    ],
    "keyword_string": "question answering answering lec2 what these question",
    "token_count": 264,
    "word_count": 198,
    "sentence_count": 15,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.75,
    "avg_sentence_length": 13.2
  },
  {
    "chunk_id": 103,
    "chunk_hash": "777a0f9b3fad",
    "text": "What are Dialogue Systems? Dialogue systems are computer programs designed to talk or interact with humans in ways both can understand. These interactions can happen through: Speech (voice assistants like Siri or Alexa) Text (chatbots on websites, messaging apps) Gestures or graphics (like virtual avatars) Other sensory methods (haptics, animations, etc.) Examples of Dialogue Systems: ChatGPT — A popular AI chatbot that answers questions and holds conversations in natural language. Gemini — An example of a modern conversational AI system. Role of Generative AI in Dialogue Systems Generative AI is crucial because it helps create new, meaningful content based on the input it receives. If you type or say something, the system can generate relevant responses or content. It can create text, images, sounds, animations, 3D models, and more. For example, you can ask ChatGPT to write a poem, generate a story, or explain a concept — all using generative AI. Simple Example Interaction: User: \"What’s the weather like today?\" Dialogue System: \"It’s sunny and 25°C in your area.\"",
    "enhanced_text": "[NLP] What are Dialogue Systems? Dialogue systems are computer programs designed to talk or interact with humans in ways both can understand. These interactions can happen through: Speech (voice assistants like Siri or Alexa) Text (chatbots on websites, messaging apps) Gestures or graphics (like virtual avatars) Other sensory methods (haptics, animations, etc.) Examples of Dialogue Systems: ChatGPT — A popular AI chatbot that answers questions and holds conversations in natural language. Gemini — An example of a modern conversational AI system. Role of Generative AI in Dialogue Systems Generative AI is crucial because it helps create new, meaningful content based on the input it receives. If you type or say something, the system can generate relevant responses or content. It can create text, images, sounds, animations, 3D models, and more. For example, you can ask ChatGPT to write a poem, generate a story, or explain a concept — all using generative AI. Simple Example Interaction: User: \"What’s the weather like today?\" Dialogue System: \"It’s sunny and 25°C in your area.\"",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(e) Dialogue Systems.txt",
    "file_name": "lec2-(e) Dialogue Systems.txt",
    "position_in_document": 18,
    "filename_keywords": [
      "systems",
      "lec2",
      "dialogue"
    ],
    "content_keywords": [
      "dialogue systems",
      "dialogue",
      "these",
      "what"
    ],
    "all_keywords": [
      "dialogue systems",
      "systems",
      "lec2",
      "what",
      "dialogue",
      "these"
    ],
    "keyword_string": "dialogue systems systems lec2 what dialogue these",
    "token_count": 232,
    "word_count": 169,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.728448275862069,
    "avg_sentence_length": 15.363636363636363
  },
  {
    "chunk_id": 104,
    "chunk_hash": "95d326f644f5",
    "text": "What is Text Summarization? Text summarization is the process of creating a short, clear, and accurate summary from a longer text. It helps you quickly understand the main points without reading the entire document. For example, search engines show short summaries (called snippets) under each search result to give you a quick idea of what the page is about. Real-World Example: Web Search Summarization When you search for “generative AI” on Google, you see a list of links. Under each link, there’s a short summary — a snippet taken from the web page. These snippets help you decide which link to click by showing key information in a few sentences. Types of Summarization Extractive Summarization Picks important sentences or phrases directly from the original text. Example: Taking every 3rd or 4th sentence from a paragraph to form the summary. It’s like highlighting key sentences. Abstractive Summarization Understands the main ideas and rewrites them in simpler, shorter language. The summary may use new sentences or words not found exactly in the original text. Similar to how humans write summaries by paraphrasing. Original Text:\n“Generative AI refers to artificial intelligence systems that can create new content like text, images, or music. These systems learn patterns from data and generate creative outputs.” Extractive Summary:\n“Generative AI refers to artificial intelligence systems that can create new content like text, images, or music.” Abstractive Summary:\n“Generative AI creates new content by learning from existing data.”",
    "enhanced_text": "[NLP] What is Text Summarization? Text summarization is the process of creating a short, clear, and accurate summary from a longer text. It helps you quickly understand the main points without reading the entire document. For example, search engines show short summaries (called snippets) under each search result to give you a quick idea of what the page is about. Real-World Example: Web Search Summarization When you search for “generative AI” on Google, you see a list of links. Under each link, there’s a short summary — a snippet taken from the web page. These snippets help you decide which link to click by showing key information in a few sentences. Types of Summarization Extractive Summarization Picks important sentences or phrases directly from the original text. Example: Taking every 3rd or 4th sentence from a paragraph to form the summary. It’s like highlighting key sentences. Abstractive Summarization Understands the main ideas and rewrites them in simpler, shorter language. The summary may use new sentences or words not found exactly in the original text. Similar to how humans write summaries by paraphrasing. Original Text:\n“Generative AI refers to artificial intelligence systems that can create new content like text, images, or music. These systems learn patterns from data and generate creative outputs.” Extractive Summary:\n“Generative AI refers to artificial intelligence systems that can create new content like text, images, or music.” Abstractive Summary:\n“Generative AI creates new content by learning from existing data.”",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(f) Summarization.txt",
    "file_name": "lec2-(f) Summarization.txt",
    "position_in_document": 21,
    "filename_keywords": [
      "summarization",
      "lec2"
    ],
    "content_keywords": [
      "text summarization",
      "text",
      "what"
    ],
    "all_keywords": [
      "summarization",
      "text summarization",
      "text",
      "lec2",
      "what"
    ],
    "keyword_string": "summarization text summarization text lec2 what",
    "token_count": 325,
    "word_count": 240,
    "sentence_count": 15,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7384615384615385,
    "avg_sentence_length": 16.0
  },
  {
    "chunk_id": 105,
    "chunk_hash": "315e68a0dc37",
    "text": "What is Machine Translation (MT)? Machine Translation is the automatic process of converting text from one natural language (like English) into another language (like Portuguese or French). Key Challenges in Machine Translation Lexical Ambiguity:\nA word can have multiple meanings. For example, the English word “bank” could mean a financial institution or the side of a river. The correct translation depends on the context. Syntactic Ambiguity:\nSentence structure can be unclear or complex, causing different possible interpretations and translations. Because of these challenges, the system needs to understand the correct meaning and structure to provide an accurate translation. Real-World Example: Google Translate You enter “Hello, how are you?” in English. Google Translate converts it into “Olá, como vai?” in Portuguese. This shows a simple, direct translation from one language to another. Advanced Example: DeepL Translator DeepL also translates “Hello, how are you?” to “Olá, como estás?” in Portuguese. It offers extra features like: Choosing between Formal or Informal tone. Providing alternative translations. This makes DeepL more nuanced and flexible than basic translation tools, adapting translations to the situation or style. Simple Example of Ambiguity English:\n“He saw the bat.” Possible Translations in another language: Bat (baseball equipment) The system must decide which meaning fits the sentence to translate correctly.",
    "enhanced_text": "[NLP] What is Machine Translation (MT)? Machine Translation is the automatic process of converting text from one natural language (like English) into another language (like Portuguese or French). Key Challenges in Machine Translation Lexical Ambiguity:\nA word can have multiple meanings. For example, the English word “bank” could mean a financial institution or the side of a river. The correct translation depends on the context. Syntactic Ambiguity:\nSentence structure can be unclear or complex, causing different possible interpretations and translations. Because of these challenges, the system needs to understand the correct meaning and structure to provide an accurate translation. Real-World Example: Google Translate You enter “Hello, how are you?” in English. Google Translate converts it into “Olá, como vai?” in Portuguese. This shows a simple, direct translation from one language to another. Advanced Example: DeepL Translator DeepL also translates “Hello, how are you?” to “Olá, como estás?” in Portuguese. It offers extra features like: Choosing between Formal or Informal tone. Providing alternative translations. This makes DeepL more nuanced and flexible than basic translation tools, adapting translations to the situation or style. Simple Example of Ambiguity English:\n“He saw the bat.” Possible Translations in another language: Bat (baseball equipment) The system must decide which meaning fits the sentence to translate correctly.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(g) Machine Translation & Multilinguality.txt",
    "file_name": "lec2-(g) Machine Translation & Multilinguality.txt",
    "position_in_document": 23,
    "filename_keywords": [
      "multilinguality",
      "lec2",
      "machine",
      "translation"
    ],
    "content_keywords": [
      "key challenges",
      "french",
      "english",
      "machine translation",
      "what",
      "portuguese"
    ],
    "all_keywords": [
      "key challenges",
      "french",
      "lec2",
      "machine translation",
      "what",
      "english",
      "multilinguality",
      "portuguese",
      "machine",
      "translation"
    ],
    "keyword_string": "key challenges french lec2 machine translation what english multilinguality portuguese machine translation",
    "token_count": 279,
    "word_count": 209,
    "sentence_count": 15,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7491039426523297,
    "avg_sentence_length": 13.933333333333334
  },
  {
    "chunk_id": 106,
    "chunk_hash": "c027475c23a8",
    "text": "What is Speech Recognition? Speech Recognition (also called Automatic Speech Recognition or ASR) is a technology that converts spoken language (human speech) into written text. It allows computers and applications to understand and process what people say. Opposite Technology: Speech Synthesis (Text-to-Speech) Speech Synthesis or Text-to-Speech (TTS) does the opposite — it converts written text into spoken words (an audio waveform). For example, when your phone reads out a message aloud, it uses speech synthesis. How Speech Recognition Works (Simple Explanation) When you speak, your voice produces sound waves (audio waveform). The speech recognition system processes these sound waves to identify words and convert them into text. This text can then be used by computers to perform tasks like sending messages, searching the web, or controlling devices. Example Applications of Speech Recognition Virtual Assistants: Siri, Alexa, Google Assistant Transcription Services: Converting meeting recordings to text Voice Commands: Controlling phones, cars, or smart home devices Customer Support: Automated phone systems understanding spoken requests Imagine a graph showing a wavy line — this represents the sound waves of your speech. Speech recognition systems analyze these waves to understand and write down your spoken words.",
    "enhanced_text": "[NLP] What is Speech Recognition? Speech Recognition (also called Automatic Speech Recognition or ASR) is a technology that converts spoken language (human speech) into written text. It allows computers and applications to understand and process what people say. Opposite Technology: Speech Synthesis (Text-to-Speech) Speech Synthesis or Text-to-Speech (TTS) does the opposite — it converts written text into spoken words (an audio waveform). For example, when your phone reads out a message aloud, it uses speech synthesis. How Speech Recognition Works (Simple Explanation) When you speak, your voice produces sound waves (audio waveform). The speech recognition system processes these sound waves to identify words and convert them into text. This text can then be used by computers to perform tasks like sending messages, searching the web, or controlling devices. Example Applications of Speech Recognition Virtual Assistants: Siri, Alexa, Google Assistant Transcription Services: Converting meeting recordings to text Voice Commands: Controlling phones, cars, or smart home devices Customer Support: Automated phone systems understanding spoken requests Imagine a graph showing a wavy line — this represents the sound waves of your speech. Speech recognition systems analyze these waves to understand and write down your spoken words.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(h) speech recognition.txt",
    "file_name": "lec2-(h) speech recognition.txt",
    "position_in_document": 17,
    "filename_keywords": [
      "recognition",
      "lec2",
      "speech"
    ],
    "content_keywords": [
      "speech recognition",
      "automatic speech recognition",
      "asr",
      "what"
    ],
    "all_keywords": [
      "recognition",
      "speech",
      "speech recognition",
      "asr",
      "lec2",
      "what",
      "automatic speech recognition"
    ],
    "keyword_string": "recognition speech speech recognition asr lec2 what automatic speech recognition",
    "token_count": 243,
    "word_count": 192,
    "sentence_count": 10,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7901234567901234,
    "avg_sentence_length": 19.2
  },
  {
    "chunk_id": 107,
    "chunk_hash": "07d71c29257d",
    "text": "Problems with the Boolean Model AND means all keywords must be present in a document. OR means any keyword can be present. This makes it hard to express complex or nuanced user requests. Difficult to control the number of documents retrieved: All documents that match the query are returned, often leading to too many or too few results. No easy way to adjust how many documents you get back. No ranking of results: All matched documents satisfy the query equally, so there's no way to order them by relevance. Difficult to perform relevance feedback: When a user marks a document as relevant or irrelevant, it’s unclear how to modify the query to improve results. Example (Document-Term Matrix) Imagine a table where: Rows = documents (D1 to D5) Columns = terms (T1 to T6) Each cell shows if a term appears in a document (present or absent). Example queries and results: Query: Islamabad OR University Finds documents containing either \"Islamabad\" or \"University\". Returns documents: D1, D2, D3, and D4. Query: Islamabad OR Capital Finds documents with \"Islamabad\" or \"Capital\". Returns: D1, D2, D3, D4, and D5. Query: Islamabad NOT University Finds documents with \"Islamabad\" but not \"University\". Returns: D2, D3, and D4. Feast or Famine Problem Boolean queries often return too few or too many results. Using AND narrows results too much, often too few documents. Using OR broadens results too much, often too many documents. It takes skill to write queries that return a useful number of documents. When using Boolean search, you need to check: Are the correct documents included? How many incorrect or irrelevant documents are included? How do you know if the model is trustworthy and effective?",
    "enhanced_text": "[NLP] Problems with the Boolean Model AND means all keywords must be present in a document. OR means any keyword can be present. This makes it hard to express complex or nuanced user requests. Difficult to control the number of documents retrieved: All documents that match the query are returned, often leading to too many or too few results. No easy way to adjust how many documents you get back. No ranking of results: All matched documents satisfy the query equally, so there's no way to order them by relevance. Difficult to perform relevance feedback: When a user marks a document as relevant or irrelevant, it’s unclear how to modify the query to improve results. Example (Document-Term Matrix) Imagine a table where: Rows = documents (D1 to D5) Columns = terms (T1 to T6) Each cell shows if a term appears in a document (present or absent). Example queries and results: Query: Islamabad OR University Finds documents containing either \"Islamabad\" or \"University\". Returns documents: D1, D2, D3, and D4. Query: Islamabad OR Capital Finds documents with \"Islamabad\" or \"Capital\". Returns: D1, D2, D3, D4, and D5. Query: Islamabad NOT University Finds documents with \"Islamabad\" but not \"University\". Returns: D2, D3, and D4. Feast or Famine Problem Boolean queries often return too few or too many results. Using AND narrows results too much, often too few documents. Using OR broadens results too much, often too many documents. It takes skill to write queries that return a useful number of documents. When using Boolean search, you need to check: Are the correct documents included? How many incorrect or irrelevant documents are included? How do you know if the model is trustworthy and effective?",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(c)Boolean Models Problems.txt",
    "file_name": "lec3-(c)Boolean Models Problems.txt",
    "position_in_document": 35,
    "filename_keywords": [
      "boolean",
      "problems",
      "models",
      "lec3"
    ],
    "content_keywords": [
      "problems",
      "and",
      "boolean model and"
    ],
    "all_keywords": [
      "problems",
      "and",
      "lec3",
      "boolean",
      "models",
      "boolean model and"
    ],
    "keyword_string": "problems and lec3 boolean models boolean model and",
    "token_count": 382,
    "word_count": 280,
    "sentence_count": 21,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7329842931937173,
    "avg_sentence_length": 13.333333333333334
  },
  {
    "chunk_id": 108,
    "chunk_hash": "40152699240c",
    "text": "Naive Bayes Classifier Naive Bayes is a popular classification algorithm used for both binary and multi-class problems. It’s based on Bayes’ theorem, which calculates the probability of a class given some observed data. Conceptual Example: Fruit Classification Imagine you have a mixed basket of fruits—strawberries, watermelons, and bananas. A classifier looks at each fruit and sorts them into groups based on their characteristics. Similarly, Naive Bayes looks at data points and assigns them to categories based on the probability that they belong there. Bag of Words Representation (for Text) Text data needs to be converted into numbers for the classifier to work. One common way is the Bag of Words model: The text is broken into unique words. For each word, count how many times it appears in the text. The order and grammar of words are ignored. Example:\n\"I love this movie! It’s sweet, but with satirical humor.\" Words like \"It\" might appear 6 times, \"the\" 4 times, \"and\" 3 times, etc. This numeric representation helps Naive Bayes understand the content of the text. How Naive Bayes Works Assumption: All features (words) are independent from each other (this is the \"naive\" assumption). Bayes’ theorem: It calculates the probability that a text belongs to a class based on the presence of words. Example: Movie Review Sentiment Classification Review: \"This movie was great!\" Review: \"The acting was terrible\"\nLabel: Negative Review: \"I love this film\"\nLabel: Positive Review: \"It was boring and dull\"\nLabel: Negative Goal: Given a new review, classify it as Positive or Negative based on the words it contains and what the classifier has learned. Steps to Build Naive Bayes Classifier Data Preparation:\nSplit the reviews into individual words (tokenization) and create a vocabulary of all unique words. Feature Extraction:\nRepresent each review as a vector indicating which words are present or absent. For example, “This movie was great!” might become:\n[1, 1, 1, 0, 0, 0, ...] where 1 means the word exists in the review. Training:\nCalculate the probability of each word appearing in positive and negative reviews. Calculate the prior probabilities of positive and negative classes (how common each class is). Classification:\nFor a new review, compute the probability it is positive or negative using Bayes’ theorem. Assign the label with the higher probability.",
    "enhanced_text": "[NLP] Naive Bayes Classifier Naive Bayes is a popular classification algorithm used for both binary and multi-class problems. It’s based on Bayes’ theorem, which calculates the probability of a class given some observed data. Conceptual Example: Fruit Classification Imagine you have a mixed basket of fruits—strawberries, watermelons, and bananas. A classifier looks at each fruit and sorts them into groups based on their characteristics. Similarly, Naive Bayes looks at data points and assigns them to categories based on the probability that they belong there. Bag of Words Representation (for Text) Text data needs to be converted into numbers for the classifier to work. One common way is the Bag of Words model: The text is broken into unique words. For each word, count how many times it appears in the text. The order and grammar of words are ignored. Example:\n\"I love this movie! It’s sweet, but with satirical humor.\" Words like \"It\" might appear 6 times, \"the\" 4 times, \"and\" 3 times, etc. This numeric representation helps Naive Bayes understand the content of the text. How Naive Bayes Works Assumption: All features (words) are independent from each other (this is the \"naive\" assumption). Bayes’ theorem: It calculates the probability that a text belongs to a class based on the presence of words. Example: Movie Review Sentiment Classification Review: \"This movie was great!\" Review: \"The acting was terrible\"\nLabel: Negative Review: \"I love this film\"\nLabel: Positive Review: \"It was boring and dull\"\nLabel: Negative Goal: Given a new review, classify it as Positive or Negative based on the words it contains and what the classifier has learned. Steps to Build Naive Bayes Classifier Data Preparation:\nSplit the reviews into individual words (tokenization) and create a vocabulary of all unique words. Feature Extraction:\nRepresent each review as a vector indicating which words are present or absent. For example, “This movie was great!” might become:\n[1, 1, 1, 0, 0, 0, ...] where 1 means the word exists in the review. Training:\nCalculate the probability of each word appearing in positive and negative reviews. Calculate the prior probabilities of positive and negative classes (how common each class is). Classification:\nFor a new review, compute the probability it is positive or negative using Bayes’ theorem. Assign the label with the higher probability.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(e) Naive Bayes Classifier.txt",
    "file_name": "lec3-(e) Naive Bayes Classifier.txt",
    "position_in_document": 34,
    "filename_keywords": [
      "classifier",
      "bayes",
      "naive",
      "lec3"
    ],
    "content_keywords": [
      "naive bayes classifier naive bayes",
      "bayes"
    ],
    "all_keywords": [
      "lec3",
      "bayes",
      "naive",
      "naive bayes classifier naive bayes",
      "classifier"
    ],
    "keyword_string": "lec3 bayes naive naive bayes classifier naive bayes classifier",
    "token_count": 509,
    "word_count": 379,
    "sentence_count": 24,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7445972495088409,
    "avg_sentence_length": 15.791666666666666
  },
  {
    "chunk_id": 109,
    "chunk_hash": "930b7dcb9e8f",
    "text": "A confusion matrix helps us see where a classification model is making mistakes—where it gets \"confused\" between different classes. The rows show the actual classes (the true labels). The columns show the predicted classes (what the model guessed). For a binary classification (two classes, like Positive and Negative), the confusion matrix has four parts: True Positive (TP): The model correctly predicted Positive when the actual class is Positive. False Positive (FP): The model predicted Positive but the actual class is Negative (a wrong positive). False Negative (FN): The model predicted Negative but the actual class is Positive (a missed positive). True Negative (TN): The model correctly predicted Negative when the actual class is Negative. This matrix is essential because it helps calculate evaluation metrics like accuracy, precision, and recall. For more than two classes (multi-class), the confusion matrix grows into an N x N matrix, where N is the number of classes. For example, with three classes like urgent, normal, and spam: The matrix shows how many items of each actual class were predicted as each possible class. The diagonal cells represent correct predictions for each class (true positives). The off-diagonal cells show errors, like an \"urgent\" message wrongly classified as \"normal.\" This helps calculate class-specific precision and recall, so you know how well the model performs on each category.",
    "enhanced_text": "[NLP] A confusion matrix helps us see where a classification model is making mistakes—where it gets \"confused\" between different classes. The rows show the actual classes (the true labels). The columns show the predicted classes (what the model guessed). For a binary classification (two classes, like Positive and Negative), the confusion matrix has four parts: True Positive (TP): The model correctly predicted Positive when the actual class is Positive. False Positive (FP): The model predicted Positive but the actual class is Negative (a wrong positive). False Negative (FN): The model predicted Negative but the actual class is Positive (a missed positive). True Negative (TN): The model correctly predicted Negative when the actual class is Negative. This matrix is essential because it helps calculate evaluation metrics like accuracy, precision, and recall. For more than two classes (multi-class), the confusion matrix grows into an N x N matrix, where N is the number of classes. For example, with three classes like urgent, normal, and spam: The matrix shows how many items of each actual class were predicted as each possible class. The diagonal cells represent correct predictions for each class (true positives). The off-diagonal cells show errors, like an \"urgent\" message wrongly classified as \"normal.\" This helps calculate class-specific precision and recall, so you know how well the model performs on each category.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(g) Confusion Matrix.txt",
    "file_name": "lec3-(g) Confusion Matrix.txt",
    "position_in_document": 15,
    "filename_keywords": [
      "confusion",
      "lec3",
      "matrix"
    ],
    "content_keywords": [
      "confused",
      "the"
    ],
    "all_keywords": [
      "lec3",
      "confused",
      "matrix",
      "confusion",
      "the"
    ],
    "keyword_string": "lec3 confused matrix confusion the",
    "token_count": 292,
    "word_count": 220,
    "sentence_count": 13,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7534246575342466,
    "avg_sentence_length": 16.923076923076923
  },
  {
    "chunk_id": 110,
    "chunk_hash": "0e8fd1227f40",
    "text": "Accuracy is a performance metric that measures the overall correctness of a classification model. It represents the proportion of total instances (or predictions) that the model classified correctly, encompassing both true positives and true negatives. Accuracy = (TP + TN) / (TP + FP + FN + TN) * TP (True Positives): The number of positive instances that were correctly classified as positive. * TN (True Negatives): The number of negative instances that were correctly classified as negative. * FP (False Positives): The number of negative instances that were incorrectly classified as positive (also known as a Type I error). * FN (False Negatives): The number of positive instances that were incorrectly classified as negative (also known as a Type II error). * The numerator (TP + TN) represents the total number of correct predictions made by the model. * The denominator (TP + FP + FN + TN) represents the total number of instances or predictions made. * Accuracy provides a general sense of how often the classifier is right overall. However, it can be misleading for imbalanced datasets (where one class significantly outnumbers others). ________________________________________ Precision is a performance metric that measures the model’s ability to correctly identify only the relevant (positive) data points from all the instances it predicted as positive. It answers the question: \"Of all the instances that the model labeled as positive, how many were actually positive?\" Precision = TP / (TP + FP) * TP (True Positives): The number of positive instances correctly identified as positive. * FP (False Positives): The number of negative instances incorrectly identified as positive. * The denominator (TP + FP) represents the total number of instances the model predicted as positive. * Precision focuses on the reliability of the positive predictions. * High precision indicates that the model makes few false positive errors. For example, if a spam filter has high precision, it means that when it flags an email as spam, it is very likely to actually be spam.",
    "enhanced_text": "[NLP] Accuracy is a performance metric that measures the overall correctness of a classification model. It represents the proportion of total instances (or predictions) that the model classified correctly, encompassing both true positives and true negatives. Accuracy = (TP + TN) / (TP + FP + FN + TN) * TP (True Positives): The number of positive instances that were correctly classified as positive. * TN (True Negatives): The number of negative instances that were correctly classified as negative. * FP (False Positives): The number of negative instances that were incorrectly classified as positive (also known as a Type I error). * FN (False Negatives): The number of positive instances that were incorrectly classified as negative (also known as a Type II error). * The numerator (TP + TN) represents the total number of correct predictions made by the model. * The denominator (TP + FP + FN + TN) represents the total number of instances or predictions made. * Accuracy provides a general sense of how often the classifier is right overall. However, it can be misleading for imbalanced datasets (where one class significantly outnumbers others). ________________________________________ Precision is a performance metric that measures the model’s ability to correctly identify only the relevant (positive) data points from all the instances it predicted as positive. It answers the question: \"Of all the instances that the model labeled as positive, how many were actually positive?\" Precision = TP / (TP + FP) * TP (True Positives): The number of positive instances correctly identified as positive. * FP (False Positives): The number of negative instances incorrectly identified as positive. * The denominator (TP + FP) represents the total number of instances the model predicted as positive. * Precision focuses on the reliability of the positive predictions. * High precision indicates that the model makes few false positive errors. For example, if a spam filter has high precision, it means that when it flags an email as spam, it is very likely to actually be spam.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(h) accuaracy , precision , recall.txt",
    "file_name": "lec3-(h) accuaracy , precision , recall.txt",
    "position_in_document": 21,
    "filename_keywords": [
      "precision",
      "recall",
      "accuaracy",
      "lec3"
    ],
    "content_keywords": [
      "accuracy"
    ],
    "all_keywords": [
      "lec3",
      "recall",
      "precision",
      "accuaracy",
      "accuracy"
    ],
    "keyword_string": "lec3 recall precision accuaracy accuracy",
    "token_count": 483,
    "word_count": 332,
    "sentence_count": 18,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6873706004140787,
    "avg_sentence_length": 18.444444444444443
  },
  {
    "chunk_id": 111,
    "chunk_hash": "b130f945d34f",
    "text": "* The denominator (TP + FP) represents the total number of instances the model predicted as positive. * Precision focuses on the reliability of the positive predictions. * High precision indicates that the model makes few false positive errors. For example, if a spam filter has high precision, it means that when it flags an email as spam, it is very likely to actually be spam. ________________________________________ Recall (also known as Sensitivity or True Positive Rate) Recall measures the model’s ability to find and identify all the actual relevant (positive) cases within the dataset. It answers the question: \"Of all the actual positive instances in the data, how many did the model successfully detect?\" Recall = TP / (TP + FN) * TP (True Positives): The number of positive instances correctly identified as positive. * FN (False Negatives): The number of positive instances that the model incorrectly identified as negative (i.e., missed positive cases). * The denominator (TP + FN) represents the total number of actual positive instances in the dataset. * Recall focuses on the completeness of the positive predictions – how well the model avoids missing positives. * High recall indicates that the model makes few false negative errors. For example, in a medical diagnosis task for a serious disease, high recall is crucial to ensure that most patients who actually have the disease are correctly identified.",
    "enhanced_text": "[NLP] * The denominator (TP + FP) represents the total number of instances the model predicted as positive. * Precision focuses on the reliability of the positive predictions. * High precision indicates that the model makes few false positive errors. For example, if a spam filter has high precision, it means that when it flags an email as spam, it is very likely to actually be spam. ________________________________________ Recall (also known as Sensitivity or True Positive Rate) Recall measures the model’s ability to find and identify all the actual relevant (positive) cases within the dataset. It answers the question: \"Of all the actual positive instances in the data, how many did the model successfully detect?\" Recall = TP / (TP + FN) * TP (True Positives): The number of positive instances correctly identified as positive. * FN (False Negatives): The number of positive instances that the model incorrectly identified as negative (i.e., missed positive cases). * The denominator (TP + FN) represents the total number of actual positive instances in the dataset. * Recall focuses on the completeness of the positive predictions – how well the model avoids missing positives. * High recall indicates that the model makes few false negative errors. For example, in a medical diagnosis task for a serious disease, high recall is crucial to ensure that most patients who actually have the disease are correctly identified.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(h) accuaracy , precision , recall.txt",
    "file_name": "lec3-(h) accuaracy , precision , recall.txt",
    "position_in_document": 32,
    "filename_keywords": [
      "precision",
      "recall",
      "accuaracy",
      "lec3"
    ],
    "content_keywords": [
      "accuracy"
    ],
    "all_keywords": [
      "lec3",
      "recall",
      "precision",
      "accuaracy",
      "accuracy"
    ],
    "keyword_string": "lec3 recall precision accuaracy accuracy",
    "token_count": 335,
    "word_count": 229,
    "sentence_count": 12,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6835820895522388,
    "avg_sentence_length": 19.083333333333332
  },
  {
    "chunk_id": 112,
    "chunk_hash": "f95e9b73bd1c",
    "text": "Precision is the percentage of selected items that are actually correct. Example: Out of all the emails your spam filter marked as spam, what fraction truly were spam? Recall is the percentage of correct items that are successfully selected. Example: Out of all the actual spam emails in your inbox, what fraction did the filter catch? These two metrics have an inverse relationship: When you try to increase recall (catch more relevant items), precision often drops because you include more irrelevant items too. When you try to increase precision (be more selective), recall may drop because you miss some relevant items. Trade-off Between Precision and Recall (Precision-Recall Curve) Imagine a graph where the x-axis is recall (from 0 to 1) and the y-axis is precision (also 0 to 1): In the top-left corner (high precision, low recall), the system returns only highly relevant results but misses many useful ones. It’s very careful but may overlook many positives. In the bottom-right corner (low precision, high recall), the system finds most relevant items but includes a lot of irrelevant ones too — so it returns “junk” along with the good stuff. The ideal point is the top-right corner (precision = 1, recall = 1), meaning the system finds all relevant items and nothing irrelevant, but this is very hard to achieve. This curve shows the typical trade-off between these two metrics.",
    "enhanced_text": "[NLP] Precision is the percentage of selected items that are actually correct. Example: Out of all the emails your spam filter marked as spam, what fraction truly were spam? Recall is the percentage of correct items that are successfully selected. Example: Out of all the actual spam emails in your inbox, what fraction did the filter catch? These two metrics have an inverse relationship: When you try to increase recall (catch more relevant items), precision often drops because you include more irrelevant items too. When you try to increase precision (be more selective), recall may drop because you miss some relevant items. Trade-off Between Precision and Recall (Precision-Recall Curve) Imagine a graph where the x-axis is recall (from 0 to 1) and the y-axis is precision (also 0 to 1): In the top-left corner (high precision, low recall), the system returns only highly relevant results but misses many useful ones. It’s very careful but may overlook many positives. In the bottom-right corner (low precision, high recall), the system finds most relevant items but includes a lot of irrelevant ones too — so it returns “junk” along with the good stuff. The ideal point is the top-right corner (precision = 1, recall = 1), meaning the system finds all relevant items and nothing irrelevant, but this is very hard to achieve. This curve shows the typical trade-off between these two metrics.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(i) Precision vs recall.txt",
    "file_name": "lec3-(i) Precision vs recall.txt",
    "position_in_document": 14,
    "filename_keywords": [
      "precision",
      "recall",
      "lec3"
    ],
    "content_keywords": [
      "precision",
      "example",
      "recall",
      "out"
    ],
    "all_keywords": [
      "example",
      "out",
      "lec3",
      "recall",
      "precision"
    ],
    "keyword_string": "example out lec3 recall precision",
    "token_count": 299,
    "word_count": 229,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7658862876254181,
    "avg_sentence_length": 20.818181818181817
  },
  {
    "chunk_id": 113,
    "chunk_hash": "b2c4d6181660",
    "text": "The F-Measure, commonly known as the F1 Score, is a metric that combines precision and recall into a single value. It is the harmonic mean of precision and recall. The F1 score is particularly useful when you need a balance between precision and recall, especially if there's an uneven class distribution. F1 Score = (2 * Precision * Recall) / (Precision + Recall) Alternatively, it can be expressed as: F1 Score = 2 / ( (1 / Precision) + (1 / Recall) ) * Precision: Measures the accuracy of positive predictions (TP / (TP + FP)). * Recall: Measures the ability to find all actual positives (TP / (TP + FN)). * The F1 score seeks to find an optimal blend of precision and recall. * It ranges from 0 to 1, where 1 indicates perfect precision and recall, and 0 indicates that either precision or recall (or both) is zero. * The use of the harmonic mean is significant: * It penalizes extreme values more than an arithmetic mean would. This means if either precision or recall is very low, the F1 score will also be low. To achieve a high F1 score, both precision and recall must be reasonably high. * Unlike the arithmetic mean, which could be high even if one metric is very low and the other very high, the harmonic mean gives more weight to lower values. This reflects the common desire in many classification tasks to optimize both the ability to avoid false positives (precision) and the ability to avoid false negatives (recall) simultaneously. * For instance, if Precision = 1.0 (perfect) but Recall = 0.1 (very poor), the F1 score would be 2 * (1.0 * 0.1) / (1.0 + 0.1) = 0.2 / 1.1 ≈ 0.18, which is low, reflecting the poor recall. An arithmetic mean would be (1.0 + 0.1) / 2 = 0.55, which might give a misleadingly optimistic view of performance. When to use F1 Score: * When you care equally about precision and recall. * When dealing with imbalanced datasets, where accuracy can be misleading. For example, if 99% of instances are negative, a model predicting everything as negative would have 99% accuracy but would be useless for identifying positive cases. The F1 score would better reflect its poor performance on the positive class.",
    "enhanced_text": "[NLP] The F-Measure, commonly known as the F1 Score, is a metric that combines precision and recall into a single value. It is the harmonic mean of precision and recall. The F1 score is particularly useful when you need a balance between precision and recall, especially if there's an uneven class distribution. F1 Score = (2 * Precision * Recall) / (Precision + Recall) Alternatively, it can be expressed as: F1 Score = 2 / ( (1 / Precision) + (1 / Recall) ) * Precision: Measures the accuracy of positive predictions (TP / (TP + FP)). * Recall: Measures the ability to find all actual positives (TP / (TP + FN)). * The F1 score seeks to find an optimal blend of precision and recall. * It ranges from 0 to 1, where 1 indicates perfect precision and recall, and 0 indicates that either precision or recall (or both) is zero. * The use of the harmonic mean is significant: * It penalizes extreme values more than an arithmetic mean would. This means if either precision or recall is very low, the F1 score will also be low. To achieve a high F1 score, both precision and recall must be reasonably high. * Unlike the arithmetic mean, which could be high even if one metric is very low and the other very high, the harmonic mean gives more weight to lower values. This reflects the common desire in many classification tasks to optimize both the ability to avoid false positives (precision) and the ability to avoid false negatives (recall) simultaneously. * For instance, if Precision = 1.0 (perfect) but Recall = 0.1 (very poor), the F1 score would be 2 * (1.0 * 0.1) / (1.0 + 0.1) = 0.2 / 1.1 ≈ 0.18, which is low, reflecting the poor recall. An arithmetic mean would be (1.0 + 0.1) / 2 = 0.55, which might give a misleadingly optimistic view of performance. When to use F1 Score: * When you care equally about precision and recall. * When dealing with imbalanced datasets, where accuracy can be misleading. For example, if 99% of instances are negative, a model predicting everything as negative would have 99% accuracy but would be useless for identifying positive cases. The F1 score would better reflect its poor performance on the positive class.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(k) F-measure.txt",
    "file_name": "lec3-(k) F-measure.txt",
    "position_in_document": 23,
    "filename_keywords": [
      "measure",
      "lec3"
    ],
    "content_keywords": [
      "measure",
      "the",
      "the f",
      "score"
    ],
    "all_keywords": [
      "score",
      "lec3",
      "measure",
      "the f",
      "the"
    ],
    "keyword_string": "score lec3 measure the f the",
    "token_count": 506,
    "word_count": 386,
    "sentence_count": 18,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7628458498023716,
    "avg_sentence_length": 21.444444444444443
  },
  {
    "chunk_id": 114,
    "chunk_hash": "638a7794914b",
    "text": "Multi-class Confusion Matrix and Evaluation For multiple classes (like \"urgent,\" \"normal,\" and \"spam\"), the confusion matrix expands beyond 2x2, and you compute precision and recall for each class separately: For a given class (e.g., \"urgent\"): True Positives (TP) are items actually urgent and predicted urgent. False Positives (FP) are items predicted urgent but actually belong to other classes (\"normal\" or \"spam\"). False Negatives (FN) are actual urgent items predicted as other classes. Precision for \"urgent\" = TP_urgent / (TP_urgent + FP_urgent) Recall for \"urgent\" = TP_urgent / (TP_urgent + FN_urgent) Similarly, you compute precision and recall for \"normal\" and \"spam\" classes. Overall accuracy is the sum of all correct predictions (all true positives across classes) divided by total items. Multi-class Exercise Example (no table format) Imagine a dataset with these counts: Actual urgent items predicted as urgent: 96 Actual urgent items predicted as normal: 3 Actual urgent items predicted as spam: 1 Actual normal items predicted as urgent: 4 Actual normal items predicted as normal: 89 Actual normal items predicted as spam: 7 Actual spam items predicted as urgent: 2 Actual spam items predicted as normal: 3 Actual spam items predicted as spam: 95 You can calculate precision and recall for each class using the formulas above: For urgent precision: true positives are 96; false positives are predictions of urgent that were actually normal or spam (4 + 2). For urgent recall: false negatives are actual urgent items predicted as normal or spam (3 + 1). And similarly for the other classes.",
    "enhanced_text": "[NLP] Multi-class Confusion Matrix and Evaluation For multiple classes (like \"urgent,\" \"normal,\" and \"spam\"), the confusion matrix expands beyond 2x2, and you compute precision and recall for each class separately: For a given class (e.g., \"urgent\"): True Positives (TP) are items actually urgent and predicted urgent. False Positives (FP) are items predicted urgent but actually belong to other classes (\"normal\" or \"spam\"). False Negatives (FN) are actual urgent items predicted as other classes. Precision for \"urgent\" = TP_urgent / (TP_urgent + FP_urgent) Recall for \"urgent\" = TP_urgent / (TP_urgent + FN_urgent) Similarly, you compute precision and recall for \"normal\" and \"spam\" classes. Overall accuracy is the sum of all correct predictions (all true positives across classes) divided by total items. Multi-class Exercise Example (no table format) Imagine a dataset with these counts: Actual urgent items predicted as urgent: 96 Actual urgent items predicted as normal: 3 Actual urgent items predicted as spam: 1 Actual normal items predicted as urgent: 4 Actual normal items predicted as normal: 89 Actual normal items predicted as spam: 7 Actual spam items predicted as urgent: 2 Actual spam items predicted as normal: 3 Actual spam items predicted as spam: 95 You can calculate precision and recall for each class using the formulas above: For urgent precision: true positives are 96; false positives are predictions of urgent that were actually normal or spam (4 + 2). For urgent recall: false negatives are actual urgent items predicted as normal or spam (3 + 1). And similarly for the other classes.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(m) Multi-class Confusion Matrix.txt",
    "file_name": "lec3-(m) Multi-class Confusion Matrix.txt",
    "position_in_document": 25,
    "filename_keywords": [
      "lec3",
      "class",
      "multi",
      "matrix",
      "confusion"
    ],
    "content_keywords": [
      "normal,",
      "urgent",
      "multi",
      "confusion matrix",
      "urgent,",
      "evaluation for",
      "for",
      "spam"
    ],
    "all_keywords": [
      "lec3",
      "class",
      "normal,",
      "urgent",
      "multi",
      "confusion matrix",
      "urgent,",
      "evaluation for",
      "matrix",
      "for",
      "confusion",
      "spam"
    ],
    "keyword_string": "lec3 class normal, urgent multi confusion matrix urgent, evaluation for matrix for confusion spam",
    "token_count": 375,
    "word_count": 252,
    "sentence_count": 8,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.672,
    "avg_sentence_length": 31.5
  },
  {
    "chunk_id": 115,
    "chunk_hash": "266cbadb24b8",
    "text": "Confusion Matrix Example: Email Spam Detection Confusion Matrix Example: Email Spam Detection A spam detection model has been developed and was tested on a dataset of 1000 emails. The goal is to classify emails as either \"Spam\" (positive class) or \"Not Spam\" (negative class, i.e., legitimate email). Test Results Overview: * Total emails tested: 1000 * Actual spam emails in the dataset: 100 * Actual legitimate (not spam) emails in the dataset: 900 The results of the model's predictions are summarized in the following confusion matrix: Predicted: Spam Predicted: Not Spam | Actual Totals +-----------------+---------------------+ Actual: Spam | 85 (TP) | 15 (FN) | 100 +-----------------+---------------------+ Actual: Not | 25 (FP) | 875 (TN) | 900 Spam +-----------------+---------------------+ | Predicted Totals| | | 110 | 890 | 1000 (Grand Total) Breaking down the numbers: True Positives (TP): 85 spam emails correctly identified as spam True Negatives (TN): 875 legitimate emails correctly identified as legitimate False Positives (FP): 25 legitimate emails incorrectly flagged as spam False Negatives (FN): 15 spam emails that slipped through as legitimate Calculated Performance Metrics: * Formula: (TP + TN) / (TP + FP + FN + TN) * Calculation: (85 + 875) / (85 + 25 + 15 + 875) = 960 / 1000 = 0.96 * Interpretation: 96% or 0.96. The model correctly classified 960 out of the 1000 emails overall. Precision (for the \"Spam\" class): * Formula: TP / (TP + FP) * Calculation: 85 / (85 + 25) = 85 / 110 ≈ 0.773 * Interpretation: Approximately 77.3%.",
    "enhanced_text": "[NLP] Confusion Matrix Example: Email Spam Detection Confusion Matrix Example: Email Spam Detection A spam detection model has been developed and was tested on a dataset of 1000 emails. The goal is to classify emails as either \"Spam\" (positive class) or \"Not Spam\" (negative class, i.e., legitimate email). Test Results Overview: * Total emails tested: 1000 * Actual spam emails in the dataset: 100 * Actual legitimate (not spam) emails in the dataset: 900 The results of the model's predictions are summarized in the following confusion matrix: Predicted: Spam Predicted: Not Spam | Actual Totals +-----------------+---------------------+ Actual: Spam | 85 (TP) | 15 (FN) | 100 +-----------------+---------------------+ Actual: Not | 25 (FP) | 875 (TN) | 900 Spam +-----------------+---------------------+ | Predicted Totals| | | 110 | 890 | 1000 (Grand Total) Breaking down the numbers: True Positives (TP): 85 spam emails correctly identified as spam True Negatives (TN): 875 legitimate emails correctly identified as legitimate False Positives (FP): 25 legitimate emails incorrectly flagged as spam False Negatives (FN): 15 spam emails that slipped through as legitimate Calculated Performance Metrics: * Formula: (TP + TN) / (TP + FP + FN + TN) * Calculation: (85 + 875) / (85 + 25 + 15 + 875) = 960 / 1000 = 0.96 * Interpretation: 96% or 0.96. The model correctly classified 960 out of the 1000 emails overall. Precision (for the \"Spam\" class): * Formula: TP / (TP + FP) * Calculation: 85 / (85 + 25) = 85 / 110 ≈ 0.773 * Interpretation: Approximately 77.3%.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3_(n)_Example_of_Confusion_matrix_percision_recall_accuracy_F1_Score.txt",
    "file_name": "lec3_(n)_Example_of_Confusion_matrix_percision_recall_accuracy_F1_Score.txt",
    "position_in_document": 31,
    "filename_keywords": [
      "example",
      "score",
      "lec3",
      "percision",
      "recall",
      "matrix",
      "confusion",
      "accuracy"
    ],
    "content_keywords": [
      "email spam detection a",
      "confusion matrix example",
      "email spam detection confusion matrix example"
    ],
    "all_keywords": [
      "example",
      "email spam detection confusion matrix example",
      "score",
      "lec3",
      "percision",
      "email spam detection a",
      "recall",
      "matrix",
      "confusion matrix example",
      "confusion",
      "accuracy"
    ],
    "keyword_string": "example email spam detection confusion matrix example score lec3 percision email spam detection a recall matrix confusion matrix example confusion accuracy",
    "token_count": 506,
    "word_count": 255,
    "sentence_count": 5,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5039525691699605,
    "avg_sentence_length": 51.0
  },
  {
    "chunk_id": 116,
    "chunk_hash": "d307d00c392d",
    "text": "The model correctly classified 960 out of the 1000 emails overall. Precision (for the \"Spam\" class): * Formula: TP / (TP + FP) * Calculation: 85 / (85 + 25) = 85 / 110 ≈ 0.773 * Interpretation: Approximately 77.3%. Of all the emails that the model predicted as spam, 77.3% were actually spam. This means that about 22.7% (100% - 77.3%) of emails flagged as spam were actually legitimate (false alarms). Recall (Sensitivity, True Positive Rate for the \"Spam\" class): * Formula: TP / (TP + FN) * Calculation: 85 / (85 + 15) = 85 / 100 = 0.85 * Interpretation: 85%. The model successfully caught 85% of all the actual spam emails present in the dataset. However, it missed 15% of the spam emails. F1-Score (for the \"Spam\" class): * Formula: 2 * (Precision * Recall) / (Precision + Recall) * Calculation (using rounded precision 0.77 for simplicity as in original): 2 * (0.77 * 0.85) / (0.77 + 0.85) = 2 * 0.6545 / 1.62 = 1.309 / 1.62 ≈ 0.808 * (Using more precise precision 0.773: 2 * (0.773 * 0.85) / (0.773 + 0.85) = 2 * 0.65705 / 1.623 = 1.3141 / 1.623 ≈ 0.809) * Interpretation: Approximately 81%. This balanced metric, which considers both precision and recall, indicates that the model has a reasonably good overall performance in identifying spam, though there's room for improvement, especially in precision. Accuracy: (85 + 875) / 1000 = 0.96 or 96% The model correctly classified 960 out of 1000 emails. Precision: 85 / (85 + 25) = 0.77 or 77% Of all emails flagged as spam, 77% were actually spam. This means 23% of flagged emails were false alarms. Recall: 85 / (85 + 15) = 0.85 or 85% The model caught 85% of all actual spam emails, but missed 15% of spam. F1-Score: 2 × (0.77 × 0.85) / (0.77 + 0.85) = 0.81 or 81% This balanced metric shows the model performs reasonably well overall.",
    "enhanced_text": "[NLP] The model correctly classified 960 out of the 1000 emails overall. Precision (for the \"Spam\" class): * Formula: TP / (TP + FP) * Calculation: 85 / (85 + 25) = 85 / 110 ≈ 0.773 * Interpretation: Approximately 77.3%. Of all the emails that the model predicted as spam, 77.3% were actually spam. This means that about 22.7% (100% - 77.3%) of emails flagged as spam were actually legitimate (false alarms). Recall (Sensitivity, True Positive Rate for the \"Spam\" class): * Formula: TP / (TP + FN) * Calculation: 85 / (85 + 15) = 85 / 100 = 0.85 * Interpretation: 85%. The model successfully caught 85% of all the actual spam emails present in the dataset. However, it missed 15% of the spam emails. F1-Score (for the \"Spam\" class): * Formula: 2 * (Precision * Recall) / (Precision + Recall) * Calculation (using rounded precision 0.77 for simplicity as in original): 2 * (0.77 * 0.85) / (0.77 + 0.85) = 2 * 0.6545 / 1.62 = 1.309 / 1.62 ≈ 0.808 * (Using more precise precision 0.773: 2 * (0.773 * 0.85) / (0.773 + 0.85) = 2 * 0.65705 / 1.623 = 1.3141 / 1.623 ≈ 0.809) * Interpretation: Approximately 81%. This balanced metric, which considers both precision and recall, indicates that the model has a reasonably good overall performance in identifying spam, though there's room for improvement, especially in precision. Accuracy: (85 + 875) / 1000 = 0.96 or 96% The model correctly classified 960 out of 1000 emails. Precision: 85 / (85 + 25) = 0.77 or 77% Of all emails flagged as spam, 77% were actually spam. This means 23% of flagged emails were false alarms. Recall: 85 / (85 + 15) = 0.85 or 85% The model caught 85% of all actual spam emails, but missed 15% of spam. F1-Score: 2 × (0.77 × 0.85) / (0.77 + 0.85) = 0.81 or 81% This balanced metric shows the model performs reasonably well overall.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3_(n)_Example_of_Confusion_matrix_percision_recall_accuracy_F1_Score.txt",
    "file_name": "lec3_(n)_Example_of_Confusion_matrix_percision_recall_accuracy_F1_Score.txt",
    "position_in_document": 50,
    "filename_keywords": [
      "example",
      "score",
      "lec3",
      "percision",
      "recall",
      "matrix",
      "confusion",
      "accuracy"
    ],
    "content_keywords": [
      "email spam detection a",
      "confusion matrix example",
      "email spam detection confusion matrix example"
    ],
    "all_keywords": [
      "example",
      "email spam detection confusion matrix example",
      "score",
      "lec3",
      "percision",
      "email spam detection a",
      "recall",
      "matrix",
      "confusion matrix example",
      "confusion",
      "accuracy"
    ],
    "keyword_string": "example email spam detection confusion matrix example score lec3 percision email spam detection a recall matrix confusion matrix example confusion accuracy",
    "token_count": 548,
    "word_count": 332,
    "sentence_count": 14,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6058394160583942,
    "avg_sentence_length": 23.714285714285715
  },
  {
    "chunk_id": 117,
    "chunk_hash": "819ff6ecc095",
    "text": "Probability in Natural Language Processing (NLP) Natural language is inherently filled with uncertainty and ambiguity. * Words can have multiple meanings (polysemy) depending on the surrounding context (e.g., \"bank\" can be a financial institution or the side of a river). * Sentences can be structured in many grammatically correct ways while conveying similar or different meanings. * Textual data can be noisy, containing misspellings, ungrammatical constructions, or incomplete information. Probability theory provides a mathematical framework to model and manage this uncertainty in language. Probabilistic Models in NLP: Many advanced NLP techniques and algorithms are built upon probabilistic foundations. These models use probability to: * Predict or generate language (e.g., predicting the next word in a sentence). * Estimate the likelihood of certain words, sequences of words (phrases, sentences), or linguistic structures occurring. * Make decisions or classifications based on patterns and likelihoods observed in textual data. ________________________________________ Key Examples in NLP Using Probability * Concept: A language model assigns a probability to a sequence of words. It aims to capture how likely a given phrase or sentence is in a particular language. * Example: Estimate P(\"I love NLP\") – this represents the probability that the specific phrase \"I love NLP\" would appear naturally in English text. A well-trained language model would assign a higher probability to \"I love NLP\" than to a nonsensical or ungrammatical sequence like \"NLP love I\". * Autocomplete / Predictive Text: Suggests the most probable next words as a user types. * Speech Recognition: Helps distinguish between acoustically similar phrases by choosing the one that is more probable in the language (e.g., \"recognize speech\" vs. \"wreck a nice beach\"). * Machine Translation: Helps ensure the translated output is fluent and natural in the target language. * Spelling/Grammar Correction: Identifies and suggests corrections for improbable word sequences. Naive Bayes Classifier: * Concept: A classification algorithm based on Bayes' Theorem, with a \"naive\" assumption of conditional independence between features (words). * Application: Used to classify text into predefined categories.",
    "enhanced_text": "[NLP] Probability in Natural Language Processing (NLP) Natural language is inherently filled with uncertainty and ambiguity. * Words can have multiple meanings (polysemy) depending on the surrounding context (e.g., \"bank\" can be a financial institution or the side of a river). * Sentences can be structured in many grammatically correct ways while conveying similar or different meanings. * Textual data can be noisy, containing misspellings, ungrammatical constructions, or incomplete information. Probability theory provides a mathematical framework to model and manage this uncertainty in language. Probabilistic Models in NLP: Many advanced NLP techniques and algorithms are built upon probabilistic foundations. These models use probability to: * Predict or generate language (e.g., predicting the next word in a sentence). * Estimate the likelihood of certain words, sequences of words (phrases, sentences), or linguistic structures occurring. * Make decisions or classifications based on patterns and likelihoods observed in textual data. ________________________________________ Key Examples in NLP Using Probability * Concept: A language model assigns a probability to a sequence of words. It aims to capture how likely a given phrase or sentence is in a particular language. * Example: Estimate P(\"I love NLP\") – this represents the probability that the specific phrase \"I love NLP\" would appear naturally in English text. A well-trained language model would assign a higher probability to \"I love NLP\" than to a nonsensical or ungrammatical sequence like \"NLP love I\". * Autocomplete / Predictive Text: Suggests the most probable next words as a user types. * Speech Recognition: Helps distinguish between acoustically similar phrases by choosing the one that is more probable in the language (e.g., \"recognize speech\" vs. \"wreck a nice beach\"). * Machine Translation: Helps ensure the translated output is fluent and natural in the target language. * Spelling/Grammar Correction: Identifies and suggests corrections for improbable word sequences. Naive Bayes Classifier: * Concept: A classification algorithm based on Bayes' Theorem, with a \"naive\" assumption of conditional independence between features (words). * Application: Used to classify text into predefined categories.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(a) Probability.txt",
    "file_name": "lec4-(a) Probability.txt",
    "position_in_document": 25,
    "filename_keywords": [
      "probability",
      "lec4"
    ],
    "content_keywords": [
      "words",
      "natural language processing",
      "probability",
      "nlp",
      "bank",
      "natural"
    ],
    "all_keywords": [
      "words",
      "natural language processing",
      "probability",
      "nlp",
      "lec4",
      "bank",
      "natural"
    ],
    "keyword_string": "words natural language processing probability nlp lec4 bank natural",
    "token_count": 502,
    "word_count": 331,
    "sentence_count": 19,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6593625498007968,
    "avg_sentence_length": 17.42105263157895
  },
  {
    "chunk_id": 118,
    "chunk_hash": "792e68403c4e",
    "text": "* Machine Translation: Helps ensure the translated output is fluent and natural in the target language. * Spelling/Grammar Correction: Identifies and suggests corrections for improbable word sequences. Naive Bayes Classifier: * Concept: A classification algorithm based on Bayes' Theorem, with a \"naive\" assumption of conditional independence between features (words). * Application: Used to classify text into predefined categories. * Spam Detection: Calculates P(Spam | Document_Words) – the probability that an email is spam given the words it contains. * Sentiment Analysis: Calculates P(Positive_Sentiment | Review_Text) – the probability that a product review expresses positive sentiment given its text. * Mechanism: It learns the probability of words appearing in documents of different classes from training data (e.g., P(\"free\" | Spam) vs. P(\"free\" | Not_Spam)). Word Sense Disambiguation (WSD): * Concept: The task of identifying which specific meaning (sense) of a word is used in a particular context, especially when the word has multiple meanings. * Probabilistic Approach: Models estimate P(Sense_k | Context_Words) – the probability of a particular sense_k of a target word being the correct one, given the surrounding words (the context). Part-of-Speech (POS) Tagging: * Concept: Assigning grammatical categories (like noun, verb, adjective, adverb, etc.) to each word in a sentence. * Probabilistic Approach: Models like Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs) assign the most probable sequence of tags to a sentence. * P(Tag | Word): Probability of a tag given a word (e.g., \"book\" is more likely P(Noun | \"book\") than P(Verb | \"book\") in many contexts, but the reverse can also be true). * P(Current_Tag | Previous_Tag): Probability of a tag given the preceding tag (e.g., a determiner is often followed by a noun). Statistical Machine Translation (SMT) (though increasingly replaced by Neural Machine Translation, SMT laid important groundwork): * Concept: Translates text from one language (source) to another (target) by modeling probabilities. * Probabilistic Approach: Aims to find the target sentence (T) that maximizes P(T | S), where S is the source sentence. Using Bayes' theorem, this is often decomposed into: * P(S | T) (Translation Model): Probability that S is the translation of T (learned from parallel corpora). * P(T) (Language Model): Probability of T being a fluent sentence in the target language.",
    "enhanced_text": "[NLP] * Machine Translation: Helps ensure the translated output is fluent and natural in the target language. * Spelling/Grammar Correction: Identifies and suggests corrections for improbable word sequences. Naive Bayes Classifier: * Concept: A classification algorithm based on Bayes' Theorem, with a \"naive\" assumption of conditional independence between features (words). * Application: Used to classify text into predefined categories. * Spam Detection: Calculates P(Spam | Document_Words) – the probability that an email is spam given the words it contains. * Sentiment Analysis: Calculates P(Positive_Sentiment | Review_Text) – the probability that a product review expresses positive sentiment given its text. * Mechanism: It learns the probability of words appearing in documents of different classes from training data (e.g., P(\"free\" | Spam) vs. P(\"free\" | Not_Spam)). Word Sense Disambiguation (WSD): * Concept: The task of identifying which specific meaning (sense) of a word is used in a particular context, especially when the word has multiple meanings. * Probabilistic Approach: Models estimate P(Sense_k | Context_Words) – the probability of a particular sense_k of a target word being the correct one, given the surrounding words (the context). Part-of-Speech (POS) Tagging: * Concept: Assigning grammatical categories (like noun, verb, adjective, adverb, etc.) to each word in a sentence. * Probabilistic Approach: Models like Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs) assign the most probable sequence of tags to a sentence. * P(Tag | Word): Probability of a tag given a word (e.g., \"book\" is more likely P(Noun | \"book\") than P(Verb | \"book\") in many contexts, but the reverse can also be true). * P(Current_Tag | Previous_Tag): Probability of a tag given the preceding tag (e.g., a determiner is often followed by a noun). Statistical Machine Translation (SMT) (though increasingly replaced by Neural Machine Translation, SMT laid important groundwork): * Concept: Translates text from one language (source) to another (target) by modeling probabilities. * Probabilistic Approach: Aims to find the target sentence (T) that maximizes P(T | S), where S is the source sentence. Using Bayes' theorem, this is often decomposed into: * P(S | T) (Translation Model): Probability that S is the translation of T (learned from parallel corpora). * P(T) (Language Model): Probability of T being a fluent sentence in the target language.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(a) Probability.txt",
    "file_name": "lec4-(a) Probability.txt",
    "position_in_document": 43,
    "filename_keywords": [
      "probability",
      "lec4"
    ],
    "content_keywords": [
      "words",
      "natural language processing",
      "probability",
      "nlp",
      "bank",
      "natural"
    ],
    "all_keywords": [
      "words",
      "natural language processing",
      "probability",
      "nlp",
      "lec4",
      "bank",
      "natural"
    ],
    "keyword_string": "words natural language processing probability nlp lec4 bank natural",
    "token_count": 596,
    "word_count": 371,
    "sentence_count": 18,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.62248322147651,
    "avg_sentence_length": 20.61111111111111
  },
  {
    "chunk_id": 119,
    "chunk_hash": "ee159ed76af1",
    "text": "Conditional Probability in Natural Language Processing (NLP) Conditional probability is a fundamental concept in probability theory that measures the likelihood of an event occurring, given that another event has already occurred. In NLP, this \"event\" can be the occurrence of a word, a phrase, a part-of-speech tag, a document class, or any other linguistic feature. Modeling the likelihood of one linguistic event given the presence or context of another is central to how many NLP tasks are approached. ________________________________________ NLP Examples Using Conditional Probability * Goal: To predict the probability of the next word (w_n) in a sequence, given the sequence of words that have come before it (w_1, w_2, ..., w_{n-1}). * Notation: This is written as P(w_n | w_1, w_2, ..., w_{n-1}). This reads as \"the probability of word w_n given the preceding words w_1 through w_{n-1}.\" * P(\"morning\" | \"Good\") would likely be high, as \"Good morning\" is a common phrase. * P(\"banana\" | \"Good\") would likely be very low, as \"Good banana\" is not a typical sequence. * How it's used: Language models, such as n-gram models or more advanced neural network models (like RNNs or Transformers), estimate these conditional probabilities. N-gram models, for instance, simplify this by assuming the next word only depends on a fixed number of preceding words (e.g., P(w_n | w_{n-1}) for a bigram model). Naive Bayes Text Classification: * Goal: To assign a category or class label to a document (e.g., spam/not-spam, positive/negative sentiment). This involves calculating P(Class | Document) — the probability that a document belongs to a particular class, given the words (or other features) in that document. * Using Bayes’ Theorem: P(Class | Document) = [P(Document | Class) * P(Class)] / P(Document)",
    "enhanced_text": "[NLP] Conditional Probability in Natural Language Processing (NLP) Conditional probability is a fundamental concept in probability theory that measures the likelihood of an event occurring, given that another event has already occurred. In NLP, this \"event\" can be the occurrence of a word, a phrase, a part-of-speech tag, a document class, or any other linguistic feature. Modeling the likelihood of one linguistic event given the presence or context of another is central to how many NLP tasks are approached. ________________________________________ NLP Examples Using Conditional Probability * Goal: To predict the probability of the next word (w_n) in a sequence, given the sequence of words that have come before it (w_1, w_2, ..., w_{n-1}). * Notation: This is written as P(w_n | w_1, w_2, ..., w_{n-1}). This reads as \"the probability of word w_n given the preceding words w_1 through w_{n-1}.\" * P(\"morning\" | \"Good\") would likely be high, as \"Good morning\" is a common phrase. * P(\"banana\" | \"Good\") would likely be very low, as \"Good banana\" is not a typical sequence. * How it's used: Language models, such as n-gram models or more advanced neural network models (like RNNs or Transformers), estimate these conditional probabilities. N-gram models, for instance, simplify this by assuming the next word only depends on a fixed number of preceding words (e.g., P(w_n | w_{n-1}) for a bigram model). Naive Bayes Text Classification: * Goal: To assign a category or class label to a document (e.g., spam/not-spam, positive/negative sentiment). This involves calculating P(Class | Document) — the probability that a document belongs to a particular class, given the words (or other features) in that document. * Using Bayes’ Theorem: P(Class | Document) = [P(Document | Class) * P(Class)] / P(Document)",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(b) Conditional Probability.txt",
    "file_name": "lec4-(b) Conditional Probability.txt",
    "position_in_document": 18,
    "filename_keywords": [
      "probability",
      "lec4",
      "conditional"
    ],
    "content_keywords": [
      "natural language processing",
      "conditional",
      "in nlp",
      "event",
      "nlp",
      "conditional probability"
    ],
    "all_keywords": [
      "conditional probability",
      "natural language processing",
      "conditional",
      "probability",
      "in nlp",
      "event",
      "nlp",
      "lec4"
    ],
    "keyword_string": "conditional probability natural language processing conditional probability in nlp event nlp lec4",
    "token_count": 505,
    "word_count": 284,
    "sentence_count": 13,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5623762376237624,
    "avg_sentence_length": 21.846153846153847
  },
  {
    "chunk_id": 120,
    "chunk_hash": "b8ed16417e16",
    "text": "This involves calculating P(Class | Document) — the probability that a document belongs to a particular class, given the words (or other features) in that document. * Using Bayes’ Theorem: P(Class | Document) = [P(Document | Class) * P(Class)] / P(Document) The term P(Document | Class) is often broken down (assuming conditional independence of words, hence \"Naive\") into a product of conditional probabilities of individual words given the class. * Example conditional probabilities involved: * P(\"free\" | spam): The probability of the word \"free\" appearing in a document, given that the document is spam. * P(\"meeting\" | not-spam): The probability of the word \"meeting\" appearing, given the document is not spam. * How it's used: The classifier assigns the class that has the highest posterior probability P(Class | Document). Part-of-Speech (POS) Tagging: * Goal: To assign the correct grammatical tag (e.g., noun, verb, adjective) to each word in a sentence. * Conditional probabilities used: * Transition Probabilities: P(Tag_i | Tag_{i-1}), the probability of the current tag (Tag_i) given the previous tag (Tag_{i-1}). This captures grammatical structure (e.g., a determiner is often followed by a noun). * Emission/Observation Probabilities: P(Word_i | Tag_i), the probability of a specific word (Word_i) being observed, given a particular tag (Tag_i). For example, P(\"run\" | Verb) would be higher than P(\"run\" | Noun) if \"run\" is more commonly a verb. (Though sometimes P(Tag_i | Word_i) is also considered). * How it's used: Models like Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs) use these probabilities to find the most likely sequence of POS tags for a given sentence. Word Sense Disambiguation (WSD): * Goal: To identify the correct meaning (sense) of a word that has multiple meanings (polysemy), based on its surrounding context. * Notation: P(Sense_k | Context) – the probability of a word having a particular sense (Sense_k) given the words or features in its surrounding context. * Example: For the word \"bank,\" * P(financial_institution_sense | \"money\", \"loan\", \"account\") would be high.",
    "enhanced_text": "[NLP] This involves calculating P(Class | Document) — the probability that a document belongs to a particular class, given the words (or other features) in that document. * Using Bayes’ Theorem: P(Class | Document) = [P(Document | Class) * P(Class)] / P(Document) The term P(Document | Class) is often broken down (assuming conditional independence of words, hence \"Naive\") into a product of conditional probabilities of individual words given the class. * Example conditional probabilities involved: * P(\"free\" | spam): The probability of the word \"free\" appearing in a document, given that the document is spam. * P(\"meeting\" | not-spam): The probability of the word \"meeting\" appearing, given the document is not spam. * How it's used: The classifier assigns the class that has the highest posterior probability P(Class | Document). Part-of-Speech (POS) Tagging: * Goal: To assign the correct grammatical tag (e.g., noun, verb, adjective) to each word in a sentence. * Conditional probabilities used: * Transition Probabilities: P(Tag_i | Tag_{i-1}), the probability of the current tag (Tag_i) given the previous tag (Tag_{i-1}). This captures grammatical structure (e.g., a determiner is often followed by a noun). * Emission/Observation Probabilities: P(Word_i | Tag_i), the probability of a specific word (Word_i) being observed, given a particular tag (Tag_i). For example, P(\"run\" | Verb) would be higher than P(\"run\" | Noun) if \"run\" is more commonly a verb. (Though sometimes P(Tag_i | Word_i) is also considered). * How it's used: Models like Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs) use these probabilities to find the most likely sequence of POS tags for a given sentence. Word Sense Disambiguation (WSD): * Goal: To identify the correct meaning (sense) of a word that has multiple meanings (polysemy), based on its surrounding context. * Notation: P(Sense_k | Context) – the probability of a word having a particular sense (Sense_k) given the words or features in its surrounding context. * Example: For the word \"bank,\" * P(financial_institution_sense | \"money\", \"loan\", \"account\") would be high.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(b) Conditional Probability.txt",
    "file_name": "lec4-(b) Conditional Probability.txt",
    "position_in_document": 37,
    "filename_keywords": [
      "probability",
      "lec4",
      "conditional"
    ],
    "content_keywords": [
      "natural language processing",
      "conditional",
      "in nlp",
      "event",
      "nlp",
      "conditional probability"
    ],
    "all_keywords": [
      "conditional probability",
      "natural language processing",
      "conditional",
      "probability",
      "in nlp",
      "event",
      "nlp",
      "lec4"
    ],
    "keyword_string": "conditional probability natural language processing conditional probability in nlp event nlp lec4",
    "token_count": 567,
    "word_count": 328,
    "sentence_count": 15,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5784832451499118,
    "avg_sentence_length": 21.866666666666667
  },
  {
    "chunk_id": 121,
    "chunk_hash": "b72620ad2996",
    "text": "Language Models: Deeper Dive A language model is essentially a probability distribution over sequences of words. It tells us how likely a sequence is to appear in a language. Mathematical Formulation Given a sequence of words W=w1,w2,...,wnW = w_1, w_2, ..., w_nW=w1​,w2​,...,wn​, the language model computes: P(W)=P(w1,w2,...,wn)P(W) = P(w_1, w_2, ..., w_n)P(W)=P(w1​,w2​,...,wn​) This joint probability can be decomposed using the chain rule of probability: P(W)=P(w1)×P(w2∣w1)×P(w3∣w1,w2)×⋯×P(wn∣w1,...,wn−1)P(W) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1, w_2) \\times \\cdots \\times P(w_n | w_1, ..., w_{n-1})P(W)=P(w1​)×P(w2​∣w1​)×P(w3​∣w1​,w2​)×⋯×P(wn​∣w1​,...,wn−1​) Because directly computing P(w1,w2,...,wn)P(w_1, w_2, ..., w_n)P(w1​,w2​,...,wn​) is almost impossible due to the enormous number of possible sequences. So instead, we calculate the probability of each word given the words that came before it. Consider the sentence: \"I love natural language processing\" A language model estimates:",
    "enhanced_text": "[NLP] Language Models: Deeper Dive A language model is essentially a probability distribution over sequences of words. It tells us how likely a sequence is to appear in a language. Mathematical Formulation Given a sequence of words W=w1,w2,...,wnW = w_1, w_2, ..., w_nW=w1​,w2​,...,wn​, the language model computes: P(W)=P(w1,w2,...,wn)P(W) = P(w_1, w_2, ..., w_n)P(W)=P(w1​,w2​,...,wn​) This joint probability can be decomposed using the chain rule of probability: P(W)=P(w1)×P(w2∣w1)×P(w3∣w1,w2)×⋯×P(wn∣w1,...,wn−1)P(W) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1, w_2) \\times \\cdots \\times P(w_n | w_1, ..., w_{n-1})P(W)=P(w1​)×P(w2​∣w1​)×P(w3​∣w1​,w2​)×⋯×P(wn​∣w1​,...,wn−1​) Because directly computing P(w1,w2,...,wn)P(w_1, w_2, ..., w_n)P(w1​,w2​,...,wn​) is almost impossible due to the enormous number of possible sequences. So instead, we calculate the probability of each word given the words that came before it. Consider the sentence: \"I love natural language processing\" A language model estimates:",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(c) Language Models.txt",
    "file_name": "lec4-(c) Language Models.txt",
    "position_in_document": 12,
    "filename_keywords": [
      "lec4",
      "models",
      "language"
    ],
    "content_keywords": [
      "language models",
      "deeper dive a"
    ],
    "all_keywords": [
      "language models",
      "deeper dive a",
      "language",
      "models",
      "lec4"
    ],
    "keyword_string": "language models deeper dive a language models lec4",
    "token_count": 403,
    "word_count": 130,
    "sentence_count": 5,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.3225806451612903,
    "avg_sentence_length": 26.0
  },
  {
    "chunk_id": 122,
    "chunk_hash": "0b25a557e7ec",
    "text": "So instead, we calculate the probability of each word given the words that came before it. Consider the sentence: \"I love natural language processing\" A language model estimates: P(\"I love natural language processing\")=P(\"I\")×P(\"love\"∣\"I\")×P(\"natural\"∣\"I love\")×P(\"language\"∣\"I love natural\")×P(\"processing\"∣\"I love natural language\")P(\\text{\"I love natural language processing\"}) = P(\\text{\"I\"}) \\times P(\\text{\"love\"}|\\text{\"I\"}) \\times P(\\text{\"natural\"}|\\text{\"I love\"}) \\times P(\\text{\"language\"}|\\text{\"I love natural\"}) \\times P(\\text{\"processing\"}|\\text{\"I love natural language\"})P(\"I love natural language processing\")=P(\"I\")×P(\"love\"∣\"I\")×P(\"natural\"∣\"I love\")×P(\"language\"∣\"I love natural\")×P(\"processing\"∣\"I love natural language\") Types of Language Models Unigram Model:\nAssumes each word is independent of others: P(W)=∏i=1nP(wi)P(W) = \\prod_{i=1}^{n} P(w_i)P(W)=i=1∏n​P(wi​) (ignores context, so not very accurate) Bigram Model:\nLooks at only the previous word (Markov assumption): P(wi∣w1,...,wi−1)≈P(wi∣wi−1)P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-1})P(wi​∣w1​,...,wi−1​)≈P(wi​∣wi−1​) Trigram Model:\nConsiders two previous words: P(wi∣w1,...,wi−1)≈P(wi∣wi−2,wi−1)P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-2}, w_{i-1})P(wi​∣w1​,...,wi−1​)≈P(wi​∣wi−2​,wi−1​)",
    "enhanced_text": "[NLP] So instead, we calculate the probability of each word given the words that came before it. Consider the sentence: \"I love natural language processing\" A language model estimates: P(\"I love natural language processing\")=P(\"I\")×P(\"love\"∣\"I\")×P(\"natural\"∣\"I love\")×P(\"language\"∣\"I love natural\")×P(\"processing\"∣\"I love natural language\")P(\\text{\"I love natural language processing\"}) = P(\\text{\"I\"}) \\times P(\\text{\"love\"}|\\text{\"I\"}) \\times P(\\text{\"natural\"}|\\text{\"I love\"}) \\times P(\\text{\"language\"}|\\text{\"I love natural\"}) \\times P(\\text{\"processing\"}|\\text{\"I love natural language\"})P(\"I love natural language processing\")=P(\"I\")×P(\"love\"∣\"I\")×P(\"natural\"∣\"I love\")×P(\"language\"∣\"I love natural\")×P(\"processing\"∣\"I love natural language\") Types of Language Models Unigram Model:\nAssumes each word is independent of others: P(W)=∏i=1nP(wi)P(W) = \\prod_{i=1}^{n} P(w_i)P(W)=i=1∏n​P(wi​) (ignores context, so not very accurate) Bigram Model:\nLooks at only the previous word (Markov assumption): P(wi∣w1,...,wi−1)≈P(wi∣wi−1)P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-1})P(wi​∣w1​,...,wi−1​)≈P(wi​∣wi−1​) Trigram Model:\nConsiders two previous words: P(wi∣w1,...,wi−1)≈P(wi∣wi−2,wi−1)P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-2}, w_{i-1})P(wi​∣w1​,...,wi−1​)≈P(wi​∣wi−2​,wi−1​)",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(c) Language Models.txt",
    "file_name": "lec4-(c) Language Models.txt",
    "position_in_document": 21,
    "filename_keywords": [
      "lec4",
      "models",
      "language"
    ],
    "content_keywords": [
      "language models",
      "deeper dive a"
    ],
    "all_keywords": [
      "language models",
      "deeper dive a",
      "language",
      "models",
      "lec4"
    ],
    "keyword_string": "language models deeper dive a language models lec4",
    "token_count": 531,
    "word_count": 127,
    "sentence_count": 2,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.2391713747645951,
    "avg_sentence_length": 63.5
  },
  {
    "chunk_id": 123,
    "chunk_hash": "9c7707c87edb",
    "text": "Quick Recap of the Chain Rule of Probability The chain rule of probability allows us to calculate the joint probability of a sequence of events occurring together. For any sequence of events, say A, B, C, and D: P(A, B, C, D) = P(A) * P(B | A) * P(C | A, B) * P(D | A, B, C) * P(A, B, C, D) is the probability that all events A, B, C, and D occur. * P(A) is the probability of event A occurring. * P(B | A) is the conditional probability of event B occurring, given that event A has already occurred. * P(C | A, B) is the conditional probability of event C occurring, given that events A and B have already occurred. * P(D | A, B, C) is the conditional probability of event D occurring, given that events A, B, and C have already occurred. This rule expresses the joint probability of a sequence of events as a product of conditional probabilities, where the probability of each subsequent event is conditioned on all the preceding events. ________________________________________ Applying the Chain Rule to Words in a Sentence We can apply the chain rule to determine the probability of a sequence of words, i.e., a sentence. Consider the sentence: \"its water is so transparent\" The joint probability of this entire sentence occurring is: P(\"its\", \"water\", \"is\", \"so\", \"transparent\") = P(\"is\" | \"its\", \"water\") * P(\"so\" | \"its\", \"water\", \"is\") * P(\"transparent\" | \"its\", \"water\", \"is\", \"so\") ________________________________________ Intuition Behind It (for the example sentence) Let's break down what each term means: * P(\"its\"): This is the probability that a sentence (or a text segment) starts with the word \"its\".",
    "enhanced_text": "[NLP] Quick Recap of the Chain Rule of Probability The chain rule of probability allows us to calculate the joint probability of a sequence of events occurring together. For any sequence of events, say A, B, C, and D: P(A, B, C, D) = P(A) * P(B | A) * P(C | A, B) * P(D | A, B, C) * P(A, B, C, D) is the probability that all events A, B, C, and D occur. * P(A) is the probability of event A occurring. * P(B | A) is the conditional probability of event B occurring, given that event A has already occurred. * P(C | A, B) is the conditional probability of event C occurring, given that events A and B have already occurred. * P(D | A, B, C) is the conditional probability of event D occurring, given that events A, B, and C have already occurred. This rule expresses the joint probability of a sequence of events as a product of conditional probabilities, where the probability of each subsequent event is conditioned on all the preceding events. ________________________________________ Applying the Chain Rule to Words in a Sentence We can apply the chain rule to determine the probability of a sequence of words, i.e., a sentence. Consider the sentence: \"its water is so transparent\" The joint probability of this entire sentence occurring is: P(\"its\", \"water\", \"is\", \"so\", \"transparent\") = P(\"is\" | \"its\", \"water\") * P(\"so\" | \"its\", \"water\", \"is\") * P(\"transparent\" | \"its\", \"water\", \"is\", \"so\") ________________________________________ Intuition Behind It (for the example sentence) Let's break down what each term means: * P(\"its\"): This is the probability that a sentence (or a text segment) starts with the word \"its\".",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(d) Chain Rule of Probability.txt",
    "file_name": "lec4-(d) Chain Rule of Probability.txt",
    "position_in_document": 24,
    "filename_keywords": [
      "probability",
      "lec4",
      "chain",
      "rule"
    ],
    "content_keywords": [
      "for",
      "chain rule",
      "quick recap",
      "probability the"
    ],
    "all_keywords": [
      "rule",
      "probability the",
      "probability",
      "quick recap",
      "for",
      "lec4",
      "chain",
      "chain rule"
    ],
    "keyword_string": "rule probability the probability quick recap for lec4 chain chain rule",
    "token_count": 508,
    "word_count": 282,
    "sentence_count": 9,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5551181102362205,
    "avg_sentence_length": 31.333333333333332
  },
  {
    "chunk_id": 124,
    "chunk_hash": "7ec62056251c",
    "text": "* P(\"water\" | \"its\"): This is the probability that the word \"water\" appears immediately after the word \"its\". * P(\"is\" | \"its\", \"water\"): This is the probability that the word \"is\" appears immediately after the sequence \"its water\". * P(\"so\" | \"its\", \"water\", \"is\"): This is the probability that the word \"so\" appears immediately after the sequence \"its water is\". * P(\"transparent\" | \"its\", \"water\", \"is\", \"so\"): This is the probability that the word \"transparent\" appears immediately after the sequence \"its water is so\". Essentially, each conditional probability term tells us how likely the next word is, given the full context of all the words that have come before it in the sentence. ________________________________________ Why is This Important for Language Modeling? * It captures context: The chain rule provides a formal way to express how the probability of a word depends on the preceding words. This dependency is crucial for understanding and modeling the structure and meaning of natural language. * It enables language models to assign probabilities to sentences: By calculating this joint probability, a language model can determine how likely a given sentence is according to the model's learned understanding of the language. * Applications: This ability to assign probabilities to sequences of words is fundamental for many NLP tasks, such as: * Predicting the next word in a sequence (e.g., in auto-complete features). * Spelling and grammar correction (suggesting more probable sequences). * Machine translation (choosing the most probable translation). * Speech recognition (disambiguating between acoustically similar phrases). * Text generation (creating coherent and natural-sounding text). ________________________________________ Practical Note: The Need for Approximation",
    "enhanced_text": "[NLP] * P(\"water\" | \"its\"): This is the probability that the word \"water\" appears immediately after the word \"its\". * P(\"is\" | \"its\", \"water\"): This is the probability that the word \"is\" appears immediately after the sequence \"its water\". * P(\"so\" | \"its\", \"water\", \"is\"): This is the probability that the word \"so\" appears immediately after the sequence \"its water is\". * P(\"transparent\" | \"its\", \"water\", \"is\", \"so\"): This is the probability that the word \"transparent\" appears immediately after the sequence \"its water is so\". Essentially, each conditional probability term tells us how likely the next word is, given the full context of all the words that have come before it in the sentence. ________________________________________ Why is This Important for Language Modeling? * It captures context: The chain rule provides a formal way to express how the probability of a word depends on the preceding words. This dependency is crucial for understanding and modeling the structure and meaning of natural language. * It enables language models to assign probabilities to sentences: By calculating this joint probability, a language model can determine how likely a given sentence is according to the model's learned understanding of the language. * Applications: This ability to assign probabilities to sequences of words is fundamental for many NLP tasks, such as: * Predicting the next word in a sequence (e.g., in auto-complete features). * Spelling and grammar correction (suggesting more probable sequences). * Machine translation (choosing the most probable translation). * Speech recognition (disambiguating between acoustically similar phrases). * Text generation (creating coherent and natural-sounding text). ________________________________________ Practical Note: The Need for Approximation",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(d) Chain Rule of Probability.txt",
    "file_name": "lec4-(d) Chain Rule of Probability.txt",
    "position_in_document": 42,
    "filename_keywords": [
      "probability",
      "lec4",
      "chain",
      "rule"
    ],
    "content_keywords": [
      "for",
      "chain rule",
      "quick recap",
      "probability the"
    ],
    "all_keywords": [
      "rule",
      "probability the",
      "probability",
      "quick recap",
      "for",
      "lec4",
      "chain",
      "chain rule"
    ],
    "keyword_string": "rule probability the probability quick recap for lec4 chain chain rule",
    "token_count": 463,
    "word_count": 266,
    "sentence_count": 15,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5745140388768899,
    "avg_sentence_length": 17.733333333333334
  },
  {
    "chunk_id": 125,
    "chunk_hash": "14731ae87157",
    "text": "* Spelling and grammar correction (suggesting more probable sequences). * Machine translation (choosing the most probable translation). * Speech recognition (disambiguating between acoustically similar phrases). * Text generation (creating coherent and natural-sounding text). ________________________________________ Practical Note: The Need for Approximation While the chain rule provides a theoretically complete way to calculate the probability of a sentence, directly estimating the conditional probabilities P(word_n | word_1, ..., word_{n-1}) for long contexts is very difficult in practice: * Data Sparsity: Many long sequences of words will occur very rarely, or not at all, in any finite training corpus, making it impossible to get reliable probability estimates. * Computational Complexity: The number of possible preceding contexts grows exponentially with the length of the context. To address these issues, practical language models often use approximations like n-gram models (as discussed previously). N-gram models make a Markov assumption, stating that the probability of a word depends only on a fixed number (n-1) of preceding words, rather than the entire history. For example, a bigram model approximates P(word_k | word_1, ..., word_{k-1}) with P(word_k | word_{k-1}).",
    "enhanced_text": "[NLP] * Spelling and grammar correction (suggesting more probable sequences). * Machine translation (choosing the most probable translation). * Speech recognition (disambiguating between acoustically similar phrases). * Text generation (creating coherent and natural-sounding text). ________________________________________ Practical Note: The Need for Approximation While the chain rule provides a theoretically complete way to calculate the probability of a sentence, directly estimating the conditional probabilities P(word_n | word_1, ..., word_{n-1}) for long contexts is very difficult in practice: * Data Sparsity: Many long sequences of words will occur very rarely, or not at all, in any finite training corpus, making it impossible to get reliable probability estimates. * Computational Complexity: The number of possible preceding contexts grows exponentially with the length of the context. To address these issues, practical language models often use approximations like n-gram models (as discussed previously). N-gram models make a Markov assumption, stating that the probability of a word depends only on a fixed number (n-1) of preceding words, rather than the entire history. For example, a bigram model approximates P(word_k | word_1, ..., word_{k-1}) with P(word_k | word_{k-1}).",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(d) Chain Rule of Probability.txt",
    "file_name": "lec4-(d) Chain Rule of Probability.txt",
    "position_in_document": 48,
    "filename_keywords": [
      "probability",
      "lec4",
      "chain",
      "rule"
    ],
    "content_keywords": [
      "for",
      "chain rule",
      "quick recap",
      "probability the"
    ],
    "all_keywords": [
      "rule",
      "probability the",
      "probability",
      "quick recap",
      "for",
      "lec4",
      "chain",
      "chain rule"
    ],
    "keyword_string": "rule probability the probability quick recap for lec4 chain chain rule",
    "token_count": 319,
    "word_count": 179,
    "sentence_count": 9,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5611285266457681,
    "avg_sentence_length": 19.88888888888889
  },
  {
    "chunk_id": 126,
    "chunk_hash": "8250ead1a9cc",
    "text": "What is an N-gram Model? * An n-gram is a contiguous sequence of 'n' items (typically words, but can also be characters or other linguistic units) from a given sample of text or speech. * Unigram (1-gram): A single word. Examples: \"please\", \"turn\", \"your\", \"homework\", \"in\" * Bigram (2-gram): A sequence of two consecutive words. Examples: \"please turn\", \"turn your\", \"your homework\", \"homework in\" * Trigram (3-gram): A sequence of three consecutive words. Examples: \"please turn your\", \"turn your homework\", \"your homework in\" * And so on for 4-grams (four words), 5-grams (five words), etc. * An n-gram model is a type of probabilistic language model that predicts the probability of a given word (w_n) occurring after a sequence of preceding words. It makes a simplifying assumption (called the Markov assumption) that the probability of the next word depends only on the previous 'n-1' words. * The general probability of a word w_n given all preceding words w_1, w_2, ..., w_{n-1} is written as: P(w_n | w_1, w_2, ..., w_{n-1}) * In an n-gram model, this probability is approximated by considering only the 'n-1' immediately preceding words: P(w_n | w_1, w_2, ..., w_{n-1}) is approximated by P(w_n | w_{n-(n-1)}, ..., w_{n-1}) This means the model looks at a context window of size 'n-1' to predict the n-th word. For specific values of 'n': * Unigram model (n=1): Assumes the probability of a word is independent of any prior words. P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n) This is simply the frequency of the word in the corpus.",
    "enhanced_text": "[NLP] What is an N-gram Model? * An n-gram is a contiguous sequence of 'n' items (typically words, but can also be characters or other linguistic units) from a given sample of text or speech. * Unigram (1-gram): A single word. Examples: \"please\", \"turn\", \"your\", \"homework\", \"in\" * Bigram (2-gram): A sequence of two consecutive words. Examples: \"please turn\", \"turn your\", \"your homework\", \"homework in\" * Trigram (3-gram): A sequence of three consecutive words. Examples: \"please turn your\", \"turn your homework\", \"your homework in\" * And so on for 4-grams (four words), 5-grams (five words), etc. * An n-gram model is a type of probabilistic language model that predicts the probability of a given word (w_n) occurring after a sequence of preceding words. It makes a simplifying assumption (called the Markov assumption) that the probability of the next word depends only on the previous 'n-1' words. * The general probability of a word w_n given all preceding words w_1, w_2, ..., w_{n-1} is written as: P(w_n | w_1, w_2, ..., w_{n-1}) * In an n-gram model, this probability is approximated by considering only the 'n-1' immediately preceding words: P(w_n | w_1, w_2, ..., w_{n-1}) is approximated by P(w_n | w_{n-(n-1)}, ..., w_{n-1}) This means the model looks at a context window of size 'n-1' to predict the n-th word. For specific values of 'n': * Unigram model (n=1): Assumes the probability of a word is independent of any prior words. P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n) This is simply the frequency of the word in the corpus.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(e) N-gram Model.txt",
    "file_name": "lec4-(e) N-gram Model.txt",
    "position_in_document": 20,
    "filename_keywords": [
      "lec4",
      "gram",
      "model"
    ],
    "content_keywords": [
      "unigram",
      "model",
      "what"
    ],
    "all_keywords": [
      "model",
      "what",
      "unigram",
      "lec4",
      "gram"
    ],
    "keyword_string": "model what unigram lec4 gram",
    "token_count": 495,
    "word_count": 259,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5232323232323233,
    "avg_sentence_length": 23.545454545454547
  },
  {
    "chunk_id": 127,
    "chunk_hash": "e85199c2969d",
    "text": "For specific values of 'n': * Unigram model (n=1): Assumes the probability of a word is independent of any prior words. P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n) This is simply the frequency of the word in the corpus. * Bigram model (n=2): The probability of a word depends only on the single immediately preceding word. P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n | w_{n-1}) Example: P(\"your\" | \"turn\") * Trigram model (n=3): The probability of a word depends only on the two immediately preceding words. P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n | w_{n-2}, w_{n-1}) Example: P(\"homework\" | \"turn\", \"your\") ________________________________________ Why do we use N-gram Models? * Simplification (Markov Assumption): Calculating the probability of a word given *all* previous words in a long sequence is computationally very complex and requires an enormous amount of data to estimate reliably (due to the curse of dimensionality). N-gram models simplify this by assuming that only a limited, fixed-size context (the last n-1 words) is relevant for predicting the next word. * Data efficiency and Estimation: The probabilities for n-grams can be estimated directly from their frequencies in a large training corpus. For example, P(w_n | w_{n-1}) in a bigram model can be estimated as: Count(w_{n-1}, w_n) / Count(w_{n-1}) where Count(w_{n-1}, w_n) is how many times the sequence \"w_{n-1} w_n\" appears, and Count(w_{n-1}) is how many times \"w_{n-1}\" appears. This count-based estimation is relatively straightforward. * Good balance between context and feasibility: * Unigrams capture no context.",
    "enhanced_text": "[NLP] For specific values of 'n': * Unigram model (n=1): Assumes the probability of a word is independent of any prior words. P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n) This is simply the frequency of the word in the corpus. * Bigram model (n=2): The probability of a word depends only on the single immediately preceding word. P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n | w_{n-1}) Example: P(\"your\" | \"turn\") * Trigram model (n=3): The probability of a word depends only on the two immediately preceding words. P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n | w_{n-2}, w_{n-1}) Example: P(\"homework\" | \"turn\", \"your\") ________________________________________ Why do we use N-gram Models? * Simplification (Markov Assumption): Calculating the probability of a word given *all* previous words in a long sequence is computationally very complex and requires an enormous amount of data to estimate reliably (due to the curse of dimensionality). N-gram models simplify this by assuming that only a limited, fixed-size context (the last n-1 words) is relevant for predicting the next word. * Data efficiency and Estimation: The probabilities for n-grams can be estimated directly from their frequencies in a large training corpus. For example, P(w_n | w_{n-1}) in a bigram model can be estimated as: Count(w_{n-1}, w_n) / Count(w_{n-1}) where Count(w_{n-1}, w_n) is how many times the sequence \"w_{n-1} w_n\" appears, and Count(w_{n-1}) is how many times \"w_{n-1}\" appears. This count-based estimation is relatively straightforward. * Good balance between context and feasibility: * Unigrams capture no context.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(e) N-gram Model.txt",
    "file_name": "lec4-(e) N-gram Model.txt",
    "position_in_document": 37,
    "filename_keywords": [
      "lec4",
      "gram",
      "model"
    ],
    "content_keywords": [
      "unigram",
      "model",
      "what"
    ],
    "all_keywords": [
      "model",
      "what",
      "unigram",
      "lec4",
      "gram"
    ],
    "keyword_string": "model what unigram lec4 gram",
    "token_count": 542,
    "word_count": 251,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.46309963099630996,
    "avg_sentence_length": 22.818181818181817
  },
  {
    "chunk_id": 128,
    "chunk_hash": "95a93c232b1c",
    "text": "The Markov assumption states:\nThe probability of the next word depends only on a limited number of previous words (usually one or two), not the entire history. In language modeling terms:\nFor a first-order Markov model (bigram model), P(wn∣w1,w2,...,wn−1)≈P(wn∣wn−1)P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-1})P(wn​∣w1​,w2​,...,wn−1​)≈P(wn​∣wn−1​) For a second-order Markov model (trigram model), P(wn∣w1,w2,...,wn−1)≈P(wn∣wn−2,wn−1)P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-2}, w_{n-1})P(wn​∣w1​,w2​,...,wn−1​)≈P(wn​∣wn−2​,wn−1​) Example of Markov Assumption in Practice Let's say you want to estimate: P(\"the\"∣\"its water so transparent that\")P(\\text{\"the\"} | \\text{\"its water so transparent that\"})P(\"the\"∣\"its water so transparent that\") Without Markov assumption, you consider the entire history: P(\"the\"∣\"its water so transparent that\")P(\\text{\"the\"} | \\text{\"its water so transparent that\"})P(\"the\"∣\"its water so transparent that\") With a first-order Markov assumption (bigram), you approximate: P(\"the\"∣\"that\")P(\\text{\"the\"} | \\text{\"that\"})P(\"the\"∣\"that\") Or with a second-order Markov assumption (trigram), you approximate: P(\"the\"∣\"transparent that\")P(\\text{\"the\"} | \\text{\"transparent that\"})P(\"the\"∣\"transparent that\") This massively reduces complexity and data requirements.",
    "enhanced_text": "[NLP] The Markov assumption states:\nThe probability of the next word depends only on a limited number of previous words (usually one or two), not the entire history. In language modeling terms:\nFor a first-order Markov model (bigram model), P(wn∣w1,w2,...,wn−1)≈P(wn∣wn−1)P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-1})P(wn​∣w1​,w2​,...,wn−1​)≈P(wn​∣wn−1​) For a second-order Markov model (trigram model), P(wn∣w1,w2,...,wn−1)≈P(wn∣wn−2,wn−1)P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-2}, w_{n-1})P(wn​∣w1​,w2​,...,wn−1​)≈P(wn​∣wn−2​,wn−1​) Example of Markov Assumption in Practice Let's say you want to estimate: P(\"the\"∣\"its water so transparent that\")P(\\text{\"the\"} | \\text{\"its water so transparent that\"})P(\"the\"∣\"its water so transparent that\") Without Markov assumption, you consider the entire history: P(\"the\"∣\"its water so transparent that\")P(\\text{\"the\"} | \\text{\"its water so transparent that\"})P(\"the\"∣\"its water so transparent that\") With a first-order Markov assumption (bigram), you approximate: P(\"the\"∣\"that\")P(\\text{\"the\"} | \\text{\"that\"})P(\"the\"∣\"that\") Or with a second-order Markov assumption (trigram), you approximate: P(\"the\"∣\"transparent that\")P(\\text{\"the\"} | \\text{\"transparent that\"})P(\"the\"∣\"transparent that\") This massively reduces complexity and data requirements.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(f) Markov Assumption.txt",
    "file_name": "lec4-(f) Markov Assumption.txt",
    "position_in_document": 15,
    "filename_keywords": [
      "lec4",
      "markov",
      "assumption"
    ],
    "content_keywords": [
      "for",
      "the",
      "markov",
      "the markov"
    ],
    "all_keywords": [
      "assumption",
      "the markov",
      "for",
      "lec4",
      "the",
      "markov"
    ],
    "keyword_string": "assumption the markov for lec4 the markov",
    "token_count": 494,
    "word_count": 149,
    "sentence_count": 2,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.3016194331983806,
    "avg_sentence_length": 74.5
  },
  {
    "chunk_id": 129,
    "chunk_hash": "fd578b0178d1",
    "text": "Spelling Correction Using N-grams and Edit Distance N-grams for Spelling Correction An n-gram is a sequence of nnn contiguous units (words or characters). For spelling correction, character n-grams are often used: Helps detect misspellings by comparing likely sequences of letters. For example, “spelling” might generate bigrams: \"sp\", \"pe\", \"el\", \"ll\", \"in\", \"ng\". Comparing n-gram overlaps can suggest the closest correct spelling to a misspelled word. Allowing Errors in Spelling Queries Real-world text input often contains typos or misspellings. Edit Distance (Levenshtein Distance): Measures how many operations (insertions, deletions, substitutions) it takes to transform one string into another. \"misspell\" → \"mispell\" (distance = 1, delete 's') \"misspell\" → \"mistell\" (distance = 2, substitute 's'→'t', delete 'p') \"misspell\" → \"misspelling\" (distance = 3, add 'ing') Computed efficiently with dynamic programming in O(mn)O(mn)O(mn) time where m,nm,nm,n are string lengths. Longest Common Subsequence (LCS): Finds the longest sequence of characters common to both strings in order, useful to judge similarity. Proximity Search: Retrieve documents/words within a certain edit distance threshold from the query to handle typos or variations. Searching for variations or partial matches in text requires pattern matching: Prefixes: match start of words \"anti\" → matches \"antibody\", \"antique\" Suffixes: match end of words \"ix\" → matches \"matrix\", \"fix\" Substrings: match any part within a word \"rapt\" → matches \"enrapture\", \"velociraptor\" Ranges: match words lexicographically between two strings \"tin\" to \"tix\" → matches \"tip\", \"tire\", \"title\" Pattern matching is more complex than exact word lookup and requires specialized data structures beyond inverted indices, like tries or suffix trees.",
    "enhanced_text": "[NLP] Spelling Correction Using N-grams and Edit Distance N-grams for Spelling Correction An n-gram is a sequence of nnn contiguous units (words or characters). For spelling correction, character n-grams are often used: Helps detect misspellings by comparing likely sequences of letters. For example, “spelling” might generate bigrams: \"sp\", \"pe\", \"el\", \"ll\", \"in\", \"ng\". Comparing n-gram overlaps can suggest the closest correct spelling to a misspelled word. Allowing Errors in Spelling Queries Real-world text input often contains typos or misspellings. Edit Distance (Levenshtein Distance): Measures how many operations (insertions, deletions, substitutions) it takes to transform one string into another. \"misspell\" → \"mispell\" (distance = 1, delete 's') \"misspell\" → \"mistell\" (distance = 2, substitute 's'→'t', delete 'p') \"misspell\" → \"misspelling\" (distance = 3, add 'ing') Computed efficiently with dynamic programming in O(mn)O(mn)O(mn) time where m,nm,nm,n are string lengths. Longest Common Subsequence (LCS): Finds the longest sequence of characters common to both strings in order, useful to judge similarity. Proximity Search: Retrieve documents/words within a certain edit distance threshold from the query to handle typos or variations. Searching for variations or partial matches in text requires pattern matching: Prefixes: match start of words \"anti\" → matches \"antibody\", \"antique\" Suffixes: match end of words \"ix\" → matches \"matrix\", \"fix\" Substrings: match any part within a word \"rapt\" → matches \"enrapture\", \"velociraptor\" Ranges: match words lexicographically between two strings \"tin\" to \"tix\" → matches \"tip\", \"tire\", \"title\" Pattern matching is more complex than exact word lookup and requires specialized data structures beyond inverted indices, like tries or suffix trees.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(g) Spelling Correction Using N-grams and Edit Distance.txt",
    "file_name": "lec4-(g) Spelling Correction Using N-grams and Edit Distance.txt",
    "position_in_document": 26,
    "filename_keywords": [
      "distance",
      "edit",
      "using",
      "spelling",
      "lec4",
      "grams",
      "correction"
    ],
    "content_keywords": [
      "spelling correction an",
      "edit distance n",
      "spelling correction using n"
    ],
    "all_keywords": [
      "spelling correction an",
      "distance",
      "edit",
      "using",
      "spelling correction using n",
      "edit distance n",
      "spelling",
      "lec4",
      "grams",
      "correction"
    ],
    "keyword_string": "spelling correction an distance edit using spelling correction using n edit distance n spelling lec4 grams correction",
    "token_count": 463,
    "word_count": 254,
    "sentence_count": 10,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5485961123110151,
    "avg_sentence_length": 25.4
  },
  {
    "chunk_id": 130,
    "chunk_hash": "389808d136a4",
    "text": "Google Search Operators (Conceptual Table Explanation) Google Search Operators are special commands or symbols that can be added to your search queries to refine and specify the search results. They allow users to search beyond simple keywords, enabling more powerful and targeted information retrieval. Here are some common operators and their uses: Use: Restricts search to words found in the link text on web pages. Useful for finding pages linked by specific terms. Usage Syntax: allinanchor:keyword1 keyword2 (for multiple keywords). Use: Restricts search to words found only in the body text of web pages, excluding titles, URLs, etc. Usage Syntax: allintext:keyword1 keyword2 (for multiple keywords). Use: Restricts search to words found only in the titles of web pages. Usage Syntax: allintitle:keyword1 keyword2 (for multiple keywords). Use: Restricts search to words found only in the web page addresses (URLs). Usage Syntax: allinurl:keyword1 keyword2 (for multiple keywords). Use: Restricts search to files of a specified type, such as PDF documents, Word files, etc. Usage Syntax: filetype:extension (e.g., filetype:pdf). Use: Similar to allinanchor, but allows for single keywords or more complex queries. Restricts search to words in the link text. Usage Syntax: inanchor:keyword. Use: Similar to allintext, but allows for single keywords or more complex queries. Restricts search to words in the body text. Usage Syntax: intext:keyword. Use: Similar to allintitle, but for single keywords or more complex queries. Restricts search to words in the titles only. Usage Syntax: intitle:keyword. Use: Similar to allinurl, but for single keywords or more complex queries. Restricts search to words in web page addresses. Usage Syntax: inurl:keyword. Use: Restricts search to a specific domain or website. Usage Syntax: site:domain (e.g., site:wikipedia.org). More at: https://www.google.com/advanced_search (This URL leads to Google's advanced search interface, which often provides guidance on using these and other operators).",
    "enhanced_text": "[NLP] Google Search Operators (Conceptual Table Explanation) Google Search Operators are special commands or symbols that can be added to your search queries to refine and specify the search results. They allow users to search beyond simple keywords, enabling more powerful and targeted information retrieval. Here are some common operators and their uses: Use: Restricts search to words found in the link text on web pages. Useful for finding pages linked by specific terms. Usage Syntax: allinanchor:keyword1 keyword2 (for multiple keywords). Use: Restricts search to words found only in the body text of web pages, excluding titles, URLs, etc. Usage Syntax: allintext:keyword1 keyword2 (for multiple keywords). Use: Restricts search to words found only in the titles of web pages. Usage Syntax: allintitle:keyword1 keyword2 (for multiple keywords). Use: Restricts search to words found only in the web page addresses (URLs). Usage Syntax: allinurl:keyword1 keyword2 (for multiple keywords). Use: Restricts search to files of a specified type, such as PDF documents, Word files, etc. Usage Syntax: filetype:extension (e.g., filetype:pdf). Use: Similar to allinanchor, but allows for single keywords or more complex queries. Restricts search to words in the link text. Usage Syntax: inanchor:keyword. Use: Similar to allintext, but allows for single keywords or more complex queries. Restricts search to words in the body text. Usage Syntax: intext:keyword. Use: Similar to allintitle, but for single keywords or more complex queries. Restricts search to words in the titles only. Usage Syntax: intitle:keyword. Use: Similar to allinurl, but for single keywords or more complex queries. Restricts search to words in web page addresses. Usage Syntax: inurl:keyword. Use: Restricts search to a specific domain or website. Usage Syntax: site:domain (e.g., site:wikipedia.org). More at: https://www.google.com/advanced_search (This URL leads to Google's advanced search interface, which often provides guidance on using these and other operators).",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(h) Google Search Operators.txt",
    "file_name": "lec4-(h) Google Search Operators.txt",
    "position_in_document": 30,
    "filename_keywords": [
      "lec4",
      "operators",
      "search",
      "google"
    ],
    "content_keywords": [
      "conceptual table explanation",
      "google search operators",
      "they"
    ],
    "all_keywords": [
      "conceptual table explanation",
      "search",
      "operators",
      "google search operators",
      "google",
      "they",
      "lec4"
    ],
    "keyword_string": "conceptual table explanation search operators google search operators google they lec4",
    "token_count": 505,
    "word_count": 295,
    "sentence_count": 28,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5841584158415841,
    "avg_sentence_length": 10.535714285714286
  },
  {
    "chunk_id": 131,
    "chunk_hash": "4702be339ff0",
    "text": "Low-Level Information Extraction (IE) Low-level IE refers to the automatic detection and extraction of structured information—like dates, names, places, events—from unstructured text (e.g., emails, web pages). It’s foundational in many modern applications that need to understand text content and provide actionable insights or direct answers. Works on raw text data without requiring manual tagging. Extracts named entities (dates, locations, organizations, events, phone numbers, etc. Enables contextual understanding and integration with other tools (calendars, maps, knowledge graphs). Enhances user experience by offering relevant, context-aware suggestions or direct answers. Example 1: Email Event Extraction (Apple/Google Mail) Email client scans your message for date/time expressions and event-related phrases. Detects entities like \"Friday January 6, 2012\". Recognizes this as an event date and links it to calendar functionality. Pop-up menu offers actions: Create New Calendar Event Show Date in Calendar This saves user time by transforming unstructured text into actionable data automatically. Example 2: Google Search Entity Extraction When you search for \"bhp billiton headquarters\", Google’s system: Extracts the entity “BHP Billiton Ltd.” and the attribute “Headquarters”. Identifies relevant structured facts (Melbourne, London). Displays a direct answer box at the top of results (sometimes called a knowledge panel or featured snippet). Highlights extracted entities in search snippets. This improves search relevancy and user satisfaction by reducing need to sift through multiple links. Underlying Techniques (brief) Named Entity Recognition (NER): Finds entities like dates, people, places. Relation Extraction: Links entities with their attributes or relations (e.g., company → headquarters). Parsing & Contextual Analysis: Understands sentence structure to correctly identify entities. Knowledge Graphs: Connect extracted entities to a large graph of known facts. Pattern Matching & Rules: For common formats like dates and times. Machine Learning / Deep Learning Models: To improve accuracy and generalization. Why Is Low-Level IE Important? It bridges the gap between unstructured text and structured data. Powers personal assistants, smart email clients, and intelligent search engines. Automates tedious manual data entry and enhances productivity. Enables context-aware applications that proactively assist users.",
    "enhanced_text": "[NLP] Low-Level Information Extraction (IE) Low-level IE refers to the automatic detection and extraction of structured information—like dates, names, places, events—from unstructured text (e.g., emails, web pages). It’s foundational in many modern applications that need to understand text content and provide actionable insights or direct answers. Works on raw text data without requiring manual tagging. Extracts named entities (dates, locations, organizations, events, phone numbers, etc. Enables contextual understanding and integration with other tools (calendars, maps, knowledge graphs). Enhances user experience by offering relevant, context-aware suggestions or direct answers. Example 1: Email Event Extraction (Apple/Google Mail) Email client scans your message for date/time expressions and event-related phrases. Detects entities like \"Friday January 6, 2012\". Recognizes this as an event date and links it to calendar functionality. Pop-up menu offers actions: Create New Calendar Event Show Date in Calendar This saves user time by transforming unstructured text into actionable data automatically. Example 2: Google Search Entity Extraction When you search for \"bhp billiton headquarters\", Google’s system: Extracts the entity “BHP Billiton Ltd.” and the attribute “Headquarters”. Identifies relevant structured facts (Melbourne, London). Displays a direct answer box at the top of results (sometimes called a knowledge panel or featured snippet). Highlights extracted entities in search snippets. This improves search relevancy and user satisfaction by reducing need to sift through multiple links. Underlying Techniques (brief) Named Entity Recognition (NER): Finds entities like dates, people, places. Relation Extraction: Links entities with their attributes or relations (e.g., company → headquarters). Parsing & Contextual Analysis: Understands sentence structure to correctly identify entities. Knowledge Graphs: Connect extracted entities to a large graph of known facts. Pattern Matching & Rules: For common formats like dates and times. Machine Learning / Deep Learning Models: To improve accuracy and generalization. Why Is Low-Level IE Important? It bridges the gap between unstructured text and structured data. Powers personal assistants, smart email clients, and intelligent search engines. Automates tedious manual data entry and enhances productivity. Enables context-aware applications that proactively assist users.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(a) Low-level Information Extraction.txt",
    "file_name": "lec5-(a) Low-level Information Extraction.txt",
    "position_in_document": 34,
    "filename_keywords": [
      "information",
      "extraction",
      "lec5",
      "low",
      "level"
    ],
    "content_keywords": [
      "level information extraction",
      "low"
    ],
    "all_keywords": [
      "information",
      "extraction",
      "level information extraction",
      "lec5",
      "low",
      "level"
    ],
    "keyword_string": "information extraction level information extraction lec5 low level",
    "token_count": 481,
    "word_count": 329,
    "sentence_count": 26,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.683991683991684,
    "avg_sentence_length": 12.653846153846153
  },
  {
    "chunk_id": 132,
    "chunk_hash": "c23bc5f420dc",
    "text": "Similarity Measure — Overview A similarity measure is a mathematical function or algorithm that quantifies how alike two objects are. In the context of information retrieval (IR) and natural language processing (NLP), these objects are often represented as vectors (lists of numbers). For example, a user's search query and a document from a collection can both be transformed into vector representations, and a similarity measure can then be used to determine how closely related they are. ________________________________________ Why use similarity measures? Similarity measures are crucial in various applications for several reasons: * To rank documents: They allow systems to order documents based on how relevant they are to a given query. Documents with higher similarity scores to the query are ranked higher. * To filter results: A similarity threshold can be set, so only documents exceeding that level of similarity to a query or another document are considered or returned. * To improve search accuracy: By providing a quantitative way to assess relevance, similarity measures help in delivering more accurate and pertinent search results. * For clustering: Grouping similar items together. * For recommendations: Suggesting items similar to what a user has liked or viewed. * For anomaly detection: Identifying items that are dissimilar to the norm. ________________________________________ Common Similarity Measures * Measures the overlap between two sets of items. It is calculated as the size of the intersection of the sets divided by the size of their union. Jaccard(Set_A, Set_B) = |Set_A intersect Set_B| / |Set_A union Set_B| `|Set_A intersect Set_B|` is the number of elements common to both Set_A and Set_B. `|Set_A union Set_B|` is the total number of unique elements in either Set_A or Set_B. * Useful for comparing sets of words (e.g., unique keywords in documents, items in user profiles). * Range: 0 (indicating no overlap between the sets) to 1 (indicating the sets are identical). Doc1 words = {apple, orange, banana}",
    "enhanced_text": "[NLP] Similarity Measure — Overview A similarity measure is a mathematical function or algorithm that quantifies how alike two objects are. In the context of information retrieval (IR) and natural language processing (NLP), these objects are often represented as vectors (lists of numbers). For example, a user's search query and a document from a collection can both be transformed into vector representations, and a similarity measure can then be used to determine how closely related they are. ________________________________________ Why use similarity measures? Similarity measures are crucial in various applications for several reasons: * To rank documents: They allow systems to order documents based on how relevant they are to a given query. Documents with higher similarity scores to the query are ranked higher. * To filter results: A similarity threshold can be set, so only documents exceeding that level of similarity to a query or another document are considered or returned. * To improve search accuracy: By providing a quantitative way to assess relevance, similarity measures help in delivering more accurate and pertinent search results. * For clustering: Grouping similar items together. * For recommendations: Suggesting items similar to what a user has liked or viewed. * For anomaly detection: Identifying items that are dissimilar to the norm. ________________________________________ Common Similarity Measures * Measures the overlap between two sets of items. It is calculated as the size of the intersection of the sets divided by the size of their union. Jaccard(Set_A, Set_B) = |Set_A intersect Set_B| / |Set_A union Set_B| `|Set_A intersect Set_B|` is the number of elements common to both Set_A and Set_B. `|Set_A union Set_B|` is the total number of unique elements in either Set_A or Set_B. * Useful for comparing sets of words (e.g., unique keywords in documents, items in user profiles). * Range: 0 (indicating no overlap between the sets) to 1 (indicating the sets are identical). Doc1 words = {apple, orange, banana}",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(a) Similarity Measure.txt",
    "file_name": "lec6-(a) Similarity Measure.txt",
    "position_in_document": 24,
    "filename_keywords": [
      "measure",
      "lec6",
      "similarity"
    ],
    "content_keywords": [
      "overview a",
      "similarity measure",
      "nlp"
    ],
    "all_keywords": [
      "overview a",
      "similarity",
      "measure",
      "nlp",
      "lec6",
      "similarity measure"
    ],
    "keyword_string": "overview a similarity measure nlp lec6 similarity measure",
    "token_count": 504,
    "word_count": 316,
    "sentence_count": 18,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.626984126984127,
    "avg_sentence_length": 17.555555555555557
  },
  {
    "chunk_id": 133,
    "chunk_hash": "e74dbaa90b66",
    "text": "`|Set_A union Set_B|` is the total number of unique elements in either Set_A or Set_B. * Useful for comparing sets of words (e.g., unique keywords in documents, items in user profiles). * Range: 0 (indicating no overlap between the sets) to 1 (indicating the sets are identical). Doc1 words = {apple, orange, banana} Doc2 words = {apple, banana, kiwi} Intersection (Doc1, Doc2) = {apple, banana} -> Size = 2 Union (Doc1, Doc2) = {apple, orange, banana, kiwi} -> Size = 4 Jaccard Similarity = 2 / 4 = 0.5 ________________________________________ * Measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. It focuses on the orientation of the vectors, not their magnitude. Cosine_Similarity(Vector_A, Vector_B) = (Vector_A . Vector_B) / (||Vector_A|| * ||Vector_B||) Vector_B` is the dot product of Vector_A and Vector_B. `||Vector_A||` is the magnitude (or Euclidean norm/length) of Vector_A. `||Vector_B||` is the magnitude of Vector_B. * Values range from -1 (exactly opposite) to 1 (exactly the same direction). For vectors with non-negative components (like TF-IDF scores or word counts), the range is typically 0 (orthogonal, no similarity) to 1 (same direction, maximal similarity). * Popular in text similarity tasks, especially with TF-IDF vectors or word embedding vectors. B) = (1*2) + (2*3) + (3*4) = 2 + 6 + 12 = 20 Magnitude of A (||A||) = sqrt(1^2 + 2^2 + 3^2) = sqrt(1 + 4 + 9) = sqrt(14) Magnitude of B (||B||) = sqrt(2^2 + 3^2 + 4^2) = sqrt(4 + 9 + 16) = sqrt(29) Cosine Similarity = 20 / (sqrt(14) * sqrt(29)) = 20 / (approx 3.742 * approx 5.385) = 20 / (approx 20.15) which is approximately 0.992",
    "enhanced_text": "[NLP] `|Set_A union Set_B|` is the total number of unique elements in either Set_A or Set_B. * Useful for comparing sets of words (e.g., unique keywords in documents, items in user profiles). * Range: 0 (indicating no overlap between the sets) to 1 (indicating the sets are identical). Doc1 words = {apple, orange, banana} Doc2 words = {apple, banana, kiwi} Intersection (Doc1, Doc2) = {apple, banana} -> Size = 2 Union (Doc1, Doc2) = {apple, orange, banana, kiwi} -> Size = 4 Jaccard Similarity = 2 / 4 = 0.5 ________________________________________ * Measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. It focuses on the orientation of the vectors, not their magnitude. Cosine_Similarity(Vector_A, Vector_B) = (Vector_A . Vector_B) / (||Vector_A|| * ||Vector_B||) Vector_B` is the dot product of Vector_A and Vector_B. `||Vector_A||` is the magnitude (or Euclidean norm/length) of Vector_A. `||Vector_B||` is the magnitude of Vector_B. * Values range from -1 (exactly opposite) to 1 (exactly the same direction). For vectors with non-negative components (like TF-IDF scores or word counts), the range is typically 0 (orthogonal, no similarity) to 1 (same direction, maximal similarity). * Popular in text similarity tasks, especially with TF-IDF vectors or word embedding vectors. B) = (1*2) + (2*3) + (3*4) = 2 + 6 + 12 = 20 Magnitude of A (||A||) = sqrt(1^2 + 2^2 + 3^2) = sqrt(1 + 4 + 9) = sqrt(14) Magnitude of B (||B||) = sqrt(2^2 + 3^2 + 4^2) = sqrt(4 + 9 + 16) = sqrt(29) Cosine Similarity = 20 / (sqrt(14) * sqrt(29)) = 20 / (approx 3.742 * approx 5.385) = 20 / (approx 20.15) which is approximately 0.992",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(a) Similarity Measure.txt",
    "file_name": "lec6-(a) Similarity Measure.txt",
    "position_in_document": 46,
    "filename_keywords": [
      "measure",
      "lec6",
      "similarity"
    ],
    "content_keywords": [
      "overview a",
      "similarity measure",
      "nlp"
    ],
    "all_keywords": [
      "overview a",
      "similarity",
      "measure",
      "nlp",
      "lec6",
      "similarity measure"
    ],
    "keyword_string": "overview a similarity measure nlp lec6 similarity measure",
    "token_count": 571,
    "word_count": 278,
    "sentence_count": 13,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.4868651488616462,
    "avg_sentence_length": 21.384615384615383
  },
  {
    "chunk_id": 134,
    "chunk_hash": "52aeec08c7c2",
    "text": "________________________________________ * Measures the straight-line distance (or \"as the crow flies\" distance) between two points (or vectors) in Euclidean space. Euclidean_Distance(Vector_A, Vector_B) = sqrt( sum_for_each_dimension_i ( (A_i - B_i)^2 ) ) Where A_i and B_i are the components of vectors A and B in the i-th dimension. * A lower distance indicates higher similarity (i.e., the points are closer). A distance of 0 means the points are identical. * Often used in clustering algorithms (like K-means) and for comparing vectors with continuous-valued features. To use it as a similarity measure (where higher is better), it often needs to be transformed (e.g., 1 / (1 + distance) or exp(-distance)). ________________________________________ Edit Distance (Specifically Levenshtein Distance) * Measures the similarity between two strings by counting the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other. * Useful for comparing short texts, names, identifying spelling errors, or in bioinformatics for DNA sequences. * A lower edit distance means the strings are more similar. An edit distance of 0 means the strings are identical. To transform \"kitten\" into \"sitting\": 1. k -> s (substitution: \"sitten\") 2. e -> i (substitution: \"sittin\") -> g (insertion: \"sitting\") The Levenshtein distance is 3. ________________________________________ Word Embedding Similarity * Compares word or document vectors that are generated from word embedding models like Word2Vec, GloVe, FastText, or sentence/document embedding models like Sentence-BERT.",
    "enhanced_text": "[NLP] ________________________________________ * Measures the straight-line distance (or \"as the crow flies\" distance) between two points (or vectors) in Euclidean space. Euclidean_Distance(Vector_A, Vector_B) = sqrt( sum_for_each_dimension_i ( (A_i - B_i)^2 ) ) Where A_i and B_i are the components of vectors A and B in the i-th dimension. * A lower distance indicates higher similarity (i.e., the points are closer). A distance of 0 means the points are identical. * Often used in clustering algorithms (like K-means) and for comparing vectors with continuous-valued features. To use it as a similarity measure (where higher is better), it often needs to be transformed (e.g., 1 / (1 + distance) or exp(-distance)). ________________________________________ Edit Distance (Specifically Levenshtein Distance) * Measures the similarity between two strings by counting the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other. * Useful for comparing short texts, names, identifying spelling errors, or in bioinformatics for DNA sequences. * A lower edit distance means the strings are more similar. An edit distance of 0 means the strings are identical. To transform \"kitten\" into \"sitting\": 1. k -> s (substitution: \"sitten\") 2. e -> i (substitution: \"sittin\") -> g (insertion: \"sitting\") The Levenshtein distance is 3. ________________________________________ Word Embedding Similarity * Compares word or document vectors that are generated from word embedding models like Word2Vec, GloVe, FastText, or sentence/document embedding models like Sentence-BERT.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(a) Similarity Measure.txt",
    "file_name": "lec6-(a) Similarity Measure.txt",
    "position_in_document": 68,
    "filename_keywords": [
      "measure",
      "lec6",
      "similarity"
    ],
    "content_keywords": [
      "overview a",
      "similarity measure",
      "nlp"
    ],
    "all_keywords": [
      "overview a",
      "similarity",
      "measure",
      "nlp",
      "lec6",
      "similarity measure"
    ],
    "keyword_string": "overview a similarity measure nlp lec6 similarity measure",
    "token_count": 500,
    "word_count": 231,
    "sentence_count": 12,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.462,
    "avg_sentence_length": 19.25
  },
  {
    "chunk_id": 135,
    "chunk_hash": "6216b08fe492",
    "text": "________________________________________ Word Embedding Similarity * Compares word or document vectors that are generated from word embedding models like Word2Vec, GloVe, FastText, or sentence/document embedding models like Sentence-BERT. * These models learn dense vector representations where words or texts with similar meanings are located close to each other in the vector space. * The similarity between these embedding vectors is usually measured using Cosine Similarity. * This approach captures semantic similarity (meaning-based similarity) that goes beyond simple surface-level word overlap. For example, \"king\" and \"queen\" would have high similarity. ________________________________________ Semantic Similarity Measures * A broader category of measures that aim to capture the similarity in meaning or semantic content between pieces of text, rather than just lexical overlap. * Example: Two sentences that use different words but convey a similar idea can achieve a high semantic similarity score. * Knowledge-based measures: Utilizing structured knowledge bases like WordNet (which groups words into sets of synonyms called synsets and defines relationships between them). * Corpus-based measures: Statistical methods derived from large text corpora, including distributional similarity (words appearing in similar contexts are similar) and techniques like Latent Semantic Analysis (LSA). * Embedding-based measures: As described in \"Word Embedding Similarity,\" using vectors from models like BERT, ELMo, Universal Sentence Encoder. * Transformer models: Modern deep learning models like BERT can directly provide similarity scores or embeddings for comparison that capture rich contextual meaning. ________________________________________ Topic Modeling-based Similarity * Uses topic distributions derived from topic models like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF). * In these models, documents are represented as a mixture of underlying topics, and each topic is a distribution over words. * So, each document can be represented as a vector where each component is the probability or proportion of a particular topic in that document. * Similarity between documents is then computed by comparing their topic distributions (e.g., using Cosine Similarity, Jensen-Shannon divergence, or Hellinger distance on the topic vectors). * Useful for comparing documents at a conceptual or thematic level rather than relying on exact word matches.",
    "enhanced_text": "[NLP] ________________________________________ Word Embedding Similarity * Compares word or document vectors that are generated from word embedding models like Word2Vec, GloVe, FastText, or sentence/document embedding models like Sentence-BERT. * These models learn dense vector representations where words or texts with similar meanings are located close to each other in the vector space. * The similarity between these embedding vectors is usually measured using Cosine Similarity. * This approach captures semantic similarity (meaning-based similarity) that goes beyond simple surface-level word overlap. For example, \"king\" and \"queen\" would have high similarity. ________________________________________ Semantic Similarity Measures * A broader category of measures that aim to capture the similarity in meaning or semantic content between pieces of text, rather than just lexical overlap. * Example: Two sentences that use different words but convey a similar idea can achieve a high semantic similarity score. * Knowledge-based measures: Utilizing structured knowledge bases like WordNet (which groups words into sets of synonyms called synsets and defines relationships between them). * Corpus-based measures: Statistical methods derived from large text corpora, including distributional similarity (words appearing in similar contexts are similar) and techniques like Latent Semantic Analysis (LSA). * Embedding-based measures: As described in \"Word Embedding Similarity,\" using vectors from models like BERT, ELMo, Universal Sentence Encoder. * Transformer models: Modern deep learning models like BERT can directly provide similarity scores or embeddings for comparison that capture rich contextual meaning. ________________________________________ Topic Modeling-based Similarity * Uses topic distributions derived from topic models like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF). * In these models, documents are represented as a mixture of underlying topics, and each topic is a distribution over words. * So, each document can be represented as a vector where each component is the probability or proportion of a particular topic in that document. * Similarity between documents is then computed by comparing their topic distributions (e.g., using Cosine Similarity, Jensen-Shannon divergence, or Hellinger distance on the topic vectors). * Useful for comparing documents at a conceptual or thematic level rather than relying on exact word matches.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(a) Similarity Measure.txt",
    "file_name": "lec6-(a) Similarity Measure.txt",
    "position_in_document": 87,
    "filename_keywords": [
      "measure",
      "lec6",
      "similarity"
    ],
    "content_keywords": [
      "overview a",
      "similarity measure",
      "nlp"
    ],
    "all_keywords": [
      "overview a",
      "similarity",
      "measure",
      "nlp",
      "lec6",
      "similarity measure"
    ],
    "keyword_string": "overview a similarity measure nlp lec6 similarity measure",
    "token_count": 582,
    "word_count": 341,
    "sentence_count": 16,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5859106529209622,
    "avg_sentence_length": 21.3125
  },
  {
    "chunk_id": 136,
    "chunk_hash": "56d55ff29e59",
    "text": "Jaccard Similarity Coefficient The Jaccard Coefficient (also known as Jaccard Index or sometimes referred to as the Tanimoto Coefficient in some contexts, though Tanimoto often applies to binary attributes in vectors) is a statistic used to measure the similarity and diversity between two finite sets. It quantifies the degree of overlap between two sets, let's call them set A and set B. ________________________________________ Jaccard(A, B) = |A intersect B| / |A union B| * `|A intersect B|` represents the number of elements common to both set A and set B (i.e., the cardinality of the intersection of A and B). * `|A union B|` represents the total number of unique elements present in either set A or set B (or both) (i.e., the cardinality of the union of A and B). It can also be expressed as: |A union B| = |A| + |B| - |A intersect B| ________________________________________ This means a set is perfectly similar to itself. The intersection and union are the set itself, so |A| / |A| = 1. * Jaccard(A, B) = 0 if (A intersect B) is an empty set (∅). This means if there are no common elements between set A and set B, their Jaccard similarity is zero. * The Jaccard score is always between 0 and 1, inclusive (0 <= Jaccard(A, B) <= 1). A score of 1 indicates perfect similarity, and a score of 0 indicates no similarity. ________________________________________ Conceptual Diagram Explanation Imagine a Venn diagram with two overlapping circles representing set A and set B. * Intersection (A intersect B): This is the overlapping area in the Venn diagram.",
    "enhanced_text": "[NLP] Jaccard Similarity Coefficient The Jaccard Coefficient (also known as Jaccard Index or sometimes referred to as the Tanimoto Coefficient in some contexts, though Tanimoto often applies to binary attributes in vectors) is a statistic used to measure the similarity and diversity between two finite sets. It quantifies the degree of overlap between two sets, let's call them set A and set B. ________________________________________ Jaccard(A, B) = |A intersect B| / |A union B| * `|A intersect B|` represents the number of elements common to both set A and set B (i.e., the cardinality of the intersection of A and B). * `|A union B|` represents the total number of unique elements present in either set A or set B (or both) (i.e., the cardinality of the union of A and B). It can also be expressed as: |A union B| = |A| + |B| - |A intersect B| ________________________________________ This means a set is perfectly similar to itself. The intersection and union are the set itself, so |A| / |A| = 1. * Jaccard(A, B) = 0 if (A intersect B) is an empty set (∅). This means if there are no common elements between set A and set B, their Jaccard similarity is zero. * The Jaccard score is always between 0 and 1, inclusive (0 <= Jaccard(A, B) <= 1). A score of 1 indicates perfect similarity, and a score of 0 indicates no similarity. ________________________________________ Conceptual Diagram Explanation Imagine a Venn diagram with two overlapping circles representing set A and set B. * Intersection (A intersect B): This is the overlapping area in the Venn diagram.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(b) Jaccard Similarity Coefficient.txt",
    "file_name": "lec6-(b) Jaccard Similarity Coefficient.txt",
    "position_in_document": 19,
    "filename_keywords": [
      "lec6",
      "jaccard",
      "coefficient",
      "similarity"
    ],
    "content_keywords": [
      "jaccard index",
      "tanimoto coefficient",
      "tanimoto",
      "jaccard similarity coefficient the jaccard coefficient"
    ],
    "all_keywords": [
      "jaccard index",
      "tanimoto",
      "jaccard similarity coefficient the jaccard coefficient",
      "coefficient",
      "similarity",
      "tanimoto coefficient",
      "jaccard",
      "lec6"
    ],
    "keyword_string": "jaccard index tanimoto jaccard similarity coefficient the jaccard coefficient coefficient similarity tanimoto coefficient jaccard lec6",
    "token_count": 497,
    "word_count": 269,
    "sentence_count": 12,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5412474849094567,
    "avg_sentence_length": 22.416666666666668
  },
  {
    "chunk_id": 137,
    "chunk_hash": "8f7e1336cd7b",
    "text": "A score of 1 indicates perfect similarity, and a score of 0 indicates no similarity. ________________________________________ Conceptual Diagram Explanation Imagine a Venn diagram with two overlapping circles representing set A and set B. * Intersection (A intersect B): This is the overlapping area in the Venn diagram. It contains all the elements that are present in BOTH set A AND set B. The Jaccard formula uses the count of these common elements in its numerator. * Union (A union B): This is the total area covered by both circles in the Venn diagram, including the overlapping part. It contains all unique elements that are present in set A OR set B OR both. The Jaccard formula uses the count of these total unique elements in its denominator. These visual components directly correspond to the numerator (intersection size) and the denominator (union size) of the Jaccard formula. ________________________________________ S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\" S2: \"The Bachelors of Computer Science Program is offered at Bahria University, Islamabad.\" Tokenization (treating words as elements of sets, case-sensitive for this example, though typically lowercasing is done): * Set A (words from S1): A = {'I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad'} Number of elements in A, |A| = 14 * Set B (words from S2): B = {'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'is', 'offered', 'at', 'Bahria', 'University', 'Islamabad'} Number of elements in B, |B| = 12 ________________________________________ This set contains all unique words from both A and B.",
    "enhanced_text": "[NLP] A score of 1 indicates perfect similarity, and a score of 0 indicates no similarity. ________________________________________ Conceptual Diagram Explanation Imagine a Venn diagram with two overlapping circles representing set A and set B. * Intersection (A intersect B): This is the overlapping area in the Venn diagram. It contains all the elements that are present in BOTH set A AND set B. The Jaccard formula uses the count of these common elements in its numerator. * Union (A union B): This is the total area covered by both circles in the Venn diagram, including the overlapping part. It contains all unique elements that are present in set A OR set B OR both. The Jaccard formula uses the count of these total unique elements in its denominator. These visual components directly correspond to the numerator (intersection size) and the denominator (union size) of the Jaccard formula. ________________________________________ S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\" S2: \"The Bachelors of Computer Science Program is offered at Bahria University, Islamabad.\" Tokenization (treating words as elements of sets, case-sensitive for this example, though typically lowercasing is done): * Set A (words from S1): A = {'I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad'} Number of elements in A, |A| = 14 * Set B (words from S2): B = {'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'is', 'offered', 'at', 'Bahria', 'University', 'Islamabad'} Number of elements in B, |B| = 12 ________________________________________ This set contains all unique words from both A and B.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(b) Jaccard Similarity Coefficient.txt",
    "file_name": "lec6-(b) Jaccard Similarity Coefficient.txt",
    "position_in_document": 37,
    "filename_keywords": [
      "lec6",
      "jaccard",
      "coefficient",
      "similarity"
    ],
    "content_keywords": [
      "jaccard index",
      "tanimoto coefficient",
      "tanimoto",
      "jaccard similarity coefficient the jaccard coefficient"
    ],
    "all_keywords": [
      "jaccard index",
      "tanimoto",
      "jaccard similarity coefficient the jaccard coefficient",
      "coefficient",
      "similarity",
      "tanimoto coefficient",
      "jaccard",
      "lec6"
    ],
    "keyword_string": "jaccard index tanimoto jaccard similarity coefficient the jaccard coefficient coefficient similarity tanimoto coefficient jaccard lec6",
    "token_count": 545,
    "word_count": 261,
    "sentence_count": 12,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.47889908256880737,
    "avg_sentence_length": 21.75
  },
  {
    "chunk_id": 138,
    "chunk_hash": "3f8e33567ec3",
    "text": "A union B = {'I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered'} Number of elements in (A union B), |A union B| = 16 * Intersection (A intersect B): This set contains only the words common to both A and B. A intersect B = {'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad'} Number of elements in (A intersect B), |A intersect B| = 10 ________________________________________ Jaccard(A, B) = |A intersect B| / |A union B| Jaccard(A, B) = 10 / 16 Jaccard(A, B) = 0.625 ________________________________________ Alternate Formula (Often called Tanimoto Coefficient for binary attributes, equivalent to Jaccard for sets) T = Nc / (Na + Nb - Nc) * Nc = |A intersect B| (number of common elements) * Na = |A| (number of elements in set A) * Nb = |B| (number of elements in set B) T = 10 / (14 + 12 - 10) This confirms that Na + Nb - Nc is an alternative way to calculate |A union B|. ________________________________________ * Simple and intuitive: The concept of overlap relative to total unique items is easy to understand. * Efficient to calculate: Requires basic set operations (intersection, union) and counts. * Not sensitive to differences in set sizes in the same way some other metrics might be; it focuses on the proportion of shared items.",
    "enhanced_text": "[NLP] A union B = {'I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered'} Number of elements in (A union B), |A union B| = 16 * Intersection (A intersect B): This set contains only the words common to both A and B. A intersect B = {'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad'} Number of elements in (A intersect B), |A intersect B| = 10 ________________________________________ Jaccard(A, B) = |A intersect B| / |A union B| Jaccard(A, B) = 10 / 16 Jaccard(A, B) = 0.625 ________________________________________ Alternate Formula (Often called Tanimoto Coefficient for binary attributes, equivalent to Jaccard for sets) T = Nc / (Na + Nb - Nc) * Nc = |A intersect B| (number of common elements) * Na = |A| (number of elements in set A) * Nb = |B| (number of elements in set B) T = 10 / (14 + 12 - 10) This confirms that Na + Nb - Nc is an alternative way to calculate |A union B|. ________________________________________ * Simple and intuitive: The concept of overlap relative to total unique items is easy to understand. * Efficient to calculate: Requires basic set operations (intersection, union) and counts. * Not sensitive to differences in set sizes in the same way some other metrics might be; it focuses on the proportion of shared items.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(b) Jaccard Similarity Coefficient.txt",
    "file_name": "lec6-(b) Jaccard Similarity Coefficient.txt",
    "position_in_document": 59,
    "filename_keywords": [
      "lec6",
      "jaccard",
      "coefficient",
      "similarity"
    ],
    "content_keywords": [
      "jaccard index",
      "tanimoto coefficient",
      "tanimoto",
      "jaccard similarity coefficient the jaccard coefficient"
    ],
    "all_keywords": [
      "jaccard index",
      "tanimoto",
      "jaccard similarity coefficient the jaccard coefficient",
      "coefficient",
      "similarity",
      "tanimoto coefficient",
      "jaccard",
      "lec6"
    ],
    "keyword_string": "jaccard index tanimoto jaccard similarity coefficient the jaccard coefficient coefficient similarity tanimoto coefficient jaccard lec6",
    "token_count": 511,
    "word_count": 231,
    "sentence_count": 5,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.4520547945205479,
    "avg_sentence_length": 46.2
  },
  {
    "chunk_id": 139,
    "chunk_hash": "1c923ca18cc0",
    "text": "Cosine Similarity measures the similarity between two non-zero vectors by calculating the cosine of the angle between them in an inner product space. It focuses on the orientation of the vectors, ignoring their magnitudes (lengths). * Range: Cosine similarity values range from -1 to 1. However, when vectors have only non-negative components (like word counts or TF-IDF scores), the similarity typically ranges from 0 to 1. * A value of 1 means the vectors point in the exact same direction (perfectly similar orientation). * A value of 0 means the vectors are orthogonal (perpendicular), indicating no similarity in their orientation. * A value of -1 (possible with vectors containing negative values) means the vectors point in opposite directions. ________________________________________ Cosine Similarity = (A . B` represents the dot product of vectors A and B. This is calculated by multiplying corresponding elements of the two vectors and then summing all those products. For vectors A = [a1, a2, ..., an] and B = [b1, b2, ..., bn], the dot product is: B = (a1*b1) + (a2*b2) + ... + (an*bn) * `||A||` represents the magnitude (also known as the Euclidean norm or length) of vector A. This is calculated as the square root of the sum of the squares of its components. For vector A = [a1, a2, ..., an], the magnitude is: ||A|| = sqrt(a1^2 + a2^2 + ... + an^2) * `||B||` represents the magnitude of vector B. Calculated similarly to ||A||: ||B|| = sqrt(b1^2 + b2^2 + ... + bn^2) ________________________________________ S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\"",
    "enhanced_text": "[NLP] Cosine Similarity measures the similarity between two non-zero vectors by calculating the cosine of the angle between them in an inner product space. It focuses on the orientation of the vectors, ignoring their magnitudes (lengths). * Range: Cosine similarity values range from -1 to 1. However, when vectors have only non-negative components (like word counts or TF-IDF scores), the similarity typically ranges from 0 to 1. * A value of 1 means the vectors point in the exact same direction (perfectly similar orientation). * A value of 0 means the vectors are orthogonal (perpendicular), indicating no similarity in their orientation. * A value of -1 (possible with vectors containing negative values) means the vectors point in opposite directions. ________________________________________ Cosine Similarity = (A . B` represents the dot product of vectors A and B. This is calculated by multiplying corresponding elements of the two vectors and then summing all those products. For vectors A = [a1, a2, ..., an] and B = [b1, b2, ..., bn], the dot product is: B = (a1*b1) + (a2*b2) + ... + (an*bn) * `||A||` represents the magnitude (also known as the Euclidean norm or length) of vector A. This is calculated as the square root of the sum of the squares of its components. For vector A = [a1, a2, ..., an], the magnitude is: ||A|| = sqrt(a1^2 + a2^2 + ... + an^2) * `||B||` represents the magnitude of vector B. Calculated similarly to ||A||: ||B|| = sqrt(b1^2 + b2^2 + ... + bn^2) ________________________________________ S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\"",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(c) Cosine Similarity.txt",
    "file_name": "lec6-(c) Cosine Similarity.txt",
    "position_in_document": 22,
    "filename_keywords": [
      "lec6",
      "cosine",
      "similarity"
    ],
    "content_keywords": [
      "cosine",
      "range",
      "cosine similarity"
    ],
    "all_keywords": [
      "cosine",
      "range",
      "cosine similarity",
      "similarity",
      "lec6"
    ],
    "keyword_string": "cosine range cosine similarity similarity lec6",
    "token_count": 498,
    "word_count": 268,
    "sentence_count": 14,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5381526104417671,
    "avg_sentence_length": 19.142857142857142
  },
  {
    "chunk_id": 140,
    "chunk_hash": "864ca04b621b",
    "text": "Calculated similarly to ||A||: ||B|| = sqrt(b1^2 + b2^2 + ... + bn^2) ________________________________________ S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\" S2: \"The Bachelors of Computer Science Program is offered at Bahria University, Islamabad.\" ________________________________________ Step 1: Tokenize and Create Vocabulary Combine all unique words from both sentences into a vocabulary. Let's assume a fixed order for the vocabulary: Vocabulary = ['I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered'] (The vocabulary has 16 unique words.) ________________________________________ Step 2: Construct Binary Vectors Represent each sentence as a vector where each element corresponds to a word in the vocabulary. A '1' indicates the word is present in the sentence, and a '0' indicates it is absent. * Vector for Sentence 1 (V1): Based on the vocabulary order: ['I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered'] V1 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0] (The words 'is' and 'offered' are absent from S1, so their corresponding positions are 0.) * Vector for Sentence 2 (V2): V2 = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (The words 'I', 'am', 'enrolled', 'in' are absent from S2, so their corresponding positions are 0.)",
    "enhanced_text": "[NLP] Calculated similarly to ||A||: ||B|| = sqrt(b1^2 + b2^2 + ... + bn^2) ________________________________________ S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\" S2: \"The Bachelors of Computer Science Program is offered at Bahria University, Islamabad.\" ________________________________________ Step 1: Tokenize and Create Vocabulary Combine all unique words from both sentences into a vocabulary. Let's assume a fixed order for the vocabulary: Vocabulary = ['I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered'] (The vocabulary has 16 unique words.) ________________________________________ Step 2: Construct Binary Vectors Represent each sentence as a vector where each element corresponds to a word in the vocabulary. A '1' indicates the word is present in the sentence, and a '0' indicates it is absent. * Vector for Sentence 1 (V1): Based on the vocabulary order: ['I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered'] V1 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0] (The words 'is' and 'offered' are absent from S1, so their corresponding positions are 0.) * Vector for Sentence 2 (V2): V2 = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (The words 'I', 'am', 'enrolled', 'in' are absent from S2, so their corresponding positions are 0.)",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(c) Cosine Similarity.txt",
    "file_name": "lec6-(c) Cosine Similarity.txt",
    "position_in_document": 40,
    "filename_keywords": [
      "lec6",
      "cosine",
      "similarity"
    ],
    "content_keywords": [
      "cosine",
      "range",
      "cosine similarity"
    ],
    "all_keywords": [
      "cosine",
      "range",
      "cosine similarity",
      "similarity",
      "lec6"
    ],
    "keyword_string": "cosine range cosine similarity similarity lec6",
    "token_count": 578,
    "word_count": 230,
    "sentence_count": 8,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.39792387543252594,
    "avg_sentence_length": 28.75
  },
  {
    "chunk_id": 141,
    "chunk_hash": "9acaeb12d0c3",
    "text": "* Vector for Sentence 2 (V2): V2 = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (The words 'I', 'am', 'enrolled', 'in' are absent from S2, so their corresponding positions are 0.) ________________________________________ Step 3: Compute Magnitudes Magnitude of V1 (||V1||): ||V1|| = sqrt(sum of the squares of each element in V1) V1 has 14 ones and 2 zeros. ||V1|| = sqrt( (1^2 * 14) + (0^2 * 2) ) ||V1|| = sqrt( (1 * 14) + (0 * 2) ) ||V1|| = sqrt(14 + 0) Magnitude of V2 (||V2||): ||V2|| = sqrt(sum of the squares of each element in V2) V2 has 12 ones and 4 zeros. ||V2|| = sqrt( (1^2 * 12) + (0^2 * 4) ) ||V2|| = sqrt( (1 * 12) + (0 * 4) ) ||V2|| = sqrt(12 + 0) ________________________________________ Step 4: Compute Dot Product (V1 . V2 is the sum of the products of corresponding elements. V2 = (1*0) + (1*0) + (1*0) + (1*0) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (0*1) + (0*1) V2 = 0 + 0 + 0 + 0 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0 (Alternatively, for binary vectors, the dot product is simply the count of common words, i.e., positions where both vectors have a 1. There are 10 such common words.)",
    "enhanced_text": "[NLP] * Vector for Sentence 2 (V2): V2 = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (The words 'I', 'am', 'enrolled', 'in' are absent from S2, so their corresponding positions are 0.) ________________________________________ Step 3: Compute Magnitudes Magnitude of V1 (||V1||): ||V1|| = sqrt(sum of the squares of each element in V1) V1 has 14 ones and 2 zeros. ||V1|| = sqrt( (1^2 * 14) + (0^2 * 2) ) ||V1|| = sqrt( (1 * 14) + (0 * 2) ) ||V1|| = sqrt(14 + 0) Magnitude of V2 (||V2||): ||V2|| = sqrt(sum of the squares of each element in V2) V2 has 12 ones and 4 zeros. ||V2|| = sqrt( (1^2 * 12) + (0^2 * 4) ) ||V2|| = sqrt( (1 * 12) + (0 * 4) ) ||V2|| = sqrt(12 + 0) ________________________________________ Step 4: Compute Dot Product (V1 . V2 is the sum of the products of corresponding elements. V2 = (1*0) + (1*0) + (1*0) + (1*0) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (0*1) + (0*1) V2 = 0 + 0 + 0 + 0 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0 (Alternatively, for binary vectors, the dot product is simply the count of common words, i.e., positions where both vectors have a 1. There are 10 such common words.)",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(c) Cosine Similarity.txt",
    "file_name": "lec6-(c) Cosine Similarity.txt",
    "position_in_document": 61,
    "filename_keywords": [
      "lec6",
      "cosine",
      "similarity"
    ],
    "content_keywords": [
      "cosine",
      "range",
      "cosine similarity"
    ],
    "all_keywords": [
      "cosine",
      "range",
      "cosine similarity",
      "similarity",
      "lec6"
    ],
    "keyword_string": "cosine range cosine similarity similarity lec6",
    "token_count": 569,
    "word_count": 255,
    "sentence_count": 7,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.44815465729349735,
    "avg_sentence_length": 36.42857142857143
  },
  {
    "chunk_id": 142,
    "chunk_hash": "b3722d75cfc5",
    "text": "There are 10 such common words.) ________________________________________ Step 5: Calculate Cosine Similarity Cosine Similarity = (V1 . V2) / (||V1|| * ||V2||) Cosine Similarity = 10 / (sqrt(14) * sqrt(12)) Cosine Similarity = 10 / sqrt(14 * 12) Cosine Similarity = 10 / sqrt(168) Using approximations: sqrt(168) is approximately 12.96 Cosine Similarity approx 10 / 12.96 Cosine Similarity approx 0.771 ________________________________________ * A cosine similarity of approximately 0.771 indicates a fairly strong similarity in terms of shared words (and thus, orientation of their vector representations) between the two sentences. * It effectively ignores differences in the length of the sentences (number of words) if those different words are not shared, and focuses on the proportion of shared content relative to their individual complexities (magnitudes). ________________________________________ Summary: Why Use Cosine Similarity? * Captures similarity of orientation (direction) of vectors, regardless of their magnitude (length). This is useful when the length of the document (e.g., word count) is less important than the content. * Works well for text similarity, especially when using term frequency vectors (like TF-IDF), where the magnitude can vary significantly between documents of different lengths but the relative importance of terms (direction) is key. * Commonly used in information retrieval for document similarity, in recommendation systems (e.g., finding similar users or items), and in clustering tasks.",
    "enhanced_text": "[NLP] There are 10 such common words.) ________________________________________ Step 5: Calculate Cosine Similarity Cosine Similarity = (V1 . V2) / (||V1|| * ||V2||) Cosine Similarity = 10 / (sqrt(14) * sqrt(12)) Cosine Similarity = 10 / sqrt(14 * 12) Cosine Similarity = 10 / sqrt(168) Using approximations: sqrt(168) is approximately 12.96 Cosine Similarity approx 10 / 12.96 Cosine Similarity approx 0.771 ________________________________________ * A cosine similarity of approximately 0.771 indicates a fairly strong similarity in terms of shared words (and thus, orientation of their vector representations) between the two sentences. * It effectively ignores differences in the length of the sentences (number of words) if those different words are not shared, and focuses on the proportion of shared content relative to their individual complexities (magnitudes). ________________________________________ Summary: Why Use Cosine Similarity? * Captures similarity of orientation (direction) of vectors, regardless of their magnitude (length). This is useful when the length of the document (e.g., word count) is less important than the content. * Works well for text similarity, especially when using term frequency vectors (like TF-IDF), where the magnitude can vary significantly between documents of different lengths but the relative importance of terms (direction) is key. * Commonly used in information retrieval for document similarity, in recommendation systems (e.g., finding similar users or items), and in clustering tasks.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(c) Cosine Similarity.txt",
    "file_name": "lec6-(c) Cosine Similarity.txt",
    "position_in_document": 80,
    "filename_keywords": [
      "lec6",
      "cosine",
      "similarity"
    ],
    "content_keywords": [
      "cosine",
      "range",
      "cosine similarity"
    ],
    "all_keywords": [
      "cosine",
      "range",
      "cosine similarity",
      "similarity",
      "lec6"
    ],
    "keyword_string": "cosine range cosine similarity similarity lec6",
    "token_count": 452,
    "word_count": 217,
    "sentence_count": 9,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.48008849557522126,
    "avg_sentence_length": 24.11111111111111
  },
  {
    "chunk_id": 143,
    "chunk_hash": "3c2f80296827",
    "text": "Pros & Cons of Cosine Similarity Scale-invariant:\nNot affected by vector magnitudes, only their direction, which helps when document lengths vary. Effective for high-dimensional data:\nHandles the large number of dimensions common in text data (e.g., thousands of unique words). Suitable for large datasets:\nComputationally efficient and scalable for comparing many documents. Easy interpretability:\nSimilarity ranges from -1 (opposite) to 1 (identical), with 0 meaning orthogonal (no similarity). Invariant to vector length:\nFocuses on similarity in terms of presence or frequency patterns rather than raw word counts (especially with binary vectors). No semantic understanding:\nTreats each word independently without grasping synonyms or related meanings—unless combined with word embeddings. Not ideal for very sparse data:\nWhen documents share few common words, vectors mostly contain zeros, which may lead to unreliable similarity scores. Sensitive to outliers:\nA small number of rare or unique words might disproportionately influence similarity if not managed (e.g., through weighting).",
    "enhanced_text": "[NLP] Pros & Cons of Cosine Similarity Scale-invariant:\nNot affected by vector magnitudes, only their direction, which helps when document lengths vary. Effective for high-dimensional data:\nHandles the large number of dimensions common in text data (e.g., thousands of unique words). Suitable for large datasets:\nComputationally efficient and scalable for comparing many documents. Easy interpretability:\nSimilarity ranges from -1 (opposite) to 1 (identical), with 0 meaning orthogonal (no similarity). Invariant to vector length:\nFocuses on similarity in terms of presence or frequency patterns rather than raw word counts (especially with binary vectors). No semantic understanding:\nTreats each word independently without grasping synonyms or related meanings—unless combined with word embeddings. Not ideal for very sparse data:\nWhen documents share few common words, vectors mostly contain zeros, which may lead to unreliable similarity scores. Sensitive to outliers:\nA small number of rare or unique words might disproportionately influence similarity if not managed (e.g., through weighting).",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(d) Pros & Cons of Cosine Similarity.txt",
    "file_name": "lec6-(d) Pros & Cons of Cosine Similarity.txt",
    "position_in_document": 9,
    "filename_keywords": [
      "cosine",
      "cons",
      "similarity",
      "lec6",
      "pros"
    ],
    "content_keywords": [
      "cons",
      "cosine similarity scale",
      "effective",
      "not",
      "pros",
      "handles"
    ],
    "all_keywords": [
      "cosine",
      "cons",
      "cosine similarity scale",
      "similarity",
      "not",
      "effective",
      "lec6",
      "pros",
      "handles"
    ],
    "keyword_string": "cosine cons cosine similarity scale similarity not effective lec6 pros handles",
    "token_count": 223,
    "word_count": 152,
    "sentence_count": 8,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6816143497757847,
    "avg_sentence_length": 19.0
  },
  {
    "chunk_id": 144,
    "chunk_hash": "71e280aa0982",
    "text": "TF-IDF (Term Frequency - Inverse Document Frequency) TF-IDF measures the importance of a term in a document relative to a corpus (a collection of documents). It balances how frequent a term is in a document (TF) with how rare it is across all documents (IDF). ________________________________________ * Measures how often a term appears in a document relative to the total number of terms in that document. TF(t) = (Number of times term t appears in document) / (Total number of terms in document) ________________________________________ Inverse Document Frequency (IDF) * Measures how rare or common a term is across all documents in the corpus. IDF(t) = log((Total number of documents) / (Number of documents containing term t)) (Note: The 'log' here is often the natural logarithm (ln), but other bases can be used consistently.) * Terms common to many documents (like \"the\", \"and\") have low IDF. * Rare terms have high IDF, giving them more weight. ________________________________________ * The TF-IDF score is the product of Term Frequency and Inverse Document Frequency. TF-IDF(t) = TF(t) * IDF(t) * A high TF-IDF score means the term is important in the specific document and rare in the corpus. ________________________________________ S1: \"I love Pakistan\" S2: \"Pakistan Zindabad\" * S1 = [\"I\", \"love\", \"Pakistan\"] * S2 = [\"Pakistan\", \"Zindabad\"] * S3 = [\"I\", \"am\", \"Pakistan\"]",
    "enhanced_text": "[NLP] TF-IDF (Term Frequency - Inverse Document Frequency) TF-IDF measures the importance of a term in a document relative to a corpus (a collection of documents). It balances how frequent a term is in a document (TF) with how rare it is across all documents (IDF). ________________________________________ * Measures how often a term appears in a document relative to the total number of terms in that document. TF(t) = (Number of times term t appears in document) / (Total number of terms in document) ________________________________________ Inverse Document Frequency (IDF) * Measures how rare or common a term is across all documents in the corpus. IDF(t) = log((Total number of documents) / (Number of documents containing term t)) (Note: The 'log' here is often the natural logarithm (ln), but other bases can be used consistently.) * Terms common to many documents (like \"the\", \"and\") have low IDF. * Rare terms have high IDF, giving them more weight. ________________________________________ * The TF-IDF score is the product of Term Frequency and Inverse Document Frequency. TF-IDF(t) = TF(t) * IDF(t) * A high TF-IDF score means the term is important in the specific document and rare in the corpus. ________________________________________ S1: \"I love Pakistan\" S2: \"Pakistan Zindabad\" * S1 = [\"I\", \"love\", \"Pakistan\"] * S2 = [\"Pakistan\", \"Zindabad\"] * S3 = [\"I\", \"am\", \"Pakistan\"]",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(e) TF-IDF.txt",
    "file_name": "lec6-(e) TF-IDF.txt",
    "position_in_document": 23,
    "filename_keywords": [
      "idf",
      "lec6"
    ],
    "content_keywords": [
      "idf",
      "inverse document frequency",
      "term frequency"
    ],
    "all_keywords": [
      "idf",
      "lec6",
      "inverse document frequency",
      "term frequency"
    ],
    "keyword_string": "idf lec6 inverse document frequency term frequency",
    "token_count": 501,
    "word_count": 219,
    "sentence_count": 10,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.437125748502994,
    "avg_sentence_length": 21.9
  },
  {
    "chunk_id": 145,
    "chunk_hash": "a431fc926272",
    "text": "Step 2: Calculate Term Frequency (TF) — What is it? Term Frequency is just a way to count how often a word shows up in a sentence (or document) compared to the total number of words in that sentence. * Count how many times the word appears in the sentence. * Divide that by the total number of words in the sentence. Sentence 1: \"I love Pakistan\" * \"I\" appears 1 time -> TF(\"I\", S1) = 1 / 3 = 0.33 (approximately) * \"love\" appears 1 time -> TF(\"love\", S1) = 1 / 3 = 0.33 (approximately) * \"Pakistan\" appears 1 time -> TF(\"Pakistan\", S1) = 1 / 3 = 0.33 (approximately) This means each word has equal frequency in this sentence because they appear once and the sentence has three words. Step 3: Calculate IDF (Assuming natural log ln, corpus size = 3 documents) * The term \"I\" appears in 2 documents (S1, S3). IDF(\"I\") = ln(Total number of documents / Number of documents containing \"I\") IDF(\"I\") = ln(3 / 2) which is approximately 0.405 * The term \"love\" appears in 1 document (S1). IDF(\"love\") = ln(Total number of documents / Number of documents containing \"love\") IDF(\"love\") = ln(3 / 1) which is approximately 1.099 * The term \"Pakistan\" appears in all 3 documents (S1, S2, S3). IDF(\"Pakistan\") = ln(Total number of documents / Number of documents containing \"Pakistan\") IDF(\"Pakistan\") = ln(3 / 3) = ln(1) = 0 * The term \"Zindabad\" appears in 1 document (S2). IDF(\"Zindabad\") = ln(Total number of documents / Number of documents containing \"Zindabad\") IDF(\"Zindabad\") = ln(3 / 1) which is approximately 1.099 * The term \"am\" appears in 1 document (S3). IDF(\"am\") = ln(Total number of documents / Number of documents containing \"am\")",
    "enhanced_text": "[NLP] Step 2: Calculate Term Frequency (TF) — What is it? Term Frequency is just a way to count how often a word shows up in a sentence (or document) compared to the total number of words in that sentence. * Count how many times the word appears in the sentence. * Divide that by the total number of words in the sentence. Sentence 1: \"I love Pakistan\" * \"I\" appears 1 time -> TF(\"I\", S1) = 1 / 3 = 0.33 (approximately) * \"love\" appears 1 time -> TF(\"love\", S1) = 1 / 3 = 0.33 (approximately) * \"Pakistan\" appears 1 time -> TF(\"Pakistan\", S1) = 1 / 3 = 0.33 (approximately) This means each word has equal frequency in this sentence because they appear once and the sentence has three words. Step 3: Calculate IDF (Assuming natural log ln, corpus size = 3 documents) * The term \"I\" appears in 2 documents (S1, S3). IDF(\"I\") = ln(Total number of documents / Number of documents containing \"I\") IDF(\"I\") = ln(3 / 2) which is approximately 0.405 * The term \"love\" appears in 1 document (S1). IDF(\"love\") = ln(Total number of documents / Number of documents containing \"love\") IDF(\"love\") = ln(3 / 1) which is approximately 1.099 * The term \"Pakistan\" appears in all 3 documents (S1, S2, S3). IDF(\"Pakistan\") = ln(Total number of documents / Number of documents containing \"Pakistan\") IDF(\"Pakistan\") = ln(3 / 3) = ln(1) = 0 * The term \"Zindabad\" appears in 1 document (S2). IDF(\"Zindabad\") = ln(Total number of documents / Number of documents containing \"Zindabad\") IDF(\"Zindabad\") = ln(3 / 1) which is approximately 1.099 * The term \"am\" appears in 1 document (S3). IDF(\"am\") = ln(Total number of documents / Number of documents containing \"am\")",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(e) TF-IDF.txt",
    "file_name": "lec6-(e) TF-IDF.txt",
    "position_in_document": 47,
    "filename_keywords": [
      "idf",
      "lec6"
    ],
    "content_keywords": [
      "idf",
      "inverse document frequency",
      "term frequency"
    ],
    "all_keywords": [
      "idf",
      "lec6",
      "inverse document frequency",
      "term frequency"
    ],
    "keyword_string": "idf lec6 inverse document frequency term frequency",
    "token_count": 501,
    "word_count": 290,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5788423153692615,
    "avg_sentence_length": 26.363636363636363
  },
  {
    "chunk_id": 146,
    "chunk_hash": "f92b19061952",
    "text": "IDF(\"Zindabad\") = ln(Total number of documents / Number of documents containing \"Zindabad\") IDF(\"Zindabad\") = ln(3 / 1) which is approximately 1.099 * The term \"am\" appears in 1 document (S3). IDF(\"am\") = ln(Total number of documents / Number of documents containing \"am\") IDF(\"am\") = ln(3 / 1) which is approximately 1.099 Step 4: Calculate TF-IDF — What is it? Now, TF-IDF takes the TF (how often the word appears in one sentence) and multiplies it by the IDF (how unique or rare the word is across all sentences). * If a word appears a lot in the sentence AND is rare in other sentences, it gets a high score. * If a word is common across all sentences (like \"Pakistan\" in this example), the score will be low or zero. How to calculate TF-IDF: TF-IDF(term, document) = Term Frequency(term, document) * Inverse Document Frequency(term) In Sentence 1, for the term \"love\": * TF(\"love\", S1) = 0.33 (approximately, from step 2) * IDF(\"love\") = 1.099 (approximately, because it appears in only 1 sentence out of 3) * So, TF-IDF(\"love\", S1) = 0.33 * 1.099 which is approximately 0.36 That means \"love\" is important in Sentence 1 and unique to it (in this small corpus), so it gets a higher score. For the term \"Pakistan\" in Sentence 1: * TF(\"Pakistan\", S1) = 0.33 (approximately) * IDF(\"Pakistan\") = 0 (since \"Pakistan\" appears in all sentences) * So, TF-IDF(\"Pakistan\", S1) = 0.33 * 0 = 0, meaning it’s not useful to differentiate sentences based on this term in this corpus. Pros & Cons of TF-IDF * Weights terms by importance: Gives higher importance to terms that are significant to a document but not common across all documents. * Downweights common words: Common stop words like \"the\", \"a\", etc., get very low scores (due to high document frequency, leading to low IDF). * Good for vector representation: Turns text into numerical vectors which can be used for Machine Learning models or similarity calculations. * Language independent: Based purely on statistical counts, no language-specific rules are required for its basic calculation.",
    "enhanced_text": "[NLP] IDF(\"Zindabad\") = ln(Total number of documents / Number of documents containing \"Zindabad\") IDF(\"Zindabad\") = ln(3 / 1) which is approximately 1.099 * The term \"am\" appears in 1 document (S3). IDF(\"am\") = ln(Total number of documents / Number of documents containing \"am\") IDF(\"am\") = ln(3 / 1) which is approximately 1.099 Step 4: Calculate TF-IDF — What is it? Now, TF-IDF takes the TF (how often the word appears in one sentence) and multiplies it by the IDF (how unique or rare the word is across all sentences). * If a word appears a lot in the sentence AND is rare in other sentences, it gets a high score. * If a word is common across all sentences (like \"Pakistan\" in this example), the score will be low or zero. How to calculate TF-IDF: TF-IDF(term, document) = Term Frequency(term, document) * Inverse Document Frequency(term) In Sentence 1, for the term \"love\": * TF(\"love\", S1) = 0.33 (approximately, from step 2) * IDF(\"love\") = 1.099 (approximately, because it appears in only 1 sentence out of 3) * So, TF-IDF(\"love\", S1) = 0.33 * 1.099 which is approximately 0.36 That means \"love\" is important in Sentence 1 and unique to it (in this small corpus), so it gets a higher score. For the term \"Pakistan\" in Sentence 1: * TF(\"Pakistan\", S1) = 0.33 (approximately) * IDF(\"Pakistan\") = 0 (since \"Pakistan\" appears in all sentences) * So, TF-IDF(\"Pakistan\", S1) = 0.33 * 0 = 0, meaning it’s not useful to differentiate sentences based on this term in this corpus. Pros & Cons of TF-IDF * Weights terms by importance: Gives higher importance to terms that are significant to a document but not common across all documents. * Downweights common words: Common stop words like \"the\", \"a\", etc., get very low scores (due to high document frequency, leading to low IDF). * Good for vector representation: Turns text into numerical vectors which can be used for Machine Learning models or similarity calculations. * Language independent: Based purely on statistical counts, no language-specific rules are required for its basic calculation.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(e) TF-IDF.txt",
    "file_name": "lec6-(e) TF-IDF.txt",
    "position_in_document": 68,
    "filename_keywords": [
      "idf",
      "lec6"
    ],
    "content_keywords": [
      "idf",
      "inverse document frequency",
      "term frequency"
    ],
    "all_keywords": [
      "idf",
      "lec6",
      "inverse document frequency",
      "term frequency"
    ],
    "keyword_string": "idf lec6 inverse document frequency term frequency",
    "token_count": 568,
    "word_count": 345,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6073943661971831,
    "avg_sentence_length": 31.363636363636363
  },
  {
    "chunk_id": 147,
    "chunk_hash": "001168961944",
    "text": "What are Vector Space Models (VSMs)? * Vector Space Models (VSMs) are a way to represent text documents as mathematical vectors, which are essentially lists of numbers. * In this model, each unique word found across the entire collection of documents (corpus) acts as a dimension in a high-dimensional space. * This numerical representation allows text to be compared, analyzed, or manipulated using mathematical operations and algorithms. ________________________________________ What are VSMs used for? VSMs are utilized in a variety of Natural Language Processing (NLP) and Information Retrieval (IR) tasks, including: * Finding similar documents or texts: Identifying documents that share similar content. * Classifying documents into categories: Assigning documents to predefined topics or classes. * Summarizing content: Extracting key information from texts. * Identifying topics: Discovering underlying themes in a collection of documents (topic modeling). * Translating languages: As a component in some machine translation systems. * Any task where understanding the semantic relationship or comparing text content is important. ________________________________________ How do we build and use a VSM? Get the text data: Collect the documents, sentences, or text snippets you want to model. Clean the text (Preprocessing): * Tokenization: Break down the text into individual words or tokens. * Lowercasing: Convert all text to a consistent case, usually lowercase, to treat words like \"The\" and \"the\" as the same. * Stop word removal: Remove common words (e.g., \"is\", \"a\", \"the\", \"in\") that typically don't carry significant meaning for differentiating documents. * Stemming/Lemmatization: Reduce words to their root or base form (e.g., \"running\" to \"run\", \"studies\" to \"study\"). Lemmatization is generally more linguistically accurate than stemming. Build a vocabulary: Create a comprehensive list of all unique words (terms) that remain after the cleaning process across all documents in the corpus.",
    "enhanced_text": "[NLP] What are Vector Space Models (VSMs)? * Vector Space Models (VSMs) are a way to represent text documents as mathematical vectors, which are essentially lists of numbers. * In this model, each unique word found across the entire collection of documents (corpus) acts as a dimension in a high-dimensional space. * This numerical representation allows text to be compared, analyzed, or manipulated using mathematical operations and algorithms. ________________________________________ What are VSMs used for? VSMs are utilized in a variety of Natural Language Processing (NLP) and Information Retrieval (IR) tasks, including: * Finding similar documents or texts: Identifying documents that share similar content. * Classifying documents into categories: Assigning documents to predefined topics or classes. * Summarizing content: Extracting key information from texts. * Identifying topics: Discovering underlying themes in a collection of documents (topic modeling). * Translating languages: As a component in some machine translation systems. * Any task where understanding the semantic relationship or comparing text content is important. ________________________________________ How do we build and use a VSM? Get the text data: Collect the documents, sentences, or text snippets you want to model. Clean the text (Preprocessing): * Tokenization: Break down the text into individual words or tokens. * Lowercasing: Convert all text to a consistent case, usually lowercase, to treat words like \"The\" and \"the\" as the same. * Stop word removal: Remove common words (e.g., \"is\", \"a\", \"the\", \"in\") that typically don't carry significant meaning for differentiating documents. * Stemming/Lemmatization: Reduce words to their root or base form (e.g., \"running\" to \"run\", \"studies\" to \"study\"). Lemmatization is generally more linguistically accurate than stemming. Build a vocabulary: Create a comprehensive list of all unique words (terms) that remain after the cleaning process across all documents in the corpus.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(f) Vector Space Models (VSMs).txt",
    "file_name": "lec6-(f) Vector Space Models (VSMs).txt",
    "position_in_document": 23,
    "filename_keywords": [
      "space",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "content_keywords": [
      "vector space models",
      "vsms",
      "what"
    ],
    "all_keywords": [
      "vector space models",
      "space",
      "what",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "keyword_string": "vector space models space what vector models vsms lec6",
    "token_count": 500,
    "word_count": 290,
    "sentence_count": 19,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.58,
    "avg_sentence_length": 15.263157894736842
  },
  {
    "chunk_id": 148,
    "chunk_hash": "da8d0a4bfaa9",
    "text": "* Stemming/Lemmatization: Reduce words to their root or base form (e.g., \"running\" to \"run\", \"studies\" to \"study\"). Lemmatization is generally more linguistically accurate than stemming. Build a vocabulary: Create a comprehensive list of all unique words (terms) that remain after the cleaning process across all documents in the corpus. The order of words in this vocabulary will define the order of components in the vectors. Create vectors (Document Representation): Represent each document as a numerical vector. Each element in the vector corresponds to a word in the vocabulary. The value of each element can be determined in several ways: * Binary: 1 if the word is present in the document, 0 if absent. * Term Frequency (TF): The count of how often a word appears in the document, possibly normalized by the total number of words in that document. * TF-IDF (Term Frequency-Inverse Document Frequency): A weighted score that gives higher importance to words that are frequent in a specific document but rare across the entire corpus. Calculate similarity: Use a similarity measure (like cosine similarity) to quantify how close or related document vectors are to each other in the vector space. Search/query (Information Retrieval): For search applications, a user's query is also converted into a vector. This query vector is then compared against all document vectors in the corpus using the chosen similarity measure to find the most relevant documents. ________________________________________ * Flexible: Can be applied to a wide range of text analysis tasks and different types of textual data. * Scalable: Can handle large collections of documents and extensive vocabularies, especially with efficient data structures. * Effective at measuring similarity: Generally good at determining the degree of relatedness between two pieces of text based on shared terms. * Language independent (at a basic level): The core mechanism of counting words and creating vectors doesn't rely on deep grammatical rules of a specific language, making it broadly applicable. * Handles sparse data well: Most document vectors will have many zero entries (since a document usually contains only a small subset of the total vocabulary). VSMs and associated algorithms are often designed to work efficiently with such sparse vectors. ________________________________________ * No context understanding (Bag-of-Words assumption): VSMs typically treat words as independent units, ignoring the order of words and the grammatical structure.",
    "enhanced_text": "[NLP] * Stemming/Lemmatization: Reduce words to their root or base form (e.g., \"running\" to \"run\", \"studies\" to \"study\"). Lemmatization is generally more linguistically accurate than stemming. Build a vocabulary: Create a comprehensive list of all unique words (terms) that remain after the cleaning process across all documents in the corpus. The order of words in this vocabulary will define the order of components in the vectors. Create vectors (Document Representation): Represent each document as a numerical vector. Each element in the vector corresponds to a word in the vocabulary. The value of each element can be determined in several ways: * Binary: 1 if the word is present in the document, 0 if absent. * Term Frequency (TF): The count of how often a word appears in the document, possibly normalized by the total number of words in that document. * TF-IDF (Term Frequency-Inverse Document Frequency): A weighted score that gives higher importance to words that are frequent in a specific document but rare across the entire corpus. Calculate similarity: Use a similarity measure (like cosine similarity) to quantify how close or related document vectors are to each other in the vector space. Search/query (Information Retrieval): For search applications, a user's query is also converted into a vector. This query vector is then compared against all document vectors in the corpus using the chosen similarity measure to find the most relevant documents. ________________________________________ * Flexible: Can be applied to a wide range of text analysis tasks and different types of textual data. * Scalable: Can handle large collections of documents and extensive vocabularies, especially with efficient data structures. * Effective at measuring similarity: Generally good at determining the degree of relatedness between two pieces of text based on shared terms. * Language independent (at a basic level): The core mechanism of counting words and creating vectors doesn't rely on deep grammatical rules of a specific language, making it broadly applicable. * Handles sparse data well: Most document vectors will have many zero entries (since a document usually contains only a small subset of the total vocabulary). VSMs and associated algorithms are often designed to work efficiently with such sparse vectors. ________________________________________ * No context understanding (Bag-of-Words assumption): VSMs typically treat words as independent units, ignoring the order of words and the grammatical structure.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(f) Vector Space Models (VSMs).txt",
    "file_name": "lec6-(f) Vector Space Models (VSMs).txt",
    "position_in_document": 42,
    "filename_keywords": [
      "space",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "content_keywords": [
      "vector space models",
      "vsms",
      "what"
    ],
    "all_keywords": [
      "vector space models",
      "space",
      "what",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "keyword_string": "vector space models space what vector models vsms lec6",
    "token_count": 569,
    "word_count": 382,
    "sentence_count": 19,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.671353251318102,
    "avg_sentence_length": 20.105263157894736
  },
  {
    "chunk_id": 149,
    "chunk_hash": "32d68806fa08",
    "text": "VSMs and associated algorithms are often designed to work efficiently with such sparse vectors. ________________________________________ * No context understanding (Bag-of-Words assumption): VSMs typically treat words as independent units, ignoring the order of words and the grammatical structure. This means the meaning derived from word context (e.g., \"apple\" the fruit vs. \"Apple\" the company) can be lost. \"Bank\" in \"river bank\" and \"investment bank\" would be treated as the same dimension. * Struggles with phrases or sentence meaning: The model doesn't inherently capture the meaning of multi-word expressions, idioms, or the overall semantic meaning of a sentence beyond the sum of its words. * Semantic drift and polysemy problems: * Semantic drift: The meaning of words can change over time, which VSMs built on older corpora might not capture. * Polysemy: Words with multiple meanings can lead to ambiguous or muddled vector representations because the same term contributes to the vector regardless of its intended sense in a particular context. ________________________________________ Example: Vector Space Model with 3 documents * Doc1: \"I love Pakistan\" * Doc2: \"Pakistan Zindabad\" * Doc3: \"I am Pakistan\" ________________________________________ Step 1: Build Vocabulary (Assuming minimal preprocessing: tokenization and lowercasing. No stop word removal or stemming for this simple example.) Collect all unique words from these documents: Vocabulary = [\"i\", \"love\", \"pakistan\", \"zindabad\", \"am\"] The order in this list determines the dimension of our vectors. ________________________________________ Step 2: Calculate Term Frequency (TF) Term Frequency (TF) for a term 't' in a document 'd' is calculated as: TF(t, d) = (Number of times term 't' appears in document 'd') / (Total number of terms in document 'd') * Doc1: \"i love pakistan\" (3 words) * TF(\"i\", Doc1) = 1/3 * TF(\"love\", Doc1) = 1/3",
    "enhanced_text": "[NLP] VSMs and associated algorithms are often designed to work efficiently with such sparse vectors. ________________________________________ * No context understanding (Bag-of-Words assumption): VSMs typically treat words as independent units, ignoring the order of words and the grammatical structure. This means the meaning derived from word context (e.g., \"apple\" the fruit vs. \"Apple\" the company) can be lost. \"Bank\" in \"river bank\" and \"investment bank\" would be treated as the same dimension. * Struggles with phrases or sentence meaning: The model doesn't inherently capture the meaning of multi-word expressions, idioms, or the overall semantic meaning of a sentence beyond the sum of its words. * Semantic drift and polysemy problems: * Semantic drift: The meaning of words can change over time, which VSMs built on older corpora might not capture. * Polysemy: Words with multiple meanings can lead to ambiguous or muddled vector representations because the same term contributes to the vector regardless of its intended sense in a particular context. ________________________________________ Example: Vector Space Model with 3 documents * Doc1: \"I love Pakistan\" * Doc2: \"Pakistan Zindabad\" * Doc3: \"I am Pakistan\" ________________________________________ Step 1: Build Vocabulary (Assuming minimal preprocessing: tokenization and lowercasing. No stop word removal or stemming for this simple example.) Collect all unique words from these documents: Vocabulary = [\"i\", \"love\", \"pakistan\", \"zindabad\", \"am\"] The order in this list determines the dimension of our vectors. ________________________________________ Step 2: Calculate Term Frequency (TF) Term Frequency (TF) for a term 't' in a document 'd' is calculated as: TF(t, d) = (Number of times term 't' appears in document 'd') / (Total number of terms in document 'd') * Doc1: \"i love pakistan\" (3 words) * TF(\"i\", Doc1) = 1/3 * TF(\"love\", Doc1) = 1/3",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(f) Vector Space Models (VSMs).txt",
    "file_name": "lec6-(f) Vector Space Models (VSMs).txt",
    "position_in_document": 67,
    "filename_keywords": [
      "space",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "content_keywords": [
      "vector space models",
      "vsms",
      "what"
    ],
    "all_keywords": [
      "vector space models",
      "space",
      "what",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "keyword_string": "vector space models space what vector models vsms lec6",
    "token_count": 594,
    "word_count": 285,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.4797979797979798,
    "avg_sentence_length": 25.90909090909091
  },
  {
    "chunk_id": 150,
    "chunk_hash": "70b918b86139",
    "text": "* TF(\"pakistan\", Doc1) = 1/3 * TF(\"zindabad\", Doc1) = 0/3 = 0 * TF(\"am\", Doc1) = 0/3 = 0 * Doc2: \"pakistan zindabad\" (2 words) * TF(\"i\", Doc2) = 0/2 = 0 * TF(\"love\", Doc2) = 0/2 = 0 * TF(\"pakistan\", Doc2) = 1/2 * TF(\"zindabad\", Doc2) = 1/2 * TF(\"am\", Doc2) = 0/2 = 0 * Doc3: \"i am pakistan\" (3 words) * TF(\"i\", Doc3) = 1/3 * TF(\"love\", Doc3) = 0/3 = 0 * TF(\"pakistan\", Doc3) = 1/3 * TF(\"zindabad\", Doc3) = 0/3 = 0 * TF(\"am\", Doc3) = 1/3 ________________________________________ Step 3: Represent each document as a vector Using the vocabulary order [\"i\", \"love\", \"pakistan\", \"zindabad\", \"am\"], the TF vectors are: * V_Doc1 = [1/3, 1/3, 1/3, 0, 0] [0.333, 0.333, 0.333, 0, 0]) * V_Doc2 = [0, 0, 1/2, 1/2, 0] * V_Doc3 = [1/3, 0, 1/3, 0, 1/3] [0.333, 0, 0.333, 0, 0.333]) ________________________________________ Step 4: Compare documents using these vectors (e.g., using Cosine Similarity) We can now calculate how similar these documents are by comparing their vectors mathematically.",
    "enhanced_text": "[NLP] * TF(\"pakistan\", Doc1) = 1/3 * TF(\"zindabad\", Doc1) = 0/3 = 0 * TF(\"am\", Doc1) = 0/3 = 0 * Doc2: \"pakistan zindabad\" (2 words) * TF(\"i\", Doc2) = 0/2 = 0 * TF(\"love\", Doc2) = 0/2 = 0 * TF(\"pakistan\", Doc2) = 1/2 * TF(\"zindabad\", Doc2) = 1/2 * TF(\"am\", Doc2) = 0/2 = 0 * Doc3: \"i am pakistan\" (3 words) * TF(\"i\", Doc3) = 1/3 * TF(\"love\", Doc3) = 0/3 = 0 * TF(\"pakistan\", Doc3) = 1/3 * TF(\"zindabad\", Doc3) = 0/3 = 0 * TF(\"am\", Doc3) = 1/3 ________________________________________ Step 3: Represent each document as a vector Using the vocabulary order [\"i\", \"love\", \"pakistan\", \"zindabad\", \"am\"], the TF vectors are: * V_Doc1 = [1/3, 1/3, 1/3, 0, 0] [0.333, 0.333, 0.333, 0, 0]) * V_Doc2 = [0, 0, 1/2, 1/2, 0] * V_Doc3 = [1/3, 0, 1/3, 0, 1/3] [0.333, 0, 0.333, 0, 0.333]) ________________________________________ Step 4: Compare documents using these vectors (e.g., using Cosine Similarity) We can now calculate how similar these documents are by comparing their vectors mathematically.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(f) Vector Space Models (VSMs).txt",
    "file_name": "lec6-(f) Vector Space Models (VSMs).txt",
    "position_in_document": 93,
    "filename_keywords": [
      "space",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "content_keywords": [
      "vector space models",
      "vsms",
      "what"
    ],
    "all_keywords": [
      "vector space models",
      "space",
      "what",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "keyword_string": "vector space models space what vector models vsms lec6",
    "token_count": 504,
    "word_count": 174,
    "sentence_count": 1,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.34523809523809523,
    "avg_sentence_length": 174.0
  },
  {
    "chunk_id": 151,
    "chunk_hash": "02cda2df68fa",
    "text": "Let's calculate the cosine similarity between Doc1 and Doc3. Formula for Cosine Similarity: Cosine Similarity(A, B) = (A . B = Dot product of A and B ||A|| = Magnitude of A ||B|| = Magnitude of B Let A = V_Doc1 = [1/3, 1/3, 1/3, 0, 0] Let B = V_Doc3 = [1/3, 0, 1/3, 0, 1/3] Calculate the Dot Product (V_Doc1 . V_Doc3 = (1/3 * 1/3) + (1/3 * 0) + (1/3 * 1/3) + (0 * 0) + (0 * 1/3) = 1/9 + 0 + 1/9 + 0 + 0 Calculate the Magnitude of V_Doc1 (||V_Doc1||): ||V_Doc1|| = sqrt((1/3)^2 + (1/3)^2 + (1/3)^2 + 0^2 + 0^2) = sqrt(1/9 + 1/9 + 1/9 + 0 + 0) c. Calculate the Magnitude of V_Doc3 (||V_Doc3||): ||V_Doc3|| = sqrt((1/3)^2 + 0^2 + (1/3)^2 + 0^2 + (1/3)^2) = sqrt(1/9 + 0 + 1/9 + 0 + 1/9) d. Calculate Cosine Similarity: Cosine Similarity(V_Doc1, V_Doc3) = (2/9) / (sqrt(1/3) * sqrt(1/3)) Interpretation of this example: The cosine similarity between Doc1 (\"I love Pakistan\") and Doc3 (\"I am Pakistan\") is approximately 0.667. This indicates a moderate to strong similarity, as they share the terms \"i\" and \"pakistan\" with similar term frequencies within their respective documents. Documents sharing more common words with similar frequencies (or TF-IDF scores) will have higher cosine similarity scores, indicating they are more similar in content.",
    "enhanced_text": "[NLP] Let's calculate the cosine similarity between Doc1 and Doc3. Formula for Cosine Similarity: Cosine Similarity(A, B) = (A . B = Dot product of A and B ||A|| = Magnitude of A ||B|| = Magnitude of B Let A = V_Doc1 = [1/3, 1/3, 1/3, 0, 0] Let B = V_Doc3 = [1/3, 0, 1/3, 0, 1/3] Calculate the Dot Product (V_Doc1 . V_Doc3 = (1/3 * 1/3) + (1/3 * 0) + (1/3 * 1/3) + (0 * 0) + (0 * 1/3) = 1/9 + 0 + 1/9 + 0 + 0 Calculate the Magnitude of V_Doc1 (||V_Doc1||): ||V_Doc1|| = sqrt((1/3)^2 + (1/3)^2 + (1/3)^2 + 0^2 + 0^2) = sqrt(1/9 + 1/9 + 1/9 + 0 + 0) c. Calculate the Magnitude of V_Doc3 (||V_Doc3||): ||V_Doc3|| = sqrt((1/3)^2 + 0^2 + (1/3)^2 + 0^2 + (1/3)^2) = sqrt(1/9 + 0 + 1/9 + 0 + 1/9) d. Calculate Cosine Similarity: Cosine Similarity(V_Doc1, V_Doc3) = (2/9) / (sqrt(1/3) * sqrt(1/3)) Interpretation of this example: The cosine similarity between Doc1 (\"I love Pakistan\") and Doc3 (\"I am Pakistan\") is approximately 0.667. This indicates a moderate to strong similarity, as they share the terms \"i\" and \"pakistan\" with similar term frequencies within their respective documents. Documents sharing more common words with similar frequencies (or TF-IDF scores) will have higher cosine similarity scores, indicating they are more similar in content.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(f) Vector Space Models (VSMs).txt",
    "file_name": "lec6-(f) Vector Space Models (VSMs).txt",
    "position_in_document": 116,
    "filename_keywords": [
      "space",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "content_keywords": [
      "vector space models",
      "vsms",
      "what"
    ],
    "all_keywords": [
      "vector space models",
      "space",
      "what",
      "vector",
      "models",
      "vsms",
      "lec6"
    ],
    "keyword_string": "vector space models space what vector models vsms lec6",
    "token_count": 496,
    "word_count": 229,
    "sentence_count": 6,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.46169354838709675,
    "avg_sentence_length": 38.166666666666664
  },
  {
    "chunk_id": 152,
    "chunk_hash": "987699d136c8",
    "text": "Definition of Question Answering (QA):\nQuestion Answering (QA) is a field of computer science that focuses on building systems capable of answering questions posed in natural language. Examples of QA systems: AskJeeves (a well-known early example) AnswerBus (an open-domain question answering system) Ionaut, EasyAsk, AnswerLogic, AnswerFriend, Start, LCC, Quasm, Mulder, Webclopedia, etc. From AskJeeves: \"Search engines do not speak your language. They make you speak their language; a language that's strange, confusing, and includes words that no one is entirely sure of their meaning.\" QA engines attempt to let you ask your question the way you'd normally ask it. QA systems are beneficial for inexperienced users, simplifying the information retrieval process. The goal is to move beyond simply finding a document to directly providing an answer (Document=Answer? A typical Question Answering system involves several key components: Natural Language Processing (NLP): Semantic Processing: Understanding the meaning of words and sentences. Syntactic Processing: Analyzing the grammatical structure of sentences. Parsing: Breaking down sentences into their grammatical components. Knowledge Base: A repository of structured information from which answers can be retrieved or inferred. Answer Processing: The final stage where candidate answers are generated, ranked, and presented to the user. Natural Language Processing (NLP) QA Engines have unique processes related to NLP. START-Natural Language System (an example of an NLP system used in QA): Parsing: Analyzing sentence structure. Natural Language Annotation: Tagging and labeling linguistic features. Processing Component: Core logic for handling natural language. Conceptual Table Explanation (QA System Output Types):\nThis table outlines various Question Answering (QA) systems and the typical format of the output they provide as answers: AnswerBus: Provides answers in the form of Sentences. AskJeeves: Primarily returns relevant Documents. IONAUT: Delivers Passages (sections of text) as answers. LCC: Outputs answers as Sentences. Mulder: Provides Extracted answers, implying precise snippets. QuASM: Returns Document blocks, which are larger chunks than passages. START: Outputs a Mixture of answer types. Webclopedia: Provides answers in the form of Sentences. This table highlights the diversity in how different QA systems present their findings, ranging from full documents to specific sentences or extracted facts.",
    "enhanced_text": "[NLP] Definition of Question Answering (QA):\nQuestion Answering (QA) is a field of computer science that focuses on building systems capable of answering questions posed in natural language. Examples of QA systems: AskJeeves (a well-known early example) AnswerBus (an open-domain question answering system) Ionaut, EasyAsk, AnswerLogic, AnswerFriend, Start, LCC, Quasm, Mulder, Webclopedia, etc. From AskJeeves: \"Search engines do not speak your language. They make you speak their language; a language that's strange, confusing, and includes words that no one is entirely sure of their meaning.\" QA engines attempt to let you ask your question the way you'd normally ask it. QA systems are beneficial for inexperienced users, simplifying the information retrieval process. The goal is to move beyond simply finding a document to directly providing an answer (Document=Answer? A typical Question Answering system involves several key components: Natural Language Processing (NLP): Semantic Processing: Understanding the meaning of words and sentences. Syntactic Processing: Analyzing the grammatical structure of sentences. Parsing: Breaking down sentences into their grammatical components. Knowledge Base: A repository of structured information from which answers can be retrieved or inferred. Answer Processing: The final stage where candidate answers are generated, ranked, and presented to the user. Natural Language Processing (NLP) QA Engines have unique processes related to NLP. START-Natural Language System (an example of an NLP system used in QA): Parsing: Analyzing sentence structure. Natural Language Annotation: Tagging and labeling linguistic features. Processing Component: Core logic for handling natural language. Conceptual Table Explanation (QA System Output Types):\nThis table outlines various Question Answering (QA) systems and the typical format of the output they provide as answers: AnswerBus: Provides answers in the form of Sentences. AskJeeves: Primarily returns relevant Documents. IONAUT: Delivers Passages (sections of text) as answers. LCC: Outputs answers as Sentences. Mulder: Provides Extracted answers, implying precise snippets. QuASM: Returns Document blocks, which are larger chunks than passages. START: Outputs a Mixture of answer types. Webclopedia: Provides answers in the form of Sentences. This table highlights the diversity in how different QA systems present their findings, ranging from full documents to specific sentences or extracted facts.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(a).txt",
    "file_name": "lec7-(a).txt",
    "position_in_document": 33,
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "askjeeves",
      "question answering",
      "definition",
      "examples"
    ],
    "all_keywords": [
      "askjeeves",
      "question answering",
      "lec7",
      "definition",
      "examples"
    ],
    "keyword_string": "askjeeves question answering lec7 definition examples",
    "token_count": 506,
    "word_count": 347,
    "sentence_count": 25,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6857707509881423,
    "avg_sentence_length": 13.88
  },
  {
    "chunk_id": 153,
    "chunk_hash": "af2761643dc4",
    "text": "IONAUT: Delivers Passages (sections of text) as answers. LCC: Outputs answers as Sentences. Mulder: Provides Extracted answers, implying precise snippets. QuASM: Returns Document blocks, which are larger chunks than passages. START: Outputs a Mixture of answer types. Webclopedia: Provides answers in the form of Sentences. This table highlights the diversity in how different QA systems present their findings, ranging from full documents to specific sentences or extracted facts. AskJeeves (now Ask.com) was a popular early QA system with key features: It had its own knowledge base and partnered with other sources to answer questions. It catalogued previous questions to improve its understanding and response over time. It utilized an answer processing engine, often generating responses based on question templates. Conceptual Diagram Explanation (AnswerBus Flowchart):\nThe flowchart illustrates the process of the AnswerBus Question Answering system: User Question: The process begins with a natural language question from the user. Translated Question: The user's question is first translated or transformed into a more structured query. From the translated question, three parallel paths emerge: Question Type: The system analyzes the question to determine its type (e.g., factoid, list). Search Engine Specific Query: A query optimized for traditional search engines is generated. Matching Words: Keywords or phrases are extracted for direct matching. Hit Lists from Search Engines: Results (e.g., documents, passages) are retrieved from various search engines based on the generated queries. Extracted Sentences: Relevant sentences are extracted from the retrieved hit lists. Candidate Answers: From these extracted sentences, potential answers are identified. Ranked Answers: Finally, the candidate answers are ranked by relevance and presented to the user. This diagram shows AnswerBus as an open-domain QA system that leverages traditional search engines and then applies further processing to extract and rank specific answers from the search results. Problems (in Question Answering) Challenges in Question Answering include: \"How\" and \"Why\" questions: These often require deeper reasoning and causal understanding beyond simple fact retrieval. \": Requires understanding events and narratives. \": Requires understanding actions and consequences. Correctness: Ensuring the provided answer is factually accurate. Answer Presentation: Presenting the answer clearly, concisely, and in an understandable format. Examples of humorous or difficult-to-answer questions from Webclopedia, highlighting challenges in QA: Question: Where do lobsters like to live? Answer: on a Canadian airline (Incorrect, likely a result of misinterpretation or a factoid related to a specific event). Question: Where do hyenas live? Answer: in Saudi Arabia (Plausible but incomplete). Answer: in the back of pick-up trucks (Incorrect, likely a humorous or out-of-context misinterpretation). Question: Where are zebras most likely found?",
    "enhanced_text": "[NLP] IONAUT: Delivers Passages (sections of text) as answers. LCC: Outputs answers as Sentences. Mulder: Provides Extracted answers, implying precise snippets. QuASM: Returns Document blocks, which are larger chunks than passages. START: Outputs a Mixture of answer types. Webclopedia: Provides answers in the form of Sentences. This table highlights the diversity in how different QA systems present their findings, ranging from full documents to specific sentences or extracted facts. AskJeeves (now Ask.com) was a popular early QA system with key features: It had its own knowledge base and partnered with other sources to answer questions. It catalogued previous questions to improve its understanding and response over time. It utilized an answer processing engine, often generating responses based on question templates. Conceptual Diagram Explanation (AnswerBus Flowchart):\nThe flowchart illustrates the process of the AnswerBus Question Answering system: User Question: The process begins with a natural language question from the user. Translated Question: The user's question is first translated or transformed into a more structured query. From the translated question, three parallel paths emerge: Question Type: The system analyzes the question to determine its type (e.g., factoid, list). Search Engine Specific Query: A query optimized for traditional search engines is generated. Matching Words: Keywords or phrases are extracted for direct matching. Hit Lists from Search Engines: Results (e.g., documents, passages) are retrieved from various search engines based on the generated queries. Extracted Sentences: Relevant sentences are extracted from the retrieved hit lists. Candidate Answers: From these extracted sentences, potential answers are identified. Ranked Answers: Finally, the candidate answers are ranked by relevance and presented to the user. This diagram shows AnswerBus as an open-domain QA system that leverages traditional search engines and then applies further processing to extract and rank specific answers from the search results. Problems (in Question Answering) Challenges in Question Answering include: \"How\" and \"Why\" questions: These often require deeper reasoning and causal understanding beyond simple fact retrieval. \": Requires understanding events and narratives. \": Requires understanding actions and consequences. Correctness: Ensuring the provided answer is factually accurate. Answer Presentation: Presenting the answer clearly, concisely, and in an understandable format. Examples of humorous or difficult-to-answer questions from Webclopedia, highlighting challenges in QA: Question: Where do lobsters like to live? Answer: on a Canadian airline (Incorrect, likely a result of misinterpretation or a factoid related to a specific event). Question: Where do hyenas live? Answer: in Saudi Arabia (Plausible but incomplete). Answer: in the back of pick-up trucks (Incorrect, likely a humorous or out-of-context misinterpretation). Question: Where are zebras most likely found?",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(a).txt",
    "file_name": "lec7-(a).txt",
    "position_in_document": 63,
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "askjeeves",
      "question answering",
      "definition",
      "examples"
    ],
    "all_keywords": [
      "askjeeves",
      "question answering",
      "lec7",
      "definition",
      "examples"
    ],
    "keyword_string": "askjeeves question answering lec7 definition examples",
    "token_count": 598,
    "word_count": 421,
    "sentence_count": 31,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7040133779264214,
    "avg_sentence_length": 13.580645161290322
  },
  {
    "chunk_id": 154,
    "chunk_hash": "93fee0cd418a",
    "text": "(TREC) -- Text Retrieval Conference TREC is a yearly information retrieval competition. Began in 1992, with the Question Answering (QA) track introduced in 1999. Its purpose is to encourage research into systems that return direct answers rather than just lists of documents. Questions (Q's) are typically \"open domain\" (can be about any topic) and \"closed class\" (expect a specific type of answer, like a name, date, or number). Answers (A's) are constrained to be less than 50 characters and usually entities or noun phrases. (TREC) -- Text Retrieval Conference (Continued) TREC QA Track in 2001: Included 500 questions. Some answers were \"nil\" (no answer found) or presented large difficulty. Many questions were \"definition questions\" (e.g., \"What is X? QA list tasks (Example): \"Name 4 cities that have a 'Shubert' theater.\" (Requires extracting multiple entities of a specific type). QA context tasks (Examples): \"How many species of spiders are there?\" \"How many are poisonous to humans?\" \"What percentage of spider bites in the US are fatal?\" These questions require extracting specific numerical or factual information, often requiring aggregation or calculation from text. Example Questions and Results Example Questions and the systems that would attempt to answer them: Question: \"What river in the US is known as the Big Muddy?\" Systems: AskJeeves, AnswerBus, Google Question: \"What person’s head is on a dime?\" Systems: AskJeeves, AnswerBus, AltaVista Question: \"Show some paintings by Claude Monet\" The field of Question Answering shows: Strong User Demand: Users increasingly expect direct answers. Enormous Interest in the Problem: Significant research and development focus. Successes: Continuous advancements in QA system capabilities. AskMSR: Question Answering Using the Worldwide Web Authors: Michele Banko, Eric Brill, Susan Dumais, Jimmy Lin Paper URL: http://www.ai.mit.edu/people/jimmylin/publications/Banko-etal-AAAI02.pdf Published in: Proceedings of 2002 AAAI SYMPOSIUM on Mining Answers from Text and Knowledge Bases, March 2002 Web Question Answering: Is More Always Better? Authors: Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, Andrew Ng",
    "enhanced_text": "[NLP] (TREC) -- Text Retrieval Conference TREC is a yearly information retrieval competition. Began in 1992, with the Question Answering (QA) track introduced in 1999. Its purpose is to encourage research into systems that return direct answers rather than just lists of documents. Questions (Q's) are typically \"open domain\" (can be about any topic) and \"closed class\" (expect a specific type of answer, like a name, date, or number). Answers (A's) are constrained to be less than 50 characters and usually entities or noun phrases. (TREC) -- Text Retrieval Conference (Continued) TREC QA Track in 2001: Included 500 questions. Some answers were \"nil\" (no answer found) or presented large difficulty. Many questions were \"definition questions\" (e.g., \"What is X? QA list tasks (Example): \"Name 4 cities that have a 'Shubert' theater.\" (Requires extracting multiple entities of a specific type). QA context tasks (Examples): \"How many species of spiders are there?\" \"How many are poisonous to humans?\" \"What percentage of spider bites in the US are fatal?\" These questions require extracting specific numerical or factual information, often requiring aggregation or calculation from text. Example Questions and Results Example Questions and the systems that would attempt to answer them: Question: \"What river in the US is known as the Big Muddy?\" Systems: AskJeeves, AnswerBus, Google Question: \"What person’s head is on a dime?\" Systems: AskJeeves, AnswerBus, AltaVista Question: \"Show some paintings by Claude Monet\" The field of Question Answering shows: Strong User Demand: Users increasingly expect direct answers. Enormous Interest in the Problem: Significant research and development focus. Successes: Continuous advancements in QA system capabilities. AskMSR: Question Answering Using the Worldwide Web Authors: Michele Banko, Eric Brill, Susan Dumais, Jimmy Lin Paper URL: http://www.ai.mit.edu/people/jimmylin/publications/Banko-etal-AAAI02.pdf Published in: Proceedings of 2002 AAAI SYMPOSIUM on Mining Answers from Text and Knowledge Bases, March 2002 Web Question Answering: Is More Always Better? Authors: Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, Andrew Ng",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(b).txt",
    "file_name": "lec7-(b).txt",
    "position_in_document": 36,
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "began",
      "text retrieval conference trec",
      "trec",
      "question answering"
    ],
    "all_keywords": [
      "began",
      "question answering",
      "trec",
      "lec7",
      "text retrieval conference trec"
    ],
    "keyword_string": "began question answering trec lec7 text retrieval conference trec",
    "token_count": 500,
    "word_count": 316,
    "sentence_count": 21,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.632,
    "avg_sentence_length": 15.047619047619047
  },
  {
    "chunk_id": 155,
    "chunk_hash": "1768a431e9a4",
    "text": "Authors: Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, Andrew Ng Paper URL: http://research.microsoft.com/~sdumais/SIGIR2002-QA-Submit-Conf.pdf Website: www.answerbus.com http://misshoover.si.umich.edu/~zzheng/qa-new/ http://www2002.org/CDROM/poster/203/ About page: http://www.ask.co.uk/docs/about/what_is.asp TREC paper: http://trec.nist.gov/pubs/trec9/papers/webclopedia.pdf Project page: http://www.isi.edu/natural-language/projects/webclopedia/ Project page: http://www.ai.mit.edu/projects/infolab/ailab.html Text Retrieval Conference (TREC) Introduction (Revisited) Natural Language Processing (NLP) Field of Computer Science. Includes tasks like Sentiment Analysis, Word prediction, Translation, and Question Answering. Question Answering (QA) An important sub-domain of NLP. Approaches used to develop QA Systems: Conventional (Machine learning & IR) Answer Selection (a major component of QA systems) is a focus of research. Conceptual Diagram Explanation (Categories of QA Systems):\nThe diagram categorizes QA systems into three main types based on their domain, approach, and underlying mechanisms: Domain-Based Categorization: Closed Domain: QA systems designed to answer questions within a very specific, limited topic area (e.g., medical QA system). Open Domain: QA systems that can answer questions about a wide range of topics, often leveraging large text corpora like the web. Linguistic/Pattern-Based Approaches: Linguistic: Systems heavily relying on linguistic rules, parsing, and semantic analysis. Statistical: Systems employing statistical models derived from large datasets. Pattern Matching: Systems that identify answers by matching predefined patterns in text. Retrieval and Reasoning-Based Approaches: IR/IE based QA System: Systems that primarily use Information Retrieval (IR) to find relevant documents and Information Extraction (IE) to extract answers from them. Restricted domain QA system: Similar to closed domain, emphasizing a narrow focus for better accuracy.",
    "enhanced_text": "[NLP] Authors: Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, Andrew Ng Paper URL: http://research.microsoft.com/~sdumais/SIGIR2002-QA-Submit-Conf.pdf Website: www.answerbus.com http://misshoover.si.umich.edu/~zzheng/qa-new/ http://www2002.org/CDROM/poster/203/ About page: http://www.ask.co.uk/docs/about/what_is.asp TREC paper: http://trec.nist.gov/pubs/trec9/papers/webclopedia.pdf Project page: http://www.isi.edu/natural-language/projects/webclopedia/ Project page: http://www.ai.mit.edu/projects/infolab/ailab.html Text Retrieval Conference (TREC) Introduction (Revisited) Natural Language Processing (NLP) Field of Computer Science. Includes tasks like Sentiment Analysis, Word prediction, Translation, and Question Answering. Question Answering (QA) An important sub-domain of NLP. Approaches used to develop QA Systems: Conventional (Machine learning & IR) Answer Selection (a major component of QA systems) is a focus of research. Conceptual Diagram Explanation (Categories of QA Systems):\nThe diagram categorizes QA systems into three main types based on their domain, approach, and underlying mechanisms: Domain-Based Categorization: Closed Domain: QA systems designed to answer questions within a very specific, limited topic area (e.g., medical QA system). Open Domain: QA systems that can answer questions about a wide range of topics, often leveraging large text corpora like the web. Linguistic/Pattern-Based Approaches: Linguistic: Systems heavily relying on linguistic rules, parsing, and semantic analysis. Statistical: Systems employing statistical models derived from large datasets. Pattern Matching: Systems that identify answers by matching predefined patterns in text. Retrieval and Reasoning-Based Approaches: IR/IE based QA System: Systems that primarily use Information Retrieval (IR) to find relevant documents and Information Extraction (IE) to extract answers from them. Restricted domain QA system: Similar to closed domain, emphasizing a narrow focus for better accuracy.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(b).txt",
    "file_name": "lec7-(b).txt",
    "position_in_document": 65,
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "began",
      "text retrieval conference trec",
      "trec",
      "question answering"
    ],
    "all_keywords": [
      "began",
      "question answering",
      "trec",
      "lec7",
      "text retrieval conference trec"
    ],
    "keyword_string": "began question answering trec lec7 text retrieval conference trec",
    "token_count": 509,
    "word_count": 231,
    "sentence_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.4538310412573674,
    "avg_sentence_length": 21.0
  },
  {
    "chunk_id": 156,
    "chunk_hash": "a133fa8c2c91",
    "text": "Statistical: Systems employing statistical models derived from large datasets. Pattern Matching: Systems that identify answers by matching predefined patterns in text. Retrieval and Reasoning-Based Approaches: IR/IE based QA System: Systems that primarily use Information Retrieval (IR) to find relevant documents and Information Extraction (IE) to extract answers from them. Restricted domain QA system: Similar to closed domain, emphasizing a narrow focus for better accuracy. Rule based QA system: Systems that use a set of predefined rules and ontologies to derive answers. Categories of QA systems QA Systems based on NLP and IR: These systems utilize syntax processing, Named Entity Tagging, and Information Retrieval (IR) techniques. They often use free text documents from open-domain sources as their data resource. They typically deal with \"wh-type\" questions (who, what, when, where, why) and provide query responses in the form of extracted snippets from documents. QA Systems Reasoning with NLP: These systems employ Semantic Analysis or high-level reasoning techniques. They perform tasks on Knowledge Base data resources and are often domain-oriented. They are not restricted to \"wh-type\" questions and can provide synthesized responses that go beyond simple text extraction. Conceptual Diagram Explanation (QAS Architecture):\nThe diagram illustrates a common architecture for a Question Answering System (QAS), highlighting its main components and their sub-components. The architecture generally involves a pipeline of processing stages: High-Level Components (Left and Right Bubbles): Question Processing (1): Handles the user's input question. Document Processing (2): Manages the collection of documents (corpus) from which answers are sought. Answer Processing (3): Generates and refines the final answers. Detailed Sub-Components (Bottom Panel): Question Processing / Analysis: This phase analyzes the input query to understand its intent and identify key elements. Question Analysis: Initial understanding of the question. Question Classification: Categorizing the question type (e.g., factoid, list). Question Reformulation: Rephrasing the question for better retrieval. Document Processing / Information Retrieval: This phase finds relevant information from the corpus. Information Retrieval: Retrieving documents or passages relevant to the processed question. Paragraph Filtering: Selecting paragraphs or smaller units likely to contain answers. Paragraph Ordering: Ranking the selected paragraphs by relevance. Answer Processing / Answer Identification: This final phase extracts and validates the answer. Answer Identification: Pinpointing specific candidate answers within the retrieved text. Answer Extraction: Extracting the actual answer text. Answer Validation: Verifying the correctness and quality of the extracted answer. This layered architecture shows a typical flow from understanding a user's question to delivering a verified answer, emphasizing that QA is a complex task involving multiple NLP and IR sub-tasks. Question types commonly handled by QA systems: Simple and fact-based.",
    "enhanced_text": "[NLP] Statistical: Systems employing statistical models derived from large datasets. Pattern Matching: Systems that identify answers by matching predefined patterns in text. Retrieval and Reasoning-Based Approaches: IR/IE based QA System: Systems that primarily use Information Retrieval (IR) to find relevant documents and Information Extraction (IE) to extract answers from them. Restricted domain QA system: Similar to closed domain, emphasizing a narrow focus for better accuracy. Rule based QA system: Systems that use a set of predefined rules and ontologies to derive answers. Categories of QA systems QA Systems based on NLP and IR: These systems utilize syntax processing, Named Entity Tagging, and Information Retrieval (IR) techniques. They often use free text documents from open-domain sources as their data resource. They typically deal with \"wh-type\" questions (who, what, when, where, why) and provide query responses in the form of extracted snippets from documents. QA Systems Reasoning with NLP: These systems employ Semantic Analysis or high-level reasoning techniques. They perform tasks on Knowledge Base data resources and are often domain-oriented. They are not restricted to \"wh-type\" questions and can provide synthesized responses that go beyond simple text extraction. Conceptual Diagram Explanation (QAS Architecture):\nThe diagram illustrates a common architecture for a Question Answering System (QAS), highlighting its main components and their sub-components. The architecture generally involves a pipeline of processing stages: High-Level Components (Left and Right Bubbles): Question Processing (1): Handles the user's input question. Document Processing (2): Manages the collection of documents (corpus) from which answers are sought. Answer Processing (3): Generates and refines the final answers. Detailed Sub-Components (Bottom Panel): Question Processing / Analysis: This phase analyzes the input query to understand its intent and identify key elements. Question Analysis: Initial understanding of the question. Question Classification: Categorizing the question type (e.g., factoid, list). Question Reformulation: Rephrasing the question for better retrieval. Document Processing / Information Retrieval: This phase finds relevant information from the corpus. Information Retrieval: Retrieving documents or passages relevant to the processed question. Paragraph Filtering: Selecting paragraphs or smaller units likely to contain answers. Paragraph Ordering: Ranking the selected paragraphs by relevance. Answer Processing / Answer Identification: This final phase extracts and validates the answer. Answer Identification: Pinpointing specific candidate answers within the retrieved text. Answer Extraction: Extracting the actual answer text. Answer Validation: Verifying the correctness and quality of the extracted answer. This layered architecture shows a typical flow from understanding a user's question to delivering a verified answer, emphasizing that QA is a complex task involving multiple NLP and IR sub-tasks. Question types commonly handled by QA systems: Simple and fact-based.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(b).txt",
    "file_name": "lec7-(b).txt",
    "position_in_document": 97,
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "began",
      "text retrieval conference trec",
      "trec",
      "question answering"
    ],
    "all_keywords": [
      "began",
      "question answering",
      "trec",
      "lec7",
      "text retrieval conference trec"
    ],
    "keyword_string": "began question answering trec lec7 text retrieval conference trec",
    "token_count": 601,
    "word_count": 425,
    "sentence_count": 29,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.7071547420965059,
    "avg_sentence_length": 14.655172413793103
  },
  {
    "chunk_id": 157,
    "chunk_hash": "3dc93fdb02d0",
    "text": "Answer Identification: Pinpointing specific candidate answers within the retrieved text. Answer Extraction: Extracting the actual answer text. Answer Validation: Verifying the correctness and quality of the extracted answer. This layered architecture shows a typical flow from understanding a user's question to delivering a verified answer, emphasizing that QA is a complex task involving multiple NLP and IR sub-tasks. Question types commonly handled by QA systems: Simple and fact-based. Generally starts with \"Wh-\" words (Who, What, When, Where). Requires the answer to be in a list of facts or entities. Requires explanations, reasons, or elaborations about an entity/event. Typically \"How\" or \"Why\" type questions. Requires answers in the form of \"yes\" or \"no\". Systems need inference mechanisms, world knowledge, and common sense reasoning to reply. Related to any hypothetical event. Generally begins with \"what would happen if\". There are no specific correct answers for these questions, making them challenging for QA. Answer selection is one of the important tasks of Question Answering. It deals with the selection of the correct and suitable answer from a set of candidate answers retrieved based upon a user query in natural language. Research work has been done in Answer selection using: Conventional Machine learning & IR methods Deep learning methods Conventional Approaches for Answer Selection Conventional approaches for answer selection use linguistic tools, feature engineering, and external resources. Tree-edit distance: Measures the similarity between linguistic parse trees. Support Vector Machine (SVM): A supervised machine learning model used for classification. Lexical semantic features calculation using WordNet: Leveraging a lexical database to understand word meanings and relationships. Matching between the words using their semantic relevance: Comparing words based on their meaning rather than just exact matches. Deep Learning is a sub-field of Machine Learning that mimics the human brain's structure and function. Neural Networks are a core component of deep learning. Common types used in NLP for tasks like answer selection include: Recurrent Neural Network (RNN): Long-Short Term Memory (LSTM) Bidirectional LSTM (BiLSTM) Convolutional Neural Network (CNN): Literature Review (Deep Learning Approaches for Answer Selection) Conceptual Table Explanation (Literature Review - Page 26):\nThis table summarizes several research papers on applying Deep Learning (DL) approaches to Answer Selection, providing an overview of the author(s), title, DL approach used, dataset, question types addressed, and the reported results (often in metrics like MAP, MRR, or Precision). (2015): Applied CNN to InsuranceQA dataset for Non-factoid answer selection, achieving Precision 62.8. (2015): Used Stacked BiLSTM on TREC QA dataset for Factoid questions, reporting MAP 0.7134 and MRR 0.7913.",
    "enhanced_text": "[NLP] Answer Identification: Pinpointing specific candidate answers within the retrieved text. Answer Extraction: Extracting the actual answer text. Answer Validation: Verifying the correctness and quality of the extracted answer. This layered architecture shows a typical flow from understanding a user's question to delivering a verified answer, emphasizing that QA is a complex task involving multiple NLP and IR sub-tasks. Question types commonly handled by QA systems: Simple and fact-based. Generally starts with \"Wh-\" words (Who, What, When, Where). Requires the answer to be in a list of facts or entities. Requires explanations, reasons, or elaborations about an entity/event. Typically \"How\" or \"Why\" type questions. Requires answers in the form of \"yes\" or \"no\". Systems need inference mechanisms, world knowledge, and common sense reasoning to reply. Related to any hypothetical event. Generally begins with \"what would happen if\". There are no specific correct answers for these questions, making them challenging for QA. Answer selection is one of the important tasks of Question Answering. It deals with the selection of the correct and suitable answer from a set of candidate answers retrieved based upon a user query in natural language. Research work has been done in Answer selection using: Conventional Machine learning & IR methods Deep learning methods Conventional Approaches for Answer Selection Conventional approaches for answer selection use linguistic tools, feature engineering, and external resources. Tree-edit distance: Measures the similarity between linguistic parse trees. Support Vector Machine (SVM): A supervised machine learning model used for classification. Lexical semantic features calculation using WordNet: Leveraging a lexical database to understand word meanings and relationships. Matching between the words using their semantic relevance: Comparing words based on their meaning rather than just exact matches. Deep Learning is a sub-field of Machine Learning that mimics the human brain's structure and function. Neural Networks are a core component of deep learning. Common types used in NLP for tasks like answer selection include: Recurrent Neural Network (RNN): Long-Short Term Memory (LSTM) Bidirectional LSTM (BiLSTM) Convolutional Neural Network (CNN): Literature Review (Deep Learning Approaches for Answer Selection) Conceptual Table Explanation (Literature Review - Page 26):\nThis table summarizes several research papers on applying Deep Learning (DL) approaches to Answer Selection, providing an overview of the author(s), title, DL approach used, dataset, question types addressed, and the reported results (often in metrics like MAP, MRR, or Precision). (2015): Applied CNN to InsuranceQA dataset for Non-factoid answer selection, achieving Precision 62.8. (2015): Used Stacked BiLSTM on TREC QA dataset for Factoid questions, reporting MAP 0.7134 and MRR 0.7913.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(b).txt",
    "file_name": "lec7-(b).txt",
    "position_in_document": 128,
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "began",
      "text retrieval conference trec",
      "trec",
      "question answering"
    ],
    "all_keywords": [
      "began",
      "question answering",
      "trec",
      "lec7",
      "text retrieval conference trec"
    ],
    "keyword_string": "began question answering trec lec7 text retrieval conference trec",
    "token_count": 596,
    "word_count": 417,
    "sentence_count": 26,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.6996644295302014,
    "avg_sentence_length": 16.03846153846154
  },
  {
    "chunk_id": 158,
    "chunk_hash": "34b84656e817",
    "text": "(2015): Applied CNN to InsuranceQA dataset for Non-factoid answer selection, achieving Precision 62.8. (2015): Used Stacked BiLSTM on TREC QA dataset for Factoid questions, reporting MAP 0.7134 and MRR 0.7913. (2014): Employed CNN on TREC QA for Factoid answer selection, with results MAP 0.7113 and MRR 0.7846. (2015): Utilized BiLSTM then CNN (with Attention) on TREC QA & InsuranceQA for Non-factoid questions, yielding MAP 72.79 and MRR 82.40. (2016): Developed Inner Attention based Recurrent Neural Networks (RNN), tested on WikiQA and TREC QA, with results MAP 0.7341/0.7369 and MRR 0.7418/0.8208. (2017): Implemented an Attention-based encoder-decoder model (BiLSTM) on TREC QA for Factoid questions, reporting MAP 0.7261 and MRR 0.8018. This table demonstrates the evolution and effectiveness of different neural network architectures for improving answer selection in QA systems across various datasets and question complexities. Literature Review (Continued) Conceptual Table Explanation (Literature Review - Page 27):\nThis table continues the summary of deep learning literature in Question Answering, focusing on more recent work and hybrid approaches. (2019): Explored Transformer-Based Neural Networks (BiLSTM) for answer selection in question answering, using WikiQA data. Reported MAP 0.6941 and MRR 0.7077. (2017): Proposed a Hybrid Framework for Text Modeling with Convolutional RNN (Conv-RNN), evaluated on WikiQA and InsuranceQA. Achieved Accuracy 71.7, MAP 0.7427, and MRR 0.7504. (2019): Presented Collaborative Learning for Answer Selection using CNN and BiLSTM in parallel on Insurance QA. Reported MAP 0.7219 and MRR 0.6756. The section also includes a reference to a paper on putting QA systems into practice: B. Kratzwald and S. Feuerriegel, \"Putting Question-Answering Systems into Practice: Transfer Learning for Efficient Domain Customization,\" ACM Trans. 15:1-15:20, Feb. 2019. This highlights the practical challenges and solutions, such as transfer learning for domain customization, in deploying QA systems.",
    "enhanced_text": "[NLP] (2015): Applied CNN to InsuranceQA dataset for Non-factoid answer selection, achieving Precision 62.8. (2015): Used Stacked BiLSTM on TREC QA dataset for Factoid questions, reporting MAP 0.7134 and MRR 0.7913. (2014): Employed CNN on TREC QA for Factoid answer selection, with results MAP 0.7113 and MRR 0.7846. (2015): Utilized BiLSTM then CNN (with Attention) on TREC QA & InsuranceQA for Non-factoid questions, yielding MAP 72.79 and MRR 82.40. (2016): Developed Inner Attention based Recurrent Neural Networks (RNN), tested on WikiQA and TREC QA, with results MAP 0.7341/0.7369 and MRR 0.7418/0.8208. (2017): Implemented an Attention-based encoder-decoder model (BiLSTM) on TREC QA for Factoid questions, reporting MAP 0.7261 and MRR 0.8018. This table demonstrates the evolution and effectiveness of different neural network architectures for improving answer selection in QA systems across various datasets and question complexities. Literature Review (Continued) Conceptual Table Explanation (Literature Review - Page 27):\nThis table continues the summary of deep learning literature in Question Answering, focusing on more recent work and hybrid approaches. (2019): Explored Transformer-Based Neural Networks (BiLSTM) for answer selection in question answering, using WikiQA data. Reported MAP 0.6941 and MRR 0.7077. (2017): Proposed a Hybrid Framework for Text Modeling with Convolutional RNN (Conv-RNN), evaluated on WikiQA and InsuranceQA. Achieved Accuracy 71.7, MAP 0.7427, and MRR 0.7504. (2019): Presented Collaborative Learning for Answer Selection using CNN and BiLSTM in parallel on Insurance QA. Reported MAP 0.7219 and MRR 0.6756. The section also includes a reference to a paper on putting QA systems into practice: B. Kratzwald and S. Feuerriegel, \"Putting Question-Answering Systems into Practice: Transfer Learning for Efficient Domain Customization,\" ACM Trans. 15:1-15:20, Feb. 2019. This highlights the practical challenges and solutions, such as transfer learning for domain customization, in deploying QA systems.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(b).txt",
    "file_name": "lec7-(b).txt",
    "position_in_document": 145,
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "began",
      "text retrieval conference trec",
      "trec",
      "question answering"
    ],
    "all_keywords": [
      "began",
      "question answering",
      "trec",
      "lec7",
      "text retrieval conference trec"
    ],
    "keyword_string": "began question answering trec lec7 text retrieval conference trec",
    "token_count": 528,
    "word_count": 287,
    "sentence_count": 17,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "chunk_density": 0.5435606060606061,
    "avg_sentence_length": 16.88235294117647
  }
]